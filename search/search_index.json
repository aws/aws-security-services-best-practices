{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#what-is-this-guide","title":"What is this guide?","text":"<p>As AWS security professionals we are often asked by customers to validate their use of AWS security services and to give tips and tricks on how to use these services and how others use AWS security services. With this guide we have the goal of more broadly sharing this knowledge with the user community and at the same time give the ability for others outside of AWS to contribute.</p>"},{"location":"#what-you-will-and-wont-find-here","title":"What you will and won't find here","text":"<p>Simply, we will be covering best practices for configuring AWS security services. This is NOT overall AWS security best practices. This documentation is not simply a numbered list of best practices. Instead this documentation is meant to walk you through what you need to know before deploying an AWS security service to what you should be doing after enablement and through fully operationalizing the service. Often this is done through discussing different use cases and different factors associated with specific use cases that can help in making design decisions. Following this guide you should feel confident that you have the ability configure and use an AWS security service effectively.</p> <p>Often in this documentation we will be referring back to the full service documentation. Many times in this documentation we will briefly mention some information that provides context into a potential service best practice without explaining a particular feature in complete depth. It is our goal not to replicate the AWS service documentation but instead provide as much detail as possible into different components of a service or feature so that you can effectively use a service. Keep in mind you might need to dive deep into certain pieces of documentation to understand a feature or functionality in it\u2019s entirety.</p>"},{"location":"#related-guides","title":"Related guides","text":"<ul> <li>EKS Best Practices Guides</li> <li>EMR Containers Best Practices Guides</li> <li>AWS Observability Best Practices</li> <li>ELB Best Practices Guides</li> <li>Cloud Operations Best Practices Guide</li> </ul>"},{"location":"contributors/","title":"Contributors","text":"<p>The content on this site is maintained by Solution Architects, AWS Security service team members and other volunteers from across the organization. Our goal is to improve the discovery of relevant best practices on how to set up and use AWS security services.</p> <p>Recipes and content contributions in general so far are from the following people:</p> Authors Authors Authors Authors Marshall Jones Ray Elkins Mutaz Hajeer Jason Schamp Scott Ward Chris Simmons Uday Chandrasen Kashish Wadhwa Priyank Ghedia Brent Maynard Shachar Hirshberg Rick Anthony Lee Messier Bryan Van Hook Jesse Lepich Michael Leighty Justin Criswell Joe Wagner Lawton Pittenger Cheryl Wang Rochak Karki <p>Note that all recipes published on this site are available via the MIT-0 license, a modification to the usual MIT license that removes the requirement for attribution.</p>"},{"location":"guides/certificate-services/","title":"Certificate Services","text":""},{"location":"guides/certificate-services/#introduction","title":"Introduction","text":"<p>Welcome to the AWS Certificate Manager (ACM) and AWS Private CA Guide. The purpose of this guide is to provide prescriptive guidance for leveraging AWS Certificate Manager (ACM) and AWS Private Certificate Authority (AWS Private CA) for creating and controlling the certificates that are used for maintaining a secure and effective Public Key Infrastructure (PKI). Publishing this guidance via GitHub will allow for quick iterations to enable timely recommendations that include service enhancements, as well as, the feedback of the user community. This guide is designed to provide value whether you are deploying a certificate system for the first time in a single account, or looking for ways to optimize your infrastructure in an existing multi-account infrastructure.</p>"},{"location":"guides/certificate-services/#how-to-use-this-guide","title":"How to use this guide","text":"<p>This guide is geared towards security practitioners who are responsible for working with certificates within AWS accounts (and resources). The best practices are organized into different categories for easier consumption. Each category includes a set of corresponding best practices that begin with a brief overview, followed by detailed steps for implementing the guidance. The topics do not need to be read in a particular order:</p> <ul> <li>What is AWS Certificate Manager (ACM)?</li> <li>What are the benefits of using ACM?</li> <li>Getting started with ACM<ul> <li>Certificate considerations</li> <li>Data protection considerations</li> <li>AWS CloudFormation considerations</li> <li>Turn on AWS CloudTrail</li> </ul> </li> <li>Implementing ACM<ul> <li>Deployment considerations</li> <li>Domain name considerations</li> <li>Controlling access to ACM certificates</li> <li>Certificate pinning</li> <li>Opting out of certificate transparency logging</li> </ul> </li> <li>Operationalizing ACM<ul> <li>Setting up automated notifications for expiring certificates with Amazon Eventbridge</li> <li>Setting up automated notifications for expiring certificates with CloudWatch alarms</li> <li>Certificate inventory management</li> <li>Compliance and auditing</li> <li>Managed renewal for imported certificates</li> <li>Renewal status tracking</li> </ul> </li> <li>What is AWS Private Certificate Authority (AWS Private CA)?</li> <li>What are the benefits of using AWS Private CA?</li> <li>Getting started with AWS Private CA<ul> <li>Certificate considerations</li> <li>Data protection considerations</li> <li>Validation considerations</li> <li>Domain name considerations</li> <li>Deployment considerations</li> <li>Minimize use of the Root CA if possible</li> </ul> </li> <li>Implementing AWS Private CA<ul> <li>Certificate authority hierarchy design</li> <li>Key algorithm and length selection</li> <li>Certificate template configuration</li> <li>CA Sharing</li> <li>Integration with AWS Services</li> <li>Access control and IAM policies secure</li> <li>Managed revocation</li> </ul> </li> <li>Connector for Kubernetes</li> <li>Connector for SCEP</li> <li>Connector for Active Directory</li> <li>Cross-account CA sharing</li> <li>What is the difference between ACM and AWS Private CA?</li> <li>Cost Considerations</li> <li>AWS Private CA Best Practices Checklist</li> <li>Resources</li> </ul>"},{"location":"guides/certificate-services/#what-is-aws-certificate-manager-acm","title":"What is AWS Certificate Manager (ACM)?","text":"<p>ACM is an AWS service that simplifies the management of provisioning, storing, and renewing public and private SSL/TLS X.509 certificates for use with AWS services. </p> <p>TLS plays an important role in encrypting data in transit. The security of TLS is built on cryptography and digital signatures where a certificate authority signs a document attesting to the identity of a machine, which is then digitally verified by a client initiating the TLS connection. Certificate-based signatures use digital certificates issued by a certificate authority (CA) to link a signature to a specific identity, validating the identity of the signer. ACM issues certificates through two paths: public certificates from Amazon's public certificate authority for internet domains, and private certificates via AWS Private CA for internal resources. </p> <p> Figure 1: An overview of ACM</p> <p>ACM simplifies the cryptographic complexity involved in managing certificates and their associated keys. Through ACM, you can centrally manage certificates, seamlessly integrate with native AWS services, and automate the lifecycle of certificates.</p>"},{"location":"guides/certificate-services/#what-are-the-benefits-of-using-acm","title":"What are the benefits of using ACM?","text":"<p>ACM simplifies the management and deployment of public and private SSL/TLS certificates by enabling centralized control through the AWS Management Console, AWS CLI, or APIs. Certificate usage can be monitored through AWS CloudTrail Logs for auditing and governance purposes.</p> <p>These TLS certificates can be issued at no additional cost for use with various AWS services including Elastic Load Balancers, Amazon CloudFront distributions, and APIs on Amazon API Gateway. Additionally, you can also import SSL/TLS certificates from third-party CAs.</p> <p>ACM provides managed renewals for all SSL/TLS certificates issued by Amazon. Certificates using DNS validation will be automatically renewed. For private certificates issued by the Private CA IssueCertificate API or imported certificates, users should implement automated expiration notifications.</p>"},{"location":"guides/certificate-services/#getting-started-with-acm","title":"Getting started with ACM","text":""},{"location":"guides/certificate-services/#certificate-considerations","title":"Certificate considerations","text":"<p>ACM manages three types of SSL/TLS certificates: public (domain or email-validated), private, and imported. Users can request a publicly trusted certificate, import a third-party certificate, or or request a private certificate, issued by a customer-owned CA hosted in AWS Private CA. ACM certificates are exclusively compatible with integrated AWS services such as Elastic Load Balancing, CloudFront, and API Gateway. For a full list, see Services Integrated with ACM.</p> <p>The validity period for certificates requested from ACM is fixed at 13 months and cannot be modified for either public or private certificates. If organizations require different validity periods, they should consider importing a certificate from a third-party CA vendor.</p>"},{"location":"guides/certificate-services/#data-protection-considerations","title":"Data protection considerations","text":"<p>When requesting a public certificate from ACM, the service generates a public/private key pair on your behalf. The public key becomes part of the certificate, while ACM uses KMS to encrypt the private key with an encryption context. ACM then stores the certificate and its corresponding encrypted private key. AWS uses the same KMS key to encrypt private keys for all certificates in a specific AWS account and region, with all KMS calls handled internally by ACM.</p> <p>KMS cryptographic operations with symmetric encryption keys accept an encryption context\u2014an optional set of non-secret key-value pairs containing additional contextual information about the data. This context can be inserted into KMS Encrypt operations to enhance the audibility of API decryption calls. ACM automatically sets the context with values including the certificate ARN of the private key being encrypted, the AWS Region, and the Account ID, which are then cryptographically bound to the ciphertext. The unique encryption context for every ACM certificate creates a logical isolation between certificates.</p> <p>Important considerations for ACM public certificates include:</p> <ul> <li>They cannot directly access KMS operations</li> <li>You cannot modify how ACM calls KMS</li> <li>You are unable to modify the encryption context</li> <li>You are prohibited from exporting the private key of a public certificate</li> </ul>"},{"location":"guides/certificate-services/#aws-cloudformation-considerations","title":"AWS CloudFormation considerations","text":"<p>With AWS CloudFormation you can create a template that describes the AWS resources that you want to use. AWS CloudFormation then provisions and configures those resources for you. AWS CloudFormation can provision resources that are supported by ACM such as Elastic Load Balancing, Amazon CloudFront, and Amazon API Gateway. </p> <p>Fast development cycles often require rapid iteration. If you use AWS CloudFormation to quickly create and delete multiple test environments, we recommend that you do not create a separate ACM certificate for each environment. Doing so could exhaust your certificate quota. Instead, create a wildcard certificate that covers all of the domain names that you are using for testing. For example, if you repeatedly create ACM certificates for domain names that vary by only a version number, such as \\.service.example.com, create instead a single wildcard certificate for &lt;*&gt;.service.example.com. Include the wildcard certificate in the template that AWS CloudFormation uses to create your test environment. <p>When using AWS CloudFormation to request a certificate, domain validation is only handled under specific conditions: the domain must be hosted in Amazon Route 53, reside in your AWS account, and use DNS validation. If these conditions are not met, the CloudFormation stack will remain in the CREATE_IN_PROGRESS state, delaying further stack operations.</p>"},{"location":"guides/certificate-services/#turn-on-aws-cloudtrail","title":"Turn on AWS CloudTrail","text":"<p>Turn on CloudTrail logging before you begin using ACM. CloudTrail enables you to monitor your AWS deployments by retrieving a history of AWS API calls for your account, including API calls made via the AWS Management Console, the AWS SDKs, the AWS Command Line Interface, and higher-level Amazon Web Services. You can also identify which users and accounts called the ACM APIs, the source IP address the calls were made from, and when the calls occurred. You can integrate CloudTrail into applications using the API, automate trail creation for your organization, check the status of your trails, and control how administrators turn CloudTrail logging on and off. CloudTrail also simplifies certificate lifecycle management by enabling event-driven workflows to notify or take action on expiring TLS certificates in your organization. </p> <p>For more information, see Creating a Trail. You can also go to Using CloudTrail with AWS Certificate Manager to see example trails for ACM actions.</p>"},{"location":"guides/certificate-services/#implementing-acm","title":"Implementing ACM","text":""},{"location":"guides/certificate-services/#deployment-considerations","title":"Deployment considerations","text":"<p>ACM is a regional service, which means ACM certificates must be issued in the same Region as the AWS resources where they will be used and cannot be shared across AWS accounts. A critical exception exists for Amazon CloudFront: all ACM certificates for CloudFront must be issued in the US East (N. Virginia) Region (us-east-1). Once associated with a CloudFront distribution, these certificates are then distributed to all configured geographic locations. </p>"},{"location":"guides/certificate-services/#domain-validation-considerations","title":"Domain validation considerations","text":"<p>Before a CA can issue a certificate for your site, ACM must verify that you own or control all the domains that you specified in your request. You can perform verification using either email or DNS. </p> <p> Figure 2: ACM domain validation</p> <p>Amazon recommends using DNS validation over email validation. DNS validation offers several key advantages:</p> <ul> <li>Direct integration with Route 53 allows you to update records through ACM</li> <li>ACM automatically renews DNS-validated certificates as long as the DNS record remains in place and the certificate is actively used</li> <li>No manual intervention is required for renewal</li> </ul> <p>Email-validated certificates require a different approach. Renewal notices are sent to five common system addresses for the requested domain  (e.g., admin or postmaster) starting 45 days before expiration. These certificates require manual approval from the domain owner. All listed domains must be validated for a renewed certificate with the same ARN to be issued, which can introduce potential errors or delays if email configurations are incorrect.</p>"},{"location":"guides/certificate-services/#domain-name-considerations","title":"Domain name considerations","text":"<p>ACM certificates can secure singular domain names, multiple specific domain names, wildcard domains (protecting an unlimited number of subdomains), or any combination of these. Once created, certificates cannot be modified by adding or removing domain names. To make changes, you must request a new certificate with the entire revised list of domain names, validating ownership of all domains, including previously validated names.</p> <p>You cannot add or remove domain names from an existing ACM certificate. Instead you must request a new certificate with the revised list of domain names. For example, if your certificate has five domain names and you want to add four more, you must request a new certificate with all nine domain names. As with any new certificate, you must validate ownership of all the domain names in the request, including the names that you previously validated for the original certificate.</p> <p>If you use email validation, you receive up to 8 validation email messages for each domain, at least 1 of which must be acted upon within 72 hours. For example, when you request a certificate with five domain names, you receive up to 40 validation messages, at least 5 of which must be acted upon within 72 hours. As the number of domain names in the certificate request increases, so does the work required to use email to validate domain ownership.</p> <p>If you use DNS validation instead, you must write one new DNS record to the database for the FQDN you want to validate. ACM sends you the record to create and later queries the database to determine whether the record has been added. Adding the record asserts that you own or control the domain. In the preceding example, if you request a certificate with five domain names, you must create five DNS records. We recommend that you use DNS validation when possible.</p>"},{"location":"guides/certificate-services/#controlling-access-to-acm-certificates","title":"Controlling access to ACM certificates","text":"<p>Unlike other data protection services like KMS or Secrets Manager, ACM does not support resource policies or resource control policies (RCPs). Instead, access to ACM certificates can be controlled through either identity-based IAM policies, or service control policies (SCPs) if using AWS Organizations.</p> <p>Use account-level separation in your policies to control who can access certificates at an account level. Keep your production certificates in separate accounts than your testing and development certificates. If you can't use account-level separation, you can restrict access to specific roles by denying the kms:CreateGrant action  in your policies for the KMS key that protects the certificate private key. This limits which roles in an account can sign or use certificates at a high level. For information about grants, including grant terminology, see Grants in AWS KMS in the AWS Key Management Service Developer Guide.</p> <p>If you want more granular control than restricting the use of <code>kms:CreateGrant</code> by account, you can limit <code>kms:CreateGrant</code> to specific certificates using <code>kms:EncryptionContext</code> condition keys. Specify <code>arn:aws:acm</code> as the key, and the value of the ARN to restrict. The following example policy prevents the use of a specific certificate, but allow others.</p> <pre><code>{\n   \"Version\": \"2012-10-17\",\n   \"Statement\": [\n       {\n           \"Sid\": \"VisualEditor0\",\n           \"Effect\": \"Deny\",\n           \"Action\": \"kms:CreateGrant\",\n           \"Resource\": \"*\",\n           \"Condition\": {\n               \"StringEquals\": {\n                   \"kms:EncryptionContext:aws:acm:arn\": \"arn:aws:acm:us-east-1:111122223333:certificate/[certificate-id-here]\"\n               }\n           }\n       }\n   ]\n}\n</code></pre>"},{"location":"guides/certificate-services/#certificate-pinning","title":"Certificate pinning","text":"<p>Certificate pinning (or SSL pinning) is a process that you can use in your application to validate a remote host by associating that host directly with its X.509 certificate or public key instead of with a certificate hierarchy, bypassing SSL/TLS certificate chain validation.  Instead of verifying the entire certificate chain from root CA downward, the application only trusts a pre-defined certificate or key for that host. This trusted certificate can be embedded during development or captured during the first connection to the host.</p> <p>We recommend that your application not pin an ACM certificate. ACM performs managed certificate renewal in AWS Certificate Manager to automatically renew your Amazon-issued SSL/TLS certificates before they expire. To renew a certificate, ACM generates a new public-private key pair. If your application pins the ACM certificate and the certificate is successfully renewed with a new public key, the application might be unable to connect to your domain.</p> <p>If you decide to pin a certificate, the following options will not hinder your application from connecting to your domain:</p> <ul> <li>Import your own certificate into ACM and then pin your application to the imported certificate. ACM doesn't try to automatically renew imported certificates.</li> <li>If you're using a public certificate, pin your application to all available Amazon root certificates. If you're using a private certificate, pin your application to the CA's root certificate.</li> </ul>"},{"location":"guides/certificate-services/#opting-out-of-certificate-transparency-logging","title":"Opting out of certificate transparency logging","text":"<p>As of April 30 2018, Google Chrome no longer trusts public SSL/TLS certificates that are not recorded in a certificate transparency log. Therefore, beginning April 24, 2018, the Amazon CA began publishing all new certificates and renewals to at least two public logs. Once a certificate has been logged, it cannot be removed.</p> <p>Logging is performed automatically when you request a certificate or when a certificate is renewed, but you can choose to opt out. Common reasons for doing so include concerns about security and privacy. For example, logging internal host domain names gives potential attackers information about internal networks that would otherwise not be public. In addition, logging could leak the names of new or unreleased products and websites.</p> <p>To opt out of transparency logging when you are requesting a certificate, use the options parameter of the request-certificate AWS CLI command or the RequestCertificate API operation. If your certificate was issued before April 24, 2018, and you want to make sure that it is not logged during renewal, you can use the update-certificate-options command or the UpdateCertificateOptions API operation to opt out.</p> <p>Limitations</p> <ul> <li>You cannot use the console to enable or disable transparency logging.</li> <li>You cannot change logging status after a certificate enters its renewal period, typically 60 days before certificate expiry. No error message is generated if a status change fails.</li> </ul> <p>Once a certificate has been logged, it cannot be removed from the log. Opting out at that point will have no effect. If you opt out of logging when you request a certificate and then choose later to opt back in, your certificate will not be logged until it is renewed. If you want the certificate to be logged immediately, we recommend that you issue a new one.</p> <p>The following example shows you how to use the request-certificate command to disable certificate transparency when you request a new certificate.</p> <pre><code>aws acm request-certificate \\\n--domain-name www.example.com \\\n--validation-method DNS \\\n--options CertificateTransparencyLoggingPreference=DISABLED \\\n</code></pre> <p>The preceding command outputs the ARN of your new certificate.</p> <pre><code>{ \n    \"CertificateArn\": \"arn:aws:acm:region:account:certificate/certificate_ID\" \n}\n</code></pre> <p>If you already have a certificate, and you don't want it to be logged when it is renewed, use the update-certificate-options command. This command does not return a value.</p> <pre><code>aws acm update-certificate-options \\\n--certificate-arn arn:aws:acm:region:account:\\\ncertificate/certificate_ID \\\n--options CertificateTransparencyLoggingPreference=DISABLED\n</code></pre>"},{"location":"guides/certificate-services/#operationalizing-acm","title":"Operationalizing ACM","text":""},{"location":"guides/certificate-services/#setting-up-automated-notifications-for-expiring-certificates-with-amazon-eventbridge","title":"Setting up automated notifications for expiring certificates with Amazon EventBridge","text":"<p>ACM does not manage renewal or send expiry notices for certificates issued using the Private CA Issue Certificates API or imported certificates. Instead, you can take advantage of the ACM Certificate Approaching Expiration event to notify you of expiry events. </p> <ol> <li>Create a relevant standard SNS topic.</li> <li>Open the EventBridge console. In the navigation pane, choose Buses, and then Rules.</li> <li>Click Create Rule.</li> <li>Fill in the name of your rule. Leave all other settings default.</li> </ol> <p> Figure 3: Defining the Eventbridge rule detail</p> <ol> <li>Scroll down to Event Pattern. Under AWS Service, choose Certificate Manager. </li> <li>For Event Type, choose ACM Certificate Approaching Expiration.</li> </ol> <p> Figure 4: Defining the Eventbridge event pattern</p> <ol> <li>Click Next.</li> <li>Select the SNS topic created in Step 1.</li> </ol> <p> Figure 5: Specifying the Eventbridge target</p> <ol> <li>Click Next, then skip ahead to Step 5.</li> <li>Review your selections, then click Create rule.</li> </ol> <p>Your rule will now notify you via SNS topic when your certificate approaches expiry. ACM will send daily expiration events for all active certificates (public, private and imported) starting 45 days prior to expiration. To change the timing of the expiration notification, use the <code>PutAccountConfiguration</code> action of the ACM API to select a value between 1-45 days for DaysBeforeExpiry.</p> <p>The EventBridge rule will notify you regardless if your certificate is imported, private, or issued by ACM. You may want to have more granular notifications that only notify you for certain certificates, for instance.</p>"},{"location":"guides/certificate-services/#setting-up-automated-notifications-for-expiring-certificates-with-cloudwatch-alarms","title":"Setting up automated notifications for expiring certificates with CloudWatch alarms","text":"<p>CloudWatch Alarms monitor certificate expiration in AWS environments and prevent service disruptions through automated notifications. The service sends alerts via email, SMS, or incident management systems when certificates approach their expiration threshold.</p> <p>The AWS console has one specific limitation: CloudWatch Alarms can monitor only one certificate metric per alarm configuration. For environments with multiple certificates, use the AWS CLI or Infrastructure as Code (IaC) tools to create and manage alarms programmatically.</p> <p>Two solutions exist for scalable certificate monitoring:</p> <ol> <li>AWS Lambda functions that execute certificate expiration checks on a defined schedule.</li> <li>AWS Config rules that track certificate lifecycles across AWS accounts.</li> </ol>"},{"location":"guides/certificate-services/#certificate-inventory-management","title":"Certificate inventory management","text":"<p>Organizations can implement AWS Config rules to actively track certificate usage and deployment patterns across their infrastructure. A structured resource tagging strategy assigns specific ownership and purpose metadata to each certificate, enabling clear accountability and tracking. Organizations typically generate automated inventory reports using AWS Organizations and AWS Config, providing comprehensive visibility across certificate deployments. Regular audits of certificate-to-resource mappings ensure accuracy and help identify potential gaps in certificate management. </p>"},{"location":"guides/certificate-services/#compliance-and-auditing","title":"Compliance and Auditing","text":"<p>Compliance and auditing processes focus on maintaining detailed records through AWS CloudTrail logs of all certificate operations. These logs enable automated reporting systems to identify unused certificates and track approaching expirations, supporting proactive certificate management. Organizations implement compliance validation scripts to verify certificate configurations against internal policies and regulatory requirements. Regular security reviews assess certificate policies and deployments, ensuring continuous alignment with security standards and compliance mandates.</p>"},{"location":"guides/certificate-services/#managed-renewal-for-imported-certificates","title":"Managed renewal for imported certificates","text":"<p>AWS Config automates renewal of imported certificates in ACM using resource tags and remediation rules. Unlike AWS-issued certificates which renew automatically, imported certificates require this specific automation for renewal. The automation process follows a fixed sequence: An AWS Config rule identifies expiring imported certificates, triggers a Lambda function, which then uses certificate metadata from resource tags to execute renewal. The function retrieves the new certificate, imports it to ACM, and updates associated AWS resources automatically. The solution eliminates manual renewals, prevents certificate-related outages, and standardizes management across all certificate types. Implementation requires precise configuration of AWS Config rules, Lambda functions, and resource tags.</p>"},{"location":"guides/certificate-services/#renewal-status-tracking","title":"Renewal status tracking","text":"<p>AWS Config monitors imported certificate renewals in ACM using defined rules and metadata tags. Each certificate maintains one of four status tags: pending renewal, renewal in progress, renewal complete, or renewal failed. Config rules trigger EventBridge notifications upon status changes, ensuring timely alerts for renewal activities.</p> <p>Organizations can track renewal status through AWS Config dashboards and aggregator reports, while the system logs all renewal actions for audit purposes. This automation ensures continuous monitoring of certificate lifecycles and prevents renewal-related outages.</p>"},{"location":"guides/certificate-services/#what-is-aws-private-certificate-authority-aws-private-ca","title":"What is AWS Private Certificate Authority (AWS Private CA)?","text":"<p>AWS Private CA is a highly available, managed service designed to create and maintain an internal public key infrastructure (PKI) for organizations. This service eliminates the upfront investment and ongoing maintenance costs associated with operating a private certificate authority while simplifying certificate lifecycle management.</p>"},{"location":"guides/certificate-services/#what-are-the-benefits-of-using-aws-private-ca","title":"What are the benefits of using AWS Private CA?","text":""},{"location":"guides/certificate-services/#improve-availability-and-scalability","title":"Improve availability and scalability","text":"<p>AWS Private CA\u2019s fundamental benefits of availability and scalability offer a robust 99.9% availability guarantee through a service level agreement (SLA). Private CA eliminates the complexity and overhead associated with managing on-premises PKI infrastructure. The service's flexible architecture supports seamless scaling as business demands grow, while providing versatile deployment options through multiple connector types - including Active Directory, Kubernetes, and SCEP for Mobile Device Management (MDM). Organizations can leverage various certificate modes (short-lived and general-purpose) to align with their specific security requirements, enabling a tailored approach to certificate issuance and management. </p>"},{"location":"guides/certificate-services/#enhance-security-posture","title":"Enhance security posture","text":"<p>Security is enhanced through multiple layers of protection. AWS Private CA employs FIPS 140-3 validated hardware security modules (HSMs) to safeguard CA keys, providing enterprise-grade security for critical PKI components. The integration with AWS Identity and Access Management (IAM) enables granular access control and permissions management for Certificate Authorities. Security response capabilities are strengthened through efficient certificate revocation mechanisms, including Online Certificate Status Protocol (OCSP) and Certificate Revocation Lists (CRLs), allowing organizations to quickly address potential certificate compromises and maintain a robust security posture. </p>"},{"location":"guides/certificate-services/#simplify-certificate-lifecycle-management","title":"Simplify certificate lifecycle management","text":"<p>Certificate lifecycle management is streamlined through automation and integration. AWS Private CA works seamlessly with AWS Certificate Manager to automate crucial tasks such as certificate issuance, renewal, and revocation for both AWS and non-AWS resources. This automation eliminates the manual overhead traditionally associated with PKI management, reducing the operational burden on IT teams. </p> <p>By automating routine PKI management tasks, organizations can redirect their resources toward strategic security initiatives while maintaining efficient certificate operations. </p>"},{"location":"guides/certificate-services/#accelerate-compliance-and-regulatory-adherence","title":"Accelerate compliance and regulatory adherence","text":"<p>The service accelerates compliance and regulatory adherence through centralized management and comprehensive auditing capabilities. Integration with AWS CloudTrail and Amazon CloudWatch provides unified visibility and detailed audit trails, simplifying compliance with industry regulations and standards. AWS Private CA enables organizations to easily share CAs across multiple AWS accounts while maintaining strict access controls and security best practices. The platform's customizable certificate templates support organization-specific identity and data protection policies, accommodating diverse use cases from web services to IoT devices, all while optimizing costs and maintaining security standards. </p>"},{"location":"guides/certificate-services/#getting-started-with-aws-private-ca","title":"Getting started with AWS Private CA","text":""},{"location":"guides/certificate-services/#certificate-considerations_1","title":"Certificate considerations","text":"<p>AWS Private CA enables you to create and operate private CAs on AWS. Unlike ACM's public certificates, Private CA lets you establish your own PKI hierarchy with root and subordinate CAs for issuing private certificates. These certificates can be used with both AWS services and on-premises applications.</p> <p>Types of certificates:</p> <ul> <li>Root CA - The top-level authority in your PKI hierarchy</li> <li>Subordinate CA - Issues certificates under the  authority of a root CA</li> </ul> <p> Figure 6: A simple, three-level CA hierarchy</p> <p>ACM offers fixed 13-month validity periods for certificates, while Private CA offers flexible configuration for CAs as well as issued certificates:</p> <ul> <li>There is no limit for Root CA validity. The Private CA default for root certificates is ten years.</li> <li>Subordinate CAs can have custom validity periods. The Private CA default validity period for a subordinate CA certificate is three years.</li> </ul> <p>A certificate managed by Private CA must have a validity period shorter than or equal to the validity period of the CA that issued it.</p>"},{"location":"guides/certificate-services/#data-protection-considerations_1","title":"Data protection considerations","text":"<p>When you create a private certificate authority using AWS Private CA, the service generates a private/public key pair for the root or subordinate CA. You can use CloudTrail to monitor and audit all cryptographic operations associated with your CA.</p>"},{"location":"guides/certificate-services/#validation-considerations","title":"Validation Considerations","text":"<p>Since AWS Private CA is designed for internal certificates, you control the issuance policies and validation methods rather than relying on public domain validation. You define the certificate subjects and any associated extensions or constraints in your CA\u2019s configuration. You can implement your own validation processes that relies on certificate templates that define allowed configurations, custom validation rules, and access controls implemented through IAM policies. Organizations can define their own validation criteria, including custom certificate extensions, key usage restrictions, and naming conventions. Trust is established through:</p> <ul> <li>IAM Policies: Controlling who can issue, revoke, or manage certificates. You can also use IAM condition context keys to allow or deny authorized accounts and users to request certificates for specific domains.</li> <li>Certificate Templates: Predefined or custom templates that enforce certificate  parameters (e.g., key usage, extended key usage).</li> <li>Revocation: Revoking certificates via API/console and either publishing Certificate Revocation Lists (CRLs) to an S3 bucket or enabling OCSP.</li> <li>Name Constraints: DNS name constraints that establish guardrails to mitigate certificate misuse. For example, you can set a DNS name constraint that restricts the CA from issuing certificates to a resource using a specific domain name.</li> </ul> <p>Please note for cross-account certificate issuance that explicit IAM permissions must be configured to validate and authorize requests. </p>"},{"location":"guides/certificate-services/#domain-name-considerations_1","title":"Domain name considerations","text":"<p>AWS Private CA offers flexible domain name considerations, allowing you to issue certificates for both internal and external domain names without the public domain validation requirements of ACM. You can use any valid domain naming convention, including non-public domains (like .internal, .local, .corp), private IP addresses, and custom internal naming schemes. The certificates can include Subject Distinguished Name (DN) components (such as Common Name, Organization, Country, etc.) and multiple Subject Alternative Names (SANs) that support DNS names, IP addresses, internal hostnames, and wildcard domains. </p>"},{"location":"guides/certificate-services/#deployment-considerations_1","title":"Deployment considerations","text":"<p>AWS Private CA is  a regional service. You must create your private CA in the same AWS Region  where your resources are deployed, or where you plan to use the  certificates. However, certificates are not Region bound and you can issue certificates for resources located in  different regions. You can use AWS Private CA with AWS Certificate Manager (ACM) to deploy certificates to AWS services like Elastic Load Balancing, API Gateway, and CloudFront.</p>"},{"location":"guides/certificate-services/#minimize-use-of-the-root-ca-if-possible","title":"Minimize use of the Root CA if possible","text":"<p>Customers should avoid sharing Root CA between accounts and keep it as restricted as possible. The Root CA should be placed in an isolated account with MFA enabled, with no cross-account access other than to sign subordinate CAs.</p> <p>A root CA should in general only be used to issue certificates for intermediate CAs. This allows the root CA to be stored out of harm's way in its own account while the intermediate CAs perform the daily task of issuing end-entity certificates.</p> <p> Figure 7: A Best Practice CA Hierarchy with minimal Root usage</p>"},{"location":"guides/certificate-services/#implementing-aws-private-ca","title":"Implementing AWS Private CA","text":""},{"location":"guides/certificate-services/#certificate-authority-hierarchy-design","title":"Certificate authority hierarchy design","text":"<p>The root and subordinate CA hierarchy should be carefully designed with consideration of geographic distribution, business unit separation, and specific use case requirements. A hierarchy design establishes the framework for all certificate operations and directly impacts a PKI infrastructure's security and manageability. A well-designed CA hierarchy offers granular security controls appropriate to each CA, the division of administrative tasks for better load balancing and security, the use of CAs with limited, revocable trust for daily operations, and validity periods and certificate path limits.</p> <p>In AWS Private CA, you can create a hierarchy of certificates with up to five levels. A root CA can have any number of branches, with as many as four levels of subordinate CAs per branch. It is also possible to create multiple hierarchies, each with its own root.</p> <p>A CA hierarchy should have strongest security at the top of the tree, protecting the root CA and its private key, as a compromised root could destroy trust in the entire PKI. As a result, root and high-level subordinate CAs should only be used infrequently (e.g. to sign other CA certificates, or when a CRL or OCSP responder needs to be configured) and be tightly controlled and audited. More routine administrative tasks should be given to lower-level CAs in the hierarchy.</p>"},{"location":"guides/certificate-services/#key-algorithm-and-length-selection","title":"Key algorithm and length selection","text":"<p>Private CA supports the following RSA and Elliptic Curve algorithms. Cryptographic algorithms and key lengths (RSA 2048 or 4096 bits) should be selected to balance security and performance requirements, as a choice impacts certificate processing overhead, compatibility requirements, and long-term security posture. Regular evaluation of these choices should be implemented within an organization to ensure continued alignment with evolving cryptographic standards and emerging security threats.</p>"},{"location":"guides/certificate-services/#certificate-template-configuration","title":"Certificate template configuration","text":"<p>Private CA uses configuration templates to issue CA certificates and end-entity certificates. Four varieties of templates are supported:</p> <ul> <li>Base templates - Pre-defined templates in which no passthrough parameters are allowed</li> <li>CSRPassthrough templates - Templates that extend a corresponding base template version by allowing CSR passthrough, e.g. extension values in the certificate signing request that are copied over to the issued certificate. (Note: if a CSR contains extension values that conflict with the template definition, a template definition will have higher priority).</li> <li>APIPassthrough templates - Templates that extend a corresponding base template version by allowing API passthrough, or dynamic values, e.g. dynamic values that are known to the admin or intermediate systems that may not be known by the entity that requested the certificate, are impossible to define in a template, or may not be available in a CSR (Note: if a APIPassthrough parameter contains extension values that conflict with the template definition, a template definition will have higher priority).</li> <li>APICSRPassthrough templates - Templates that extend a corresponding base template version by allowing both API and CSR passthrough. (Note: if a template definition, API passthrough values, or CSR passthrough extensions conflict, the priority will be template definition, APIPassthrough, and then CSR passthrough extensions).</li> </ul> <p>Certificate templates standardize and streamline the certificate issuance process. Organizations must define templates with appropriate validity periods, extensions, and constraints that align with different use cases and security requirements. Well-designed templates enforce organizational policies while maintaining operational flexibility. This standardization reduces human error, ensures consistency in certificate issuance, and simplifies certificate lifecycle management across the organization.</p>"},{"location":"guides/certificate-services/#ca-sharing","title":"CA Sharing","text":"<p>AWS Private CA offers cross-account sharing, or the ability to grant permissions for other accounts to use a centralized CA to issue end-entity certificates or subordinate CA certificates. This can be done using AWS Resource Access Manager (RAM) to manage permissions or by using the Private CA API or CLI to attach a resource-based policy to a CA. </p> <p>If using RAM to share Private CA, keep in mind that RAM is a Regional service and that resource share is Regional. ACM PCA resource shares with principals in another AWS account must only access resources from the same AWS Region as the resource share (and any supported global resources created in us-east-1). </p>"},{"location":"guides/certificate-services/#integration-with-aws-services","title":"Integration with AWS Services","text":"<p>Integration planning consists of connecting AWS Private CA with essential AWS services, particularly ACM for automated lifecycle management. You should evaluate your existing AWS infrastructure to identify integration points with services like IoT and Direct Connect. The integration strategy should emphasize automation, scalability, and efficient certificate deployment while leveraging the full capabilities of the AWS ecosystem.</p>"},{"location":"guides/certificate-services/#access-control-and-iam-policies-secure","title":"Access control and IAM policies secure","text":"<p>AWS Private CA operations require implementing precise access controls through AWS IAM. Organizations must create roles and policies that enforce the principle of least privilege, defining specific permissions for certificate issuance, revocation, and CA management. An effective way to have control over CA management is through separation of permissions; i.e., having User A only be able to change the state of the CA, while User B can only install and issue certificates through the CA. Both users are thus required to create and activate a new CA. This separation of permissions should also be implemented between roles that disable CAs, and roles that can delete them.</p> <p>Regular policy reviews ensure continued alignment with security requirements while maintaining operational efficiency. This structured approach to access control prevents unauthorized access and supports compliance requirements.</p>"},{"location":"guides/certificate-services/#managed-revocation","title":"Managed revocation","text":"<p>The RevokeCertificate API revokes a AWS Private CA certificate before its scheduled expiration in the event of a key being compromised, the associate domain becomes invalid, etc. AWS provides two fully managed mechanisms for the client using the certificate to check the revocation status: OCSP and certificate revocation lists (CRLs).</p> <p> Figure 8: A comparison of AWS fully managed revocation mechanisms</p> <p>OCSP</p> <p>AWS Private CA provides a fully managed OCSP solution to notify endpoints that certificates have been revoked. Customers can enable OCSP on CAs through the Private CA console, the API, the CLI, or CloudFormation. With OCSPs, the client queries an authoritative revocation database that returns a synchronous status. When OCSP is enabled, Private CA will include the URL of the OCSP responder in the Authority Information Access (AIA) extension of the new certificate.</p> <p>Some considerations for OCSP:</p> <ul> <li>OCSP responses may take up to 60 minutes to reflect a revoked certificate\u2019s new status</li> <li>APIPassthrough and CSRPassthrough certificate templates do not work with AIA if the OCSP responder is enabled</li> <li>The endpoint of the managed OCSP service is available on the public internet. Customers who want OCSP but prefer not to have a public endpoint will have to operate an alternative infrastructure.</li> <li>OCSPs are priced on-demand.</li> </ul> <p>CRL</p> <p>A CRL is a file that includes a list of all revoked certificates, as well as relevant information including reason for revocation. When you configure a CA, you can choose whether or not AWS Private CA creates a complete or partitioned CRL (where a CRL is partitioned into several smaller CRLs). As each CRL can only revoke 50,000 certificates, the more partitioned CRLs you have, the more certificates that you can revoke, up to a 1,000,000 limit for each supported region.</p> <p>Some considerations for CRL:</p> <ul> <li>CRLs are updated approximately 30 minutes after a certificate is revoked. If a CRL update fails, Private CA will continue to make further attempts every 15 minutes</li> <li>Because of the download and processing requirements, CRLs require more memory than OCSP. Consider caching revocation lists or partitioned CRLs.</li> <li>If you update your CRL from complete to partitioned, AWS Private CA creates new partitions as needed and adds the IDP extension to all CRLs, including the original. </li> <li>Partitioned CRLs dramatically increase the number of certi\ufb01cates your private CA can issue, and saves you from frequently rotating your CAs.</li> </ul>"},{"location":"guides/certificate-services/#connector-for-kubernetes","title":"Connector for Kubernetes","text":"<p>AWS Private CA introduced a powerful integration with Kubernetes through its open-source cert-manager plugin, aws-privateca-issuer. This solution enables organizations to manage certificates for their Kubernetes workloads while maintaining strict security controls and compliance requirements. The plugin eliminates the need to store sensitive private keys within the cluster, significantly reducing security risks and simplifying certificate lifecycle management.</p> <p>The aws-privateca-issuer plugin offers flexibility, supporting deployments across Amazon EKS, self-managed Kubernetes on AWS, and on-premises Kubernetes environments. Organizations can leverage this solution regardless of their infrastructure choices, including both x86 and ARM architectures. This integration is particularly valuable for enterprises with stringent regulatory requirements, as it provides enhanced auditability and control over certificate operations. Security teams can maintain centralized oversight of certificate issuance while allowing development teams to efficiently manage their certificate needs through familiar Kubernetes workflows. </p> <p> Figure 9: AWS Private CA Connector for Kubernetes</p> <p>This architecture shows you how to set up end-to-end encryption on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Private CA. . For this example of end-to-end encryption, traffic originates from your client and terminates at an Ingress controller server running inside a sample app. </p>"},{"location":"guides/certificate-services/#connector-for-scep","title":"Connector for SCEP","text":"<p>AWS Private Certificate Authority (AWS Private CA) Connector for SCEP (Simple Certificate Enrollment Protocol) offers a streamlined solution for mobile device certificate management. This preview feature integrates seamlessly with popular Mobile Device Management (MDM) solutions to enhance security and simplify certificate deployment for mobile devices.</p> <p>The connector provides several key benefits for organizations managing mobile device certificates. It significantly reduces operational costs and complexity by simplifying the process of securing mobile devices with private certificates. Organizations can leverage a single Certificate Authority with the AWS Private CA connector, eliminating the need for multiple CAs across different use cases, which streamlines certificate management and reduces infrastructure complexity.</p> <p>A major advantage of this solution is its cost-effectiveness and efficiency in PKI operations. By utilizing a managed private CA with a managed SCEP service, organizations can save both time and money on their PKI infrastructure. Furthermore, the solution enables automatic enrollment of mobile devices through AWS Private CA, working in conjunction with commercially available MDM solutions, which simplifies device onboarding and certificate deployment processes.</p> <p> Figure 10: AWS Private CA Connector for SCEP</p> <p>The architecture, as illustrated in the diagram, shows a straightforward flow where the MDM solution communicates with mobile devices through the AWS Private CA Connector for SCEP, which then interfaces with AWS Private CA for certificate management. This integrated approach provides a secure and efficient method for managing device certificates at scale.</p>"},{"location":"guides/certificate-services/#connector-for-active-directory","title":"Connector for Active Directory","text":"<p>AWS Private CA Connector for Active Directory can help you provision certificates for users and machines within your Microsoft Active Directory (AD) environment. By combining AWS Private CA with AD, you can leverage managed private CA while using your existing AD infrastructure to handle certificate enrollment, distribution, and trust. Using the AWS Private CA Connector for Active Directory, you can replace on-premises enterprise or other third-party CAs with a managed private CA that you own, providing certificate enrollment to users, groups, and machines that are managed by your AD. </p> <p>To integrate AWS Private CA with Active Directory, you can use the AWS Directory Service, which provides a managed Microsoft Active Directory in the AWS Cloud. By creating a directory using AWS Directory Service, you can then configure AWS Private CA to trust the directory and issue certificates to users and devices that are authenticated against the Active Directory. The Connector for AD can be used to issue certificates to domain-joined devices for both on-premise AD environments (using the Active Directory Connector provided by AWS Directory Services) and AD environments hosted in AWS Managed Microsoft AD.</p> <p> Figure 11: AWS Private CA Conenctor for Active Directory</p> <p>This architecture displays the difference between an on-premises ADCS implementation without AWS Private CA, and an implementation using AWS Private CA with the Connector for AD. </p>"},{"location":"guides/certificate-services/#cross-account-ca-sharing","title":"Cross account CA sharing","text":"<p>AWS Private CA supports cross-account sharing, which allows you to share your private certificate authority (CA) with other AWS accounts within your organization. </p> <p>You can use AWS RAM (Resource Access Manager) to share an ACM Private CA to create a resource share with another AWS account. Using AWS RAM to apply a resource-based policy, an AWS Private CA administrator can share access to a CA with a user in a different AWS account directly or through AWS Organizations. The primary account attaches a policy to the Private CA, specifying which secondary accounts or IAM roles can perform actions like issuing certificates, revoking certificates, or exporting the CA certificate. Secondary accounts then use IAM policies to allow their users or roles to interact with the shared CA. This setup ensures centralized control over the CA\u2019s lifecycle while delegating certificate issuance to trusted accounts.</p> <p>You can also share an ACM Private CA with other entities such as the following:</p> <ul> <li>Other principals, such as AWS Identify and Access  Management (IAM) users and IAM roles.</li> <li>Organizational units (OUs).</li> <li>The entire AWS organization that your account is a  member of.</li> </ul> <p> Figure 12: Cross Account CA Sharing</p> <p>This architecture provides a detailed overview of Cross-Account CA Sharing using AWS Private CA. It demonstrates how the Root CA, Issuing CA, and RAM Shared CA work together to enable centralized certificate management while maintaining a chain of trust across accounts.</p>"},{"location":"guides/certificate-services/#what-is-the-difference-between-acm-and-private-ca","title":"What is the difference between ACM and Private CA?","text":"<p>AWS Private CA and ACM are distinct yet complementary services. AWS Private CA enables organizations to establish and operate their own CA within AWS, allowing them to issue private certificates for both internal and external resources with complete control over the certificate hierarchy and policies. In contrast, ACM primarily focuses on managing SSL/TLS certificates specifically for AWS services, offering free public certificates from Amazon Trust Services CA and automated certificate lifecycle management including renewal and deployment.</p> <p>While Private CA provides broader flexibility and can issue certificates for any resource with associated costs for CA operation and certificate issuance, ACM streamlines certificate management specifically for AWS services with free public certificates and deep service integrations. These services often work together in practice, with AWS Private CA handling the creation of private certificates and ACM managing their deployment and lifecycle within AWS services, providing a comprehensive certificate management solution for organizations.</p> <ul> <li>Use ACM when you need free, automatically managed public certificates for your AWS services like websites, applications, or APIs running on AWS.</li> <li>Use AWS Private CA when you need to create and manage your own private certificates for internal resources, applications outside of AWS, or when you require complete control over your certificate issuance and policies.</li> </ul>"},{"location":"guides/certificate-services/#cost-considerations","title":"Cost considerations","text":"<p>AWS Certificate Manager (ACM) provides public TLS certificates at no cost. AWS PrivateCA pricing follows a three-part model: a fixed monthly fee per CA for operational costs, usage-based charges for private certificate issuance, and OCSP validation fees (while CRL usage remains free).</p> <p>If you are just getting started with AWS Private CA or enabling it on new accounts, AWS Private CA has a 30-day free trial for the first private CA created in the account in each Region. You pay for any certificates issued during the trial, including a one-time charge for the first time you export the private key and certificate of certificates requested through ACM. This trial period provides an opportunity to evaluate the service's features and confirm it meets your organization's PKI requirements without incurring the full CA operation costs. </p>"},{"location":"guides/certificate-services/#aws-private-ca-best-practices-checklist","title":"AWS Private CA best practices checklist","text":"<p>Best practices are recommendations that can help you use AWS Private CA effectively. The following best practices are based on real-world experience from current AWS Private CA customers.</p> <ul> <li>Document CA structure and policies. Do not issue end entity certificates from the root CA.</li> <li>Define and set CA validity periods during initial CA creation. </li> <li>Limit Root CA usage to only issuing intermediate CA certificates. </li> <li>Maintain a simple CA hierarchy of 2-3 levels for efficient management. </li> <li>Deploy the root CA in a dedicated AWS account separate from other CAs. </li> <li>Maintain separation between administrator and certificate issuer roles through distinct IAM permissions. </li> <li>Implement certificate revocation through OCSP or CRLs for automated certificate status management. </li> <li>Enable AWS CloudTrail logging before creating and operating private CAs. </li> <li>Regularly rotate CA private keys through certificate imports or CA replacement. </li> <li>Remove unused CAs following the recommended deletion process. </li> <li>Enable S3 Block Public Access for CRL storage buckets to protect PKI details. </li> <li>Follow Amazon EKS Best Practices Guides when using AWS Private CA with Kubernetes environments.</li> </ul>"},{"location":"guides/certificate-services/#resources","title":"Resources","text":""},{"location":"guides/certificate-services/#workshops","title":"Workshops","text":"<ul> <li>Activation Days</li> <li>AWS Encryption in Transit Workshop</li> <li>Hands-on ACM Troubleshooting Labs</li> </ul>"},{"location":"guides/certificate-services/#videos","title":"Videos","text":"<ul> <li>AWS Private Certificate Authority (AWS Private CA) Connector for SCEP</li> </ul>"},{"location":"guides/certificate-services/#blogs","title":"Blogs","text":"<ul> <li>How to manage certificate lifecycles using ACM event-driven workflows</li> <li>How to enforce DNS name constraints in AWS Private cA</li> </ul>"},{"location":"guides/detective/","title":"Amazon Detective","text":""},{"location":"guides/detective/#introduction","title":"Introduction","text":"<p>Welcome to the Amazon Detective Hub Best Practices Guide. The purpose of this guide is to provide prescriptive guidance for leveraging Amazon Detective for investigating security issues associated with your AWS resources. Publishing this guidance via GitHub will allow for quick iterations to enable timely recommendations that include service enhancements, as well as the feedback from the user community. This guide is designed to provide value whether you are deploying Detective for the first time in a single account, or looking for ways to optimize Detective in an existing multi-account deployment.</p>"},{"location":"guides/detective/#how-to-use-this-guide","title":"How to use this guide","text":"<p>This guide is geared towards security practitioners who are responsible for monitoring and remediation of threats and malicious activity within AWS accounts (and resources). The best practices are organized into categories for easier consumption. Each category includes a set of corresponding best practices that begin with a brief overview, followed by detailed steps for implementing the guidance. The topics do not need to be read in a particular order:</p> <ul> <li>What is Amazon Detective</li> <li>Who is Amazon Detective for</li> <li>What are the benefits of enabling Amazon Detective</li> <li>Getting started<ul> <li>Enable GuardDuty</li> <li>Where to enable Detective</li> <li>Region considerations</li> </ul> </li> <li>Implementation<ul> <li>Enablement</li> <li>Enroll other AWS account in your organization</li> <li>Enable optional data feeds</li> </ul> </li> <li>Operationalizing<ul> <li>Providing access</li> <li>Investigating a finding</li> <li>Start your investigation from GuardDuty</li> <li>Start your investigation from a Finding Group</li> <li>Threat hunting in Detective</li> <li>Crafting Detective Embedded URLs</li> </ul> </li> <li>Cost Considerations</li> <li>Resources</li> </ul>"},{"location":"guides/detective/#what-is-amazon-detective","title":"What is Amazon Detective?","text":"<p>Amazon Detective makes it easy to analyze, investigate, and quickly identify the root cause of security findings or suspicious activities. Detective automatically collects log data from your AWS resources and uses machine learning, statistical analysis, and graph theory to help you visualize and conduct faster and more efficient security investigations.</p>"},{"location":"guides/detective/#who-is-amazon-detective-for","title":"Who is Amazon Detective for?","text":"<p>Amazon Detective works well for customers who want to take advantage of a managed security service that helps connect the dots for investigating security issues in AWS. Many customers may be a bigger organization with many tools and want to enable analysts to quickly triage an incident. Other customers may have a leaner IT Security staff and need an effective tool to help their teams efficiently investigate security issues in AWS. Detective is for security analysts, security operations engineers, and incident response teams. Larger customers can benefit from using Amazon Detective if they have experienced tool sprawl and are looking to consolidate their SIEM tooling. Customers have particularly found value with Amazon Detective for telemetry and visibility into Amazon Elastic Kubernetes Service (EKS) clusters also, due to the tight integration of EKS audit logs and VPC flow logs.</p>"},{"location":"guides/detective/#what-are-the-benefits-of-enabling-amazon-detective","title":"What are the benefits of enabling Amazon Detective?","text":"<p>Detective aids incident responders and security engineers in determining what resources are associated with a given security event. It uses data feeds from GuardDuty findings, VPC Flow Logs, CloudTrail, EKS audit logs, and Security Hub findings, to build a behavior graph to help customers understand resource associations, abnormal activity like newly observed geo locations, understanding when roles were assumed by other roles, along with many other use cases in your environment. Many customers may not have the log and telemetry collection correlation tooling built, or the knowledge or resources, and this makes Amazon Detective a perfect fit.</p> <p>Many customers that have the tools, knowledge, and people already in place, are able to successfully navigate security events and discover related logs within an acceptable response time. Typically this requires third party tooling to be installed, configured, operated, and monitored to collect VPC Flow logs, CloudTrail logs, EKS Audit logs, and shipped to a log aggregation tool. Consider the cost tradeoff, and time saved, by using Amazon Detective where visibility into these data feeds is built-in vs aggregated and stored separately. Most customers have found Detective effective in shortening investigation times because they do not have to write queries into a log aggregation tool and instead rely on Detective provided insights.</p> <p>Many customers observe that using Detective and GuardDuty together provides the ability to investigate natively in AWS after an event. GuardDuty tells you about the threat, but Detective tells you the story around that threat. Often we hear from customers that the amount of security findings from any given service overwhelm their teams and leave them without a clear priority. Detective enables teams to focus on true security events.</p> <p>Detective is well suited Amazon EKS workloads to assist in runtime investigations. Detective allows you to view inside EKS workloads to examine pod configuration, pod image, image source, VPC flows within an EKS cluster, and activity by Kubernetes user within a cluster. If you run Amazon EKS across many AWS accounts, Detective\u2019s ability to ingest data from the backend and centralize in a centralized Security Account helps reduce time to root cause during a security event. Take a closer look at our Container Security Workshop for more in-depth walkthroughs on how to use Detective for container based workloads.</p>"},{"location":"guides/detective/#getting-started","title":"Getting started","text":""},{"location":"guides/detective/#enable-guardduty","title":"Enable GuardDuty","text":"<p>Enabling Amazon GuardDuty is a pre-requisite before enabling Amazon Detective. Follow the Amazon GuardDuty documentation for enabling GuardDuty and enrolling AWS Accounts in your Organization at scale.</p> <p>If you run Amazon EKS in your environment, it is recommended to enable EKS Audit and Runtime Protection in GuardDuty. In the enablement recommendations below we will discuss consuming EKS audit logs in Detective.</p> <p>It\u2019s recommended to update the GuardDuty CloudWatch Notification Frequency from its default of 6 hours to 15 minutes. If you leave it at 6 hours, it could take updates to recurring findings up to 6 hours to be reflected in Detective. Regardless if you do or do not change this frequency, new findings are always sent to Detective within five minutes of the finding being generated. Shortening this timeframe does not incur any additional costs.  For information about setting the notification frequency, see Monitoring GuardDuty Findings with Amazon CloudWatch Events in the Amazon GuardDuty User Guide.</p>"},{"location":"guides/detective/#where-to-enable-detective","title":"Where to enable Detective","text":"<p>We recommend enabling Amazon Detective in the same Delegated Administrator AWS Account as the rest of your AWS Security Services (GuardDuty, Security Hub, Macie, etc), as outlined in the Security Reference Architecture (SRA) whitepaper. If you don\u2019t have a dedicated Security AWS Account yet, this is the perfect opportunity to create one. If you run AWS Control Tower, you already have one which is often called the \u201cAudit\u201d or \u201cSecurity\u201d Account.</p> <p>Note that Detective currently supports a maximum of 1200 AWS accounts in each behavior graph. If you\u2019re interested in enabling Detective for more than 1200 accounts please reach out to your AWS account team. For other Detective quotas refer to the documentation on regions and quotas.</p>"},{"location":"guides/detective/#region-considerations","title":"Region Considerations","text":"<p>Amazon Detective is a regional service. This means that to use Amazon Detective you will need to enable it in every region that you would like to have incident response capabilities with Detective.</p> <p>A question that is often asked is \u201cShould I use a security service in a region that my company is not actively using?\u201d Detective\u2019s costs are based on the amount of data ingested in a given account and region. If an AWS account or region has no activity, Detective will not incur a cost. It\u2019s recommended to review the Usage section in Detective to determine if the estimated costs are acceptable. We recommend enabling Detective in every AWS account and region that you plan to make available in your AWS Organization. This ensures that if someone attempts to create resources in a region, you will have visibility if needed for any subsequent security issues.</p> <p>You can use the AWS console, AWS CLI, or the Amazon Detective python scripts to enable Detective across accounts in an AWS Organization. Review the scripts and their contents here.</p>"},{"location":"guides/detective/#implementation","title":"Implementation","text":""},{"location":"guides/detective/#enablement","title":"Enablement","text":"<p>To create a Delegated Administrator for Amazon Detective to your desired Security Tooling Account, follow the enablement documentation. Then, using your AWS console, open your Security Tooling account and browse to the Detective service.</p> <p>Detective is very easy to enable. There is nothing to install and the integration with other AWS services is seamless. In the Detective service, enable Detective by clicking on the orange Get Started button.</p> <p></p> <p>Figure 1: Detective getting started page</p> <p>Review the service linked role that Detective will create to understand the permissions needed and then click enable Amazon Detective.</p> <p></p> <p>Figure 2: Enable Detective</p>"},{"location":"guides/detective/#enroll-other-aws-accounts-in-your-organization","title":"Enroll other AWS Accounts in your Organization","text":"<p>After enabling Detective you will need to enroll other AWS Accounts in the Organization. In the Detective console browse to Account Management. From the Account Management page you can enroll all accounts in your organization by selecting the \u201cEnable all accounts\u201c button pictured below.</p> <p></p> <p>Figure 3: Enable all Detective accounts</p> <p>After you have enabled for all accounts in your organization ensure that you have turned on the \"auto-enable\" button as pictured below. This give the Detective delegated administrator the ability to automatically enable Detective in all new accounts that are created in your organization without needing to perform any actions by your team.</p> <p></p> <p>Figure 4: Detective auto enable feature</p>"},{"location":"guides/detective/#enable-optional-data-feeds","title":"Enable Optional Data Feeds","text":"<p>Browse to the General section.</p> <p>If you have just enabled Detective, you will already be ingesting findings from AWS security services and EKS audit logs. If you were using Detective before these integrations were released you will need to enable them manually.</p> <p></p> <p>Figure 5: Detective source packages</p>"},{"location":"guides/detective/#enable-amazon-security-lake-integration","title":"Enable Amazon Security Lake integration","text":"<p>Optionally, you can configure Detective to integrate with Amazon Security Lake to query and retrieve logs stored in Security Lake directly in the Detective console. With this integration, security analysts can start their security investigations based on summaries and visualizations in Detective. In the event they need to dive deeper and retrieve logs, Detective provides a pre-built query using Amazon Athena that is scoped to the time and entities under investigation. You can use this query to get a preview of CloudTrail or VPC flow logs that represent the potential security issue or download all the logs as CSV files.</p> <p> Figure 6: Security Lake integration enablement page</p>"},{"location":"guides/detective/#operationalizing","title":"Operationalizing","text":""},{"location":"guides/detective/#providing-access","title":"Providing Access","text":"<p>Amazon Detective works by consuming VPC Flow Logs, Cloudtrail Logs, EKS Audit Logs, and Security Hub findings and correlating resource IDs with events from GuardDuty into a behavior graph. This graph is what you can follow in Detective when performing an investigation to see which IP relates to which EC2 instance to which EC2 instance profile to which IAM role that created the instance profile, for example.</p> <p></p> <p>Figure 7: Detective workflow diagram</p> <p>As a result, there may be information being fed into Detective in the form of IP addresses, package vulnerabilities, IAM roles and federated users. You will want to apply least privilege to your Incident Investigator IAM Role.</p> <p>Most customers will opt to provide access to their security and incident response teams to the Security Tooling account where many AWS Security Services including Security Hub, Inspector, Macie, and GuardDuty delegated administrators have been assigned. It\u2019s best practice not to provide any team full administrative privileges to the AWS Account, and security teams are no different. We recommend granting incident responders using Detective least privilege rights to do their job, and a great way to start with this is by attaching the AWS IAM Managed Policy AmazonDetectiveInvestigatorAccess to your incident responder\u2019s IAM Role. You can view this policy, and others, on the Settings &gt; General tab.</p> <p></p> <p>Figure 8: Detective IAM policies</p>"},{"location":"guides/detective/#investigating-a-finding","title":"Investigating a Finding","text":"<p>There are four primary ways to start an investigation using Detective. We'll cover these methods in detail below.</p> <ol> <li>Using the Investigate with Detective option from GuardDuty</li> <li>Examining a Finding Group in Detective</li> <li>Threat hunting using Detective's Search and geography graphs</li> <li>Using embedded links in Splunk</li> </ol>"},{"location":"guides/detective/#start-your-investigation-from-guardduty","title":"Start your investigation from GuardDuty","text":"<p>Most customers start investigations after a threat has been detected through Amazon GuardDuty, using the Investigate with Detective option on the Finding. If you don't have any GuardDuty findings present, you can still use Detective to investigate activity or threat hunt, as we\u2019ll explain in method #3 below. When you open a GuardDuty finding, a detail pane will appear on the right. Click the Investigate with Detective option.</p> <p></p> <p>Figure 9: Investigation link in GuardDuty finding detail page</p> <p>You\u2019ll see a panel with all the related resources collected by Detective. Let\u2019s start with the GuardDuty finding.</p> <p></p> <p>Figure 10: Detective investigation options from GuardDuty finding</p> <p>From here we can see the VPC ID, the subnet ID, and other related information.</p> <p></p> <p>Figure 11: Entities related to GuardDuty Finding</p> <p>Let\u2019s browse into the EC2 instance and see the related information.</p> <p></p> <p>Figure 12: Instance profile page in Detective</p> <p>From here you will be able to determine when the EC2 was built, the AWS Account it resides in, the IAM Role or User that created the EC2 instance.</p> <p>If we click into the Created By IAM Role, we can look at the Resource Interaction tab, identify the IAM identity that created this EC2 instance, when the first observance of the user was, the most recent occurrence, and the various times this user has assumed this role during the scope time.</p> <p></p> <p>Figure 13: Role profile page in Detective</p> <p>For the sake of this walkthrough let\u2019s assume you have noticed a strange occurence with the role assumptions of the user in question. With that in mind at this point you may want to execute your incident response plan, isolate and snapshot the EC2 instance, revoke credentials for past sessions on the IAM Role, and further investigate the federated user who created the instance. If your organization does not currently have incident response playbooks, there are publicly available samples to help you get started.</p>"},{"location":"guides/detective/#start-your-investigation-from-a-finding-groups","title":"Start your investigation from a Finding Groups","text":"<p>Amazon Detective Finding Groups enable you to examine multiple activities as they relate to a single security event. If a threat actor is attempting to compromise your AWS environment, they typically perform a sequence of actions that lead to multiple security findings and unusual behaviors. These actions are often spread across time and entities. When security findings are investigated in isolation, it can lead to a misinterpretation of their significance and difficulty in finding the root cause. Amazon Detective addresses this problem by applying a graph analysis technique that infers relationships between findings and entities, and groups them together. We recommend treating Finding Groups as the starting point for investigating the involved entities and findings. While GuardDuty can show you the threat event, Detective can tell you the story of what else happened in the environment related to the event.</p> <p>When we first navigate to a Finding Group (example image below), we can see that we are presented with a generative AI powered summary of the Finding Group. This gives us an understanding of the activity associated with the Finding Group before we even dive into the data.</p> <p> Figure 14: Detective generative AI Finding Group summary</p> <p>Detective will attempt to correlate threat events and resources to the MITRE ATT&amp;CK framework. MITRE publishes a matrix of Tactics, Techniques and Procedures (TTPs) that malicious actors perform when attacking a system. These correlated TTPs are published by Detective into a Finding Group. This is a visual representation of the events, resources, and security findings that make up an event.</p> <p></p> <p>Figure 15: TTPs associated with Detective Finding Group</p> <p>The Finding Group is represented by this generated graph that provides a visualization of associated resources and how they are connected. In this example, there are many different IP addresses associated with a brute force GuardDuty finding, and some of these resources are associated with other GuardDuty findings, like this Domain Generation Algorithm (DGA) request. DGAs are used to periodically generate a large number of domain names that can be used as rendezvous points with their command and control (C&amp;C) servers. During a GuardDuty finding investigation, it might not be clear that there are other resources included in the overall security issue. Detective Finding Groups help connect these dots.</p> <p></p> <p>Figure 16: Detective Finding Group graph visualization</p> <p>Use these Finding Groups to investigate other IPs, EC2s, or AWS accounts from where an attack originated or continued onto.</p> <p>The data correlating events to resources is available in raw data if you are collecting these data sources individually, but security analysts need to know how to craft individual queries to crawl through logs and then graph to a visualization tool later. Detective short-circuits this investigative effort and query generation activity. While most customers don\u2019t replace their traditional SIEM tools with Detective, the use of Detective with a SIEM is effective at reducing investigation times.</p> <p>Compare Detective's effectiveness to collecting and aggregating your logs separately. If you are collecting data sources individually and storing in a log aggregation tool, your security analysts will need the expertise to craft queries to find the data they need, and graph to a visualization tool separately. Detective short-circuits this alert --&gt; log --&gt; query --&gt; visualize process by providing you a visualization using built-in AWS data feeds. While most customers don\u2019t replace their traditional SIEM tools with Detective, the use of Detective with a SIEM is effective at reducing investigation times.</p>"},{"location":"guides/detective/#threat-hunting-in-detective","title":"Threat Hunting in Detective","text":"<p>Customers can use Detective if they have interesting information that they want to try to correlate to AWS resources or events. Commonly, Detective is used to investigate where your AWS resources are communicating with suspicious public IPs. Browse to Amazon Detective console, on the left navigation pane click the Search option. You can search for many kinds of resources including EC2 instances, IP addresses, Kubernetes Subjects, or Container Images.</p> <p></p> <p>Figure 17: Detective search options</p> <p></p> <p>Figure 18: Searching for IP in Detective</p> <p>Important Note: With regards to IP addresses, it\u2019s important to remember which IPs are yours vs public IPs. Most customer IPv4 addresses are in the RFC1918 range (10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16). If you are using public IPs in your IP schema for your VPCs, you may need to perform lookups occasionally to determine who owns the IP.</p> <p>In addition to searching for information Detective Investigations lets you investigate IAM users and IAM roles using Indicators of Compromise (IoC). IoCs can help you determine if a resource is involved in a security incident. Detective investigations enables you to investigate attack tactics including impossible travel, \ufb02agged IP addresses, and Finding Groups. It performs initial security investigation steps and generates a report highlighting the risks identified by Detective to help you understand security events and respond.</p> <p>To start an investigation, select Run Investigation. Choose your desired date/time and confirm.</p> <p> Figure 19: Start investigation button on IAM role entity page</p> <p> Figure 20: Confirm investigation settings</p> <p>Once the investigation has run, new information will be available including TTPs, threat intelligence flagged IP addresses, impossible travel, related Finding Groups, related findings, new geolocations, new user agents, and new ASOs. All information will be provided in the console and can be downloaded in a JSON format.</p> <p> Figure 21: Detective sample investigation results</p>"},{"location":"guides/detective/#crafting-detective-embedded-urls","title":"Crafting Detective Embedded URLs","text":"<p>The fourth way to start investigating a security event with Detective is to use embedded links in Splunk through the Splunk Trumpet project. The Splunk Trumpet project allows you send data from AWS services to Splunk. You can configure the Trumpet project to generate Detective URLs for Amazon GuardDuty findings. You can then use these URLs to pivot directly from Splunk to the corresponding Detective finding profiles. The Trumpet project is available from GitHub.</p> <p></p> <p>Figure 22: Crafting Detective embedded URLs</p>"},{"location":"guides/detective/#cost-considerations","title":"Cost Considerations","text":"<p>Amazon Detective pricing is thoroughly covered in the pricing page, including dimensions used for pricing and some pricing examples. In this guide, we'll cover some options to control your Detective spend.</p> <ol> <li>Take advantage of the 30-day free trial to get an idea of how pricing works with Detective. After enabling Detective, come back in a few days and check the Usage tab on the left.</li> <li>In the Detective usage page you can view the estimated costs under the This account\u2019s projected costs and All accounts\u2019 projected costs. You can further understand which accounts are driving costs by using the Ingested volume by member account option.</li> <li>Some customers choose to exclude AWS accounts that are classified as sandbox or non-production as a way to control their cost. Also consider if you have supplementary tools that ingest VPC Flow Logs and AWS CloudTrail data separately that could be scaled down and augmented with Detective.</li> </ol> <p></p> <p>Figure 23: Detective usage page</p>"},{"location":"guides/detective/#resources","title":"Resources","text":""},{"location":"guides/detective/#workshops","title":"Workshops","text":"<ul> <li>Activation Days</li> <li>Amazon GuardDuty workshop</li> <li>Amazon Detective workshop</li> <li>EKS security workshop</li> </ul>"},{"location":"guides/detective/#videos","title":"Videos","text":"<ul> <li>AWS re:Inforce 2022 - Using Amazon Detective to improve security investigations(TDR302)</li> <li>AWS re:Inforce 2023 - Streamline security analysis with Amazon Detective (TDR210)</li> <li>How to use Amazon Detective for security investigations</li> <li>Detective Finding Groups include Amazon Inspector findings * Investigations for Amazon GuardDuty threat detections</li> <li>Using Amazon Detective to perform root cause analysis for security findings</li> <li>Amazon Detective Visualizations demo</li> <li>Reduce time to investigate GuardDuty findings by grouping related findings</li> <li>Security Scenario investigation walk through</li> <li>Deploying Amazon Detective with AWS Organizations</li> </ul>"},{"location":"guides/detective/#blogs","title":"Blogs","text":"<ul> <li>How to improve security incident investigations using Amazon Detective Finding Groups</li> <li>Improve your security investigations with Detective Finding Groups visualizations</li> <li>How to detect security issues in Amazon EKS clusters using Amazon GuardDuty \u2013 Part 1</li> <li>How to detect security issues in Amazon EKS clusters using Amazon GuardDuty \u2013 Part 2</li> </ul>"},{"location":"guides/dns-firewall/","title":"DNS Firewall Best Practices Guide","text":""},{"location":"guides/dns-firewall/#overview","title":"Overview","text":"<p>Amazon Route 53 Resolver DNS Firewall is a managed firewall service that enables you to control and filter outbound DNS queries from your VPCs. It helps protect your workloads against DNS-based threats by allowing you to block DNS queries made to known malicious domains and exfiltration attempts using DNS protocols.</p>"},{"location":"guides/dns-firewall/#what-are-the-benefits-of-enabling-amazon-route-53-resolver-dns-firewall","title":"What are the benefits of enabling Amazon Route 53 Resolver DNS Firewall?","text":"<p>Enabling Amazon Route 53 Resolver DNS Firewall offers several key benefits:</p> <ul> <li>Enhanced Security: Protect your VPCs against DNS-based threats, including malware, phishing, and command-and-control attacks.</li> <li>Reduced Operational Overhead: Leverage AWS-managed domain lists that are automatically updated, reducing the burden on your security team.</li> <li>Customizable Protection: Create and manage custom domain lists to address specific security requirements or block known threats.</li> <li>Advanced Threat Detection: Utilize DNS Firewall Advanced rule groups to protect against sophisticated DNS attacks like tunneling and exfiltration.</li> <li>Centralized Management: When used with AWS Firewall Manager, easily deploy and manage DNS Firewall rules across multiple accounts and VPCs.</li> <li>Cost Optimization: By filtering malicious traffic at the DNS layer, reduce unnecessary data processing costs on downstream security controls like Network Firewall.</li> <li>Seamless Integration: Easily integrate with existing AWS services and your current security architecture.</li> <li>Scalability: Automatically scales to handle your DNS traffic without requiring additional infrastructure management.</li> </ul> <p>By implementing Route 53 Resolver DNS Firewall, organizations can significantly enhance their security posture and protect their AWS resources from DNS-based threats.</p>"},{"location":"guides/dns-firewall/#best-practices","title":"Best Practices","text":""},{"location":"guides/dns-firewall/#implement-layer-of-defense-with-aws-managed-domain-lists","title":"Implement Layer of Defense with AWS-Managed Domain Lists","text":"<ul> <li>Utilize AWS-managed domain lists as your first line of defense</li> <li>These lists are automatically updated by AWS Security</li> </ul> <p>Reference: AWS Managed Domain Lists Documentation</p>"},{"location":"guides/dns-firewall/#leverage-dns-firewall-advanced-rule-groups","title":"Leverage DNS Firewall Advanced Rule Groups","text":"<ul> <li>Implement DNS Firewall Advanced rule groups to protect against:<ul> <li>DNS tunneling</li> <li>Domain Generation Algorithms</li> </ul> </li> </ul> <p>Reference: DNS Firewall Advanced Features</p>"},{"location":"guides/dns-firewall/#centralize-management-with-aws-firewall-manager","title":"Centralize Management with AWS Firewall Manager","text":"<ul> <li>Use AWS Firewall Manager to:<ul> <li>Deploy DNS Firewall rules consistently across your organization</li> <li>Automatically protect new VPCs as they are created</li> <li>Centrally manage rules across accounts</li> </ul> </li> </ul> <p>Firewall Manager Documentation</p>"},{"location":"guides/dns-firewall/#enable-and-configure-dns-query-logging","title":"Enable and Configure DNS Query Logging","text":"<ul> <li>Enable DNS query logging for:<ul> <li>Security investigation and threat hunting</li> <li>Traffic pattern analysis</li> <li>Configure logging to Amazon CloudWatch Logs or S3</li> <li>Set up appropriate log retention policies</li> </ul> </li> </ul> <p>Reference: DNS Query Logging Configuration</p>"},{"location":"guides/dns-firewall/#block-malicious-traffic-closer-to-the-source","title":"Block Malicious Traffic Closer to the Source","text":"<ul> <li>Use DNS Firewall as early filtering mechanism</li> <li>Block malicious traffic at DNS layer before reaching Network Firewall</li> <li>Reduce unnecessary data processing costs</li> <li>Implement in conjunction with other security controls</li> </ul>"},{"location":"guides/dns-firewall/#implementation-guidance","title":"Implementation Guidance","text":""},{"location":"guides/dns-firewall/#initial-setup","title":"Initial Setup","text":"<ol> <li>Create a DNS Firewall rule group</li> <li>Associate AWS-managed domain lists and DNS Firewall Advanced rules</li> <li>Configure custom domain lists if needed</li> <li>Create any custom rules with appropriate actions (ALLOW, ALERT, BLOCK)</li> <li>Associate the rule group with VPCs</li> </ol> <p>Reference: Getting Started Guide</p>"},{"location":"guides/dns-firewall/#monitoring-and-maintenance","title":"Monitoring and Maintenance","text":"<ul> <li>Regular review of DNS query logs</li> <li>Review and adjust rule configurations</li> <li>Validate rule effectiveness</li> </ul>"},{"location":"guides/dns-firewall/#recommended-rule-group-configuration","title":"Recommended Rule Group Configuration","text":"<ul> <li>Refer to this link for a recommended DNS Firewall rule group configuration: Recommended Rule Group Configuration </li> </ul>"},{"location":"guides/dns-firewall/#additional-resources","title":"Additional Resources","text":"<ul> <li>DNS Firewall Overview</li> <li>Automated Allow List Generator Solution</li> <li>AWS Security Blog - Protect Against Advanced DNS Threats</li> <li>Domain Lists Management Documentation</li> </ul>"},{"location":"guides/guardduty/","title":"Amazon GuardDuty","text":""},{"location":"guides/guardduty/#introduction","title":"Introduction","text":"<p>Welcome to the Amazon GuardDuty Best Practices Guide. The purpose of this guide is to provide prescriptive guidance for leveraging Amazon GuardDuty for continuous monitoring of your AWS accounts and resources. Publishing this guidance via GitHub will allow for quick iterations to enable timely recommendations that include service enhancements, as well as, the feedback of the user community. This guide is designed to provide value whether you are deploying GuardDuty for the first time in a single account, or looking for ways to optimize GuardDuty in an existing multi-account deployment.</p>"},{"location":"guides/guardduty/#how-to-use-this-guide","title":"How to use this guide","text":"<p>This guide is geared towards security practitioners who are responsible for monitoring and remediation of threats and malicious activity within AWS accounts (and resources). The best practices are organized into three categories for easier consumption. Each category includes a set of corresponding best practices that begin with a brief overview, followed by detailed steps for implementing the guidance. The topics do not need to be read in a particular order:</p> <ul> <li>Deploying GuardDuty<ul> <li>Single account deployment</li> <li>Multi-account deployment<ul> <li>Enablement of a delegated administrator</li> <li>Configuring auto-enable preferences for organization</li> <li>Add accounts as members to your organization</li> </ul> </li> <li>GuardDuty protection plans<ul> <li>Enabling S3 malware protection</li> <li>Runtime Monitoring<ul> <li>Enabling runtime monitoring for EC2</li> <li>Enabling runtime monitoring for ECS</li> <li>Enabling runtime monitoring for EKS </li> </ul> </li> </ul> </li> </ul> </li> <li>Operationalizing GuardDuty findings<ul> <li>Filtering findings</li> <li>Reducing potential noise</li> <li>Automated notification for high priority findings</li> <li>Automated remediation for common finding types</li> </ul> </li> <li>Cost optimization<ul> <li>CloudTrail and/or S3 data event utilization is high</li> <li>VPC Flow Logs</li> <li>DNS Query Logs</li> <li>Other considerations</li> </ul> </li> <li>Troubleshooting</li> <li>Resources</li> </ul>"},{"location":"guides/guardduty/#what-is-amazon-guardduty","title":"What is Amazon GuardDuty?","text":"<p>GuardDuty is an intelligent threat detection service that continuously monitors your AWS accounts, Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS Lambda functions, Amazon Elastic Kubernetes Service (Amazon EKS) clusters, Amazon Elastic Container Service (Amazon ECS), Amazon Aurora login activity, and data stored in Amazon Simple Storage Service (Amazon S3) for malicious activity. If potential malicious activity, such as anomalous behavior, credential exfiltration, or command and control infrastructure (C2) communication is detected, GuardDuty generates detailed security findings that can be used for security visibility and assisting in remediation. Additionally, using the Amazon GuardDuty Malware Protection feature helps to detect malicious files on Amazon Elastic Block Store (Amazon EBS) volumes attached to Amazon EC2 instance and container workloads.</p> <p>GuardDuty monitors Foundational data sources such as AWS CloudTrail event logs, AWS CloudTrail management events, Amazon VPC Flow Logs, and DNS logs. Enablement of the Foundational data sources is not required. Amazon GuardDuty pulls independent streams of data directly from those services.</p> <p>In addition to the foundational data sources, GuardDuty can use additional data from other AWS services in your AWS environment to monitor and provide analysis for potential security threats. Those services include:</p> <ul> <li>Amazon EKS \u2013 GuardDuty monitors EKS Audit Logs and operating system-level events via the GuardDuty security agent.</li> <li>AWS Lambda \u2013 GuardDuty monitors network activity logs, including VPC Flow Logs for suspicious network traffic.</li> <li>Amazon EC2 and Containers \u2013 GuardDuty monitors its findings for indicators of malware presence on attached Amazon Elastic Block Store (Amazon EBS) volumes, e.g. bitcoin mining activity.</li> <li>Amazon Aurora \u2013 GuardDuty monitors and profiles relational database service login activity for potential threats.</li> <li>Amazon S3 \u2013 GuardDuty monitors AWS CloudTrail S3 data events to identify potential threats in your Amazon S3 resources. AWS CloudTrail S3 management events are monitored by default after GuardDuty is enabled.</li> <li>Runtime Monitoring - Runtime Monitoring observes and analyzes operating system-level, networking, and file events to help you detect potential threats in specific AWS workloads in your environment.</li> <li>S3 Malware Protection - Malware Protection for S3 helps you detect potential presence of malware by scanning newly uploaded objects to your selected Amazon Simple Storage Service (Amazon S3) bucket. When an S3 object or a new version of an existing S3 object gets uploaded to your selected bucket, GuardDuty automatically starts a malware scan. It is important to note that S3 malware protection is not intended to be deployed across your entire S3 estate. S3 Malware protection is purpose built to provide a cost effective solution that will scan objects that are uploaded to untrusted buckets. For example, in a scenario where you have third parties sending you documents or files and you need to ensure there is no malware before processing them in an application.</li> </ul>"},{"location":"guides/guardduty/#what-are-the-benefits-of-enabling-guardduty","title":"What are the benefits of enabling GuardDuty?","text":"<p>GuardDuty is designed to operate completely independently from your resources and have no performance or availability impact to your workloads. The service is fully managed with integrated threat intelligence, machine learning (ML) anomaly detection, and malware scanning. GuardDuty delivers detailed and actionable alerts that are designed to be integrated with existing event management and workflow systems. There are no upfront costs and you pay only for the events analyzed, with no infrastructure to manage or threat intelligence feed subscriptions required.</p>"},{"location":"guides/guardduty/#deploying-guardduty","title":"Deploying GuardDuty","text":""},{"location":"guides/guardduty/#single-account-deployment","title":"Single account deployment","text":"<p>The minimum requirements for enabling GuardDuty as a standalone account or as a GuardDuty administrator with AWS Organizations are covered below.</p> <p>The first step to using GuardDuty is to enable it in your account. Once enabled, GuardDuty will immediately begin to monitor for security threats in the current Region. For a standalone account:</p> <ol> <li>Open the GuardDuty console at: https://console.aws.amazon.com/guardduty/</li> <li>Choose Get Started.  Figure 1: GuardDuty landing page</li> <li>Choose Enable GuardDuty.  Figure 2: GuardDuty enablement</li> </ol> <p>Once these steps are complete GuardDuty will begin collecting log data and monitoring the environment for malicious or suspicious activity.</p>"},{"location":"guides/guardduty/#s3-malware-protection-single-account-deployment","title":"S3 Malware Protection single account deployment","text":"<p>Although we recommend using GuardDuty across all of your AWS environment S3 Malware protection can be enabled independently from the rest of GuardDuty on a single account. Refer to the documentation on Get started with Malware Protection for S3 only to learn more.</p> <p> Figure 3: S3 Malware Protection only</p>"},{"location":"guides/guardduty/#multi-account-deployment","title":"Multi-account deployment","text":"<p>As prerequisites for this process, you must be in the same organization as all the accounts you want to manage, and have access to the AWS Organizations management account in order to delegate an administrator for GuardDuty within your organization. Additional permissions may be required to delegate an administrator, for more information see Permissions required to designate a delegated administrator.</p>"},{"location":"guides/guardduty/#enablement-of-a-delegated-administrator","title":"Enablement of a delegated administrator","text":"<ol> <li>Open the AWS Organizations console at https://console.aws.amazon.com/organizations/, using the management account.</li> <li>Open the GuardDuty console at https://console.aws.amazon.com/guardduty/.</li> </ol> <p>Is GuardDuty already enabled in your account?</p> <ul> <li>If GuardDuty is not already enabled, you can select Get Started and then designate a GuardDuty delegated administrator on the Welcome to GuardDuty page.</li> <li> <p>If GuardDuty is enabled, you can designate a GuardDuty delegated administrator on the Settings page.</p> </li> <li> <p>Enter the twelve-digit AWS account ID of the account that you want to designate as the GuardDuty delegated administrator for the organization and choose Delegate.</p> </li> </ul> <p> Figure 4: GuardDuty auto-enable off</p>"},{"location":"guides/guardduty/#configuring-auto-enable-preferences-for-organization","title":"Configuring auto-enable preferences for organization","text":"<ol> <li>Open the GuardDuty console at https://console.aws.amazon.com/guardduty/.</li> <li>In the navigation pane, choose Accounts.</li> <li>The Accounts page provides configuration options to the GuardDuty administrator to Auto-enable GuardDuty and the optional protection plans on behalf of the member accounts that belong to the organization. You can find this option next to Add accounts by invitation.</li> </ol> <p>This support is available to configure GuardDuty and all the supported optional protection plans in your AWS Region. You can select one of the following configuration options for GuardDuty on behalf of your member accounts:</p> <ul> <li>Enable for all accounts \u2013 Select to enable the corresponding option for all the member accounts in an organization automatically. This includes new accounts that join the organization and those accounts that may have been suspended or removed from the organization.</li> <li>Auto-enable for new accounts \u2013 Select to enable GuardDuty for new member accounts automatically when they join your organization.</li> <li>Do not enable \u2013 Select to prevent enabling the corresponding option for any account in your organization. In this case, the GuardDuty administrator will manage each account individually.</li> </ul> <p></p> <p>Figure 5: GuardDuty auto-enable preferences</p> <ol> <li>Choose Save Changes</li> </ol>"},{"location":"guides/guardduty/#add-accounts-as-members-to-your-organization","title":"Add accounts as members to your organization","text":"<p>This procedure covers adding members accounts to a GuardDuty delegated administrator account through AWS Organizations. To learn more about associating members in GuardDuty, see Managing multiple accounts in Amazon GuardDuty AWS service integrations with GuardDuty.</p> <ol> <li>Log in to the delegated administrator account</li> <li>Open the GuardDuty console at https://console.aws.amazon.com/guardduty/.</li> <li>In the navigation panel, choose Accounts. The accounts table displays all of the accounts that are added either Via Organizations or By invitation.</li> <li>Select one or multiple account IDs that you want to add as members. Thses account IDs must have Type as Via Organizations.</li> <li>You can select the down arrow of the Status column to sort the accounts by the Not a member status and then choose each account that does not have GuardDuty enable in the current Region.</li> <li>Choose Confirm to add the accounts as members. This action also enables GuardDuty for all of the selected accounts. The Status for the accounts will change to Enabled.</li> <li>(Recommended) Repeat these steps in each AWS Region. This ensures that the delegated administrator can manage findings and other configurations for member accounts in all the Regions where you have GuardDuty enabled.</li> </ol>"},{"location":"guides/guardduty/#guardduty-protection-plans","title":"GuardDuty protection plans","text":"<p>After enabling GuardDuty in your account(s), choosing additional protection types is highly recommended. GuardDuty protection plans are additional features that add focused threat detection for Amazon EKS, Amazon S3, Amazon Aurora, Amazon EC2, Amazon ECS, and AWS Lambda. To learn more about the benefits of what each GuardDuty protection provides, refer to the protection section of the Amazon GuardDuty User Guide.</p>"},{"location":"guides/guardduty/#s3-malware-protection","title":"S3 Malware Protection","text":"<p>You can enable S3 Malware protection through the console, CLI, API, or through infrastructure as code such as CloudFormation or Terraform. Keep in mind that you enable this specifically for a bucket at a time and not across an entire account or accounts. Refer to the deployment documentation for the exact steps but we have placed images below for you to get an idea of what the deployment looks like.</p> <p>Before deployment you need to ensure that a role exists that gives GuardDuty access to scan S3 objects in the bucket, including the ability to allow KMS key action, and that the bucket is not explicitly denying access to this role. For more information on creating this role and support encryption mechanisms look at the prerequisite documentation and the quota documentation.</p> <p>When deploying you will need to determine if you want GuardDuty to tag objects based on if there was malware found (NO_THREATS_FOUND, THREATS_FOUND, UNSUPPORTED, ACCESS_DENIED, or FAILED). This tagging allows you to create automated workflows depending on the tag. We give an example of this in the S3 Malware automation workflow section.</p> <p> Figure 7: S3 Malware Protection console</p> <p> Figure 8: S3 Malware Protection configuration</p>"},{"location":"guides/guardduty/#runtime-monitoring","title":"Runtime Monitoring","text":"<p>For GuardDuty runtime monitoring protections including GuardDuty runtime monitoring for EKS, EC2, and ECS you have the ability to further scope what resources you want coverage for and how you would like to deploy the runtime agent. We recommend allowing GuardDuty to deploy runtime monitoring which will deploy a VPC endpoint and agent for EC2, ECS, and EKS using an agent, sidecar, or EKS managed add-on, respectively. This will ensure coverage across your current resources but also apply to new resources that are created in the future. This agent is built on EBPF technology. Using the GuardDuty automation saves manual effort needed to address new resource coverage across your organization. However if you can choose to you can manage this configuration yourself.</p> <p>If choose to allow GuardDuty to deploy the needed resources to cover both current and future VPC endpoints and agents needed to provide runtime monitoring in your environment you can further scope which resources the runtime monitoring applies to by using inclusion or exclusion tags. For example, if you have a handful of EKS clusters that you don't want to monitor, than it would be more efficient to use an exclusion tag rather than the opposite scenario where you only want to apply monitoring to a few resources where you would want to use inclusion tags. The GuardDuty documentation provides use case examples and specific tags needed to implement this functionality.</p> <p>When configuring GuardDuty runtime monitoring it is important to understand the prequisites for runtime monitoring. This will give you information on supported OS's, Kernel version, CPU and Memory limits for the GuardDuty agent and more. After deployment ensure you assess your coverage of the runtime monitoring deployment to address any issues.</p>"},{"location":"guides/guardduty/#runtime-monitoring-deployment-for-ec2","title":"Runtime Monitoring Deployment for EC2","text":"<p>This deployment guide assumes the following:</p> <ul> <li>EC2 instances are running on supported Amazon Machine Images (AMIs), instance types, and operating systems.</li> <li>Foundational GuardDuty coverage is already enabled for your AWS Organization.</li> </ul> <p>For more detailed information on the prerequisites, refer here.</p> <p>By following the steps outlined in this guide, you will learn how to leverage GuardDuty Runtime Monitoring to enhance the security posture of your EC2 instances, enabling real-time threat detection and response capabilities within your AWS environment.</p>"},{"location":"guides/guardduty/#configure-aws-systems-manager-ssm-default-host-management","title":"Configure AWS Systems Manager (SSM) Default Host Management","text":"<p>The Amazon EC2 instances for which you want GuardDuty to monitor runtime events must be AWS Systems Manager (SSM) managed. With Quick Setup, a capability of AWS Systems Manager, you can activate Default Host Management Configuration for all accounts and Regions that have been added to your organization in AWS Organizations. This ensures that SSM Agent is kept up to date on all Amazon Elastic Compute Cloud (EC2) instances in the organization, and that they can connect to Systems Manager.</p> <ol> <li>Visit the AWS Systems Manager console in the Organization management account.</li> <li>In the left-hand navigation pane, choose Quick Setup.</li> <li>Under \u201cDefault Host Management Configuration\u201d, select \u201cCreate\u201d.</li> <li>Leave the options as default, and select \u201cCreate\u201d.</li> </ol> <p>Assuming the AWS Systems Manager (SSM) Agent is already running on the EC2 instance when you configured Default Host Management, you may need to wait up to 30 minutes for the agent to connect with Systems Manager and the EC2 instance to appear in Fleet Manager as a managed instance. At that point, Systems Manager will install the Amazon GuardDuty Runtime Monitoring SSM Plugin on the instance.</p>"},{"location":"guides/guardduty/#validate-ssm-host-management","title":"Validate SSM Host Management","text":"<p>Remember, it normally takes 30 minutes for hosts to show in SSM after enabling Default Host Management. </p> <ol> <li>Visit the AWS Systems Manager console, in the left-hand navigation pane, under the Node Management section, choose Fleet Manager.</li> <li>Confirm that you can see your EC2 instances in Fleet Manager. Alternatively, you can use AWS Resource Explorer to search and discover resources in your AWS account or across an entire AWS organization. </li> </ol> <p>If you do not see your EC2 instances in Fleet Manager after 30 minutes of enabling Default Host Management, refer to the SSM Agent Troubleshooting section at the bottom of this page.</p>"},{"location":"guides/guardduty/#enabling-guardduty-runtime-monitoring-for-ec2","title":"Enabling GuardDuty Runtime Monitoring for EC2","text":"<ol> <li>Visit the Amazon GuardDuty console in your GuardDuty delegated administrator account.</li> <li>In the left hand navigation pane, under the Protection plans section, choose Runtime Monitoring.</li> <li>Under Runtime Monitoring configuration, click Enable. Click Confirm when prompted.</li> <li>Under Automated agent configuration, click Enable for all accounts (recommended).</li> <li>Toggle to the tab Runtime coverage and then open the tab, EC2 instance runtime coverage. Within 5 minutes, EC2 instances will start to show a \"Healthy\" status. It may take up to 10 minutes for runtime monitoring to be \"Healthy\" on EC2 instances already meeting the prerequisite configuration.</li> <li>Repeat steps 3-5 for all Regions enabled for your AWS Organization.</li> </ol>"},{"location":"guides/guardduty/#runtime-monitoring-deployment-for-ecs","title":"Runtime Monitoring Deployment for ECS","text":"<p>This deployment guide assumes the following:</p> <ul> <li>Foundational GuardDuty coverage is already enabled for your AWS Organization.</li> <li>You have access to the delegated GuardDuty administrator account.</li> <li>Fargate platform running on supported version (at least 1.4.0 or LATEST).</li> <li>EC2 container instances are running on supported ECS-AMIs, instance types, and operating systems.</li> </ul> <p>For more information on the prerequisites for Fargate (ECS only) cluster, refer here. </p> <p>For more information on the prerequisites for Amazon ECS running on Amazon EC2, refer here.</p> <p>By following the steps outlined in this guide, you will learn how to leverage GuardDuty Runtime Monitoring to enhance the security posture of your ECS clusters, enabling real-time threat detection and response capabilities within your AWS environment. For more information on how Runtime Monitoring works with Fargate (Amazon ECS only), refer here. </p> <ol> <li>Open the Runtime Monitoring page in Amazon GuardDuty in the delegated GuardDuty administrator account.</li> <li>Under Runtime Monitoring configuration, click Enable for all accounts. Click Save when prompted. This will turn on GuardDuty ECS monitoring in all accounts.</li> <li>Under Automated agent configuration, click Enable for all accounts for AWS Fargate (ECS only). GuardDuty deploys and manages the agent in your ECS clusters on AWS Fargate, on your behalf. </li> <li>Toggle to the tab Runtime coverage and then open the tab, ECS clusters runtime coverage. </li> </ol> <p>When you want GuardDuty to monitor tasks that are part of a service, it requires a new service deployment after you enable Runtime Monitoring. If the last deployment for a specific ECS service was started before you enabled Runtime Monitoring, you can either restart the service, or update the service by using <code>forceNewDeployment</code>.</p> <p>For steps to update the service, see the following resources: * Updating an Amazon ECS service using the console in the Amazon Elastic Container Service Developer Guide. * UpdateService in the Amazon Elastic Container Service API Reference. * update-service in the AWS CLI Command Reference.</p>"},{"location":"guides/guardduty/#runtime-monitoring-deployment-for-eks","title":"Runtime Monitoring Deployment for EKS","text":"<p>This deployment guide assumes the following:</p> <ul> <li>Amazon EKS clusters are running on Amazon EC2 instances. GuardDuty doesn't support Amazon EKS clusters running on AWS Fargate.</li> <li>The Amazon EKS cluster must be running on a supported operating system and Kubernetes version.</li> <li>Foundational GuardDuty coverage is already enabled for your AWS Organization.</li> <li>You have access to the delegated GuardDuty administrator account.</li> </ul> <p>For more detailed information on the prerequisites, refer here.</p> <p>By following the steps outlined in this guide, you will learn how to leverage GuardDuty Runtime Monitoring to enhance the security posture of your EKS clusters, enabling real-time threat detection and response capabilities within your AWS environment. For more information on how Runtime Monitoring works with Amazon EKS clusters, refer here. </p> <p>In a multiple-account environments, only the delegated GuardDuty administrator account can enable or disable Automated agent configuration for the member accounts, and manage Automated agent for the EKS clusters belonging to the member accounts in their organization. The GuardDuty member accounts can't modify this configuration from their accounts. The delegated GuardDuty administrator account account manages their member accounts using AWS Organizations. For more information about multi-account environments, see Managing multiple accounts.</p> <ol> <li>Open the Runtime Monitoring page in Amazon GuardDuty.</li> <li>Under Runtime Monitoring configuration, click Enable for all accounts. Click Save when prompted.</li> <li>Under Automated agent configuration, click Enable for all accounts for Amazon EKS. GuardDuty deploys and manages the agent in your EKS clusters on AWS Fargate, on your behalf. </li> <li>Toggle to the tab Runtime coverage and then open the tab, EKS clusters runtime coverage. </li> </ol>"},{"location":"guides/guardduty/#operationalize-guardduty-findings","title":"Operationalize GuardDuty findings","text":"<p>After GuardDuty has been enabled in your account(s), GuardDuty will begin monitoring the foundational data sources and analyzing features associated with optionally enabled resource protection types. A best practice recommendation after enabling GuardDuty is to leverage the 30-day trial (enabled per account) to understand the baseline of normal activity, derived by machine learning, for your accounts and resources. During the trial period GuardDuty will also use threat-based and rules-based intelligence to generate findings in near-real time.</p> <p>Potential security issues are presented as findings in the GuardDuty console. All GuardDuty findings are assigned a severity level (low, medium, high) and corresponding value (1.0 \u2013 8.9) based on potential risks. Higher value findings indicate a greater security risk. The severity level assigned to a finding is provided to help you determine a response to a potential security issue that is highlighted by a finding.  Figure 9: GuardDuty severity levels</p> <p>It is recommended to familiarize your team with the GuardDuty Finding Types. This will give you insights into what findings you might see in your account, the details associated with these findings, and potential remediation actions. After you understand different GuardDuty findings we encourage customers to think through filtering, notifications, and potential automatic remediation which will cover in the next sections. It is also encouraged to begin writing incident response playbooks for responding to GuardDuty findings in your environment. This will likely combine multiple steps below and also details not covered that might be specific to your environment and your teams.</p>"},{"location":"guides/guardduty/#amazon-guardduty-extended-threat-detection","title":"Amazon GuardDuty Extended Threat Detection","text":"<p>GuardDuty Extended Threat Detection automatically detects multi-stage attacks that span data sources, multiple types of AWS resources, and time, within an AWS account. With this capability, GuardDuty focuses on the sequence of multiple events that it observes by monitoring different types of data sources. Extended Threat Detection correlates these events to identify scenarios that present themselves as a potential threat to your AWS environment, and then generates an attack sequence finding.</p> <p>A single finding can encompass an entire attack sequence. For example, it might detect a scenario such as:</p> <ol> <li>A threat actor gaining unauthorized access to a compute workload. </li> <li>The actor then performing a series of actions such as privilege escalation and establishing persistence. </li> <li>Finally, the actor exfiltrating data from an Amazon S3 resource.</li> </ol> <p>Extended Threat Detection covers threat scenarios that involve compromise related to AWS credentials misuse, and data compromise attempts in your AWS accounts. Because of the nature of these threat scenarios, GuardDuty considers all attack sequence finding types as Critical.</p> <p>Few Considerations</p> <p>Automatic Activation:</p> <ol> <li>ETD is automatically turned on for all GuardDuty accounts.</li> <li>No manual activation is required for ETD.</li> <li>TD is at no additional cost.</li> </ol> <p>S3 Protection:</p> <ol> <li>Remains a separate feature from ETD.</li> <li>Must be enabled if desired.</li> <li>Not automatically activated with ETD.</li> </ol> <p>Functionality:</p> <ol> <li>ETD enhances GuardDuty's threat detection capabilities automatically.</li> <li>Works alongside other GuardDuty features, including optional S3 Protection.</li> </ol> <p>Visit the Extended Threat Detection page under Protection plans in the GuardDuty console. Make sure that Extended Threat Detection is enabled.</p>"},{"location":"guides/guardduty/#filtering-findings","title":"Filtering findings","text":"<p>If you are new to GuardDuty the easiest way to view/filter your findings is via the Summary tab. The Summary dashboard displays an aggregated view of the last 10,000 GuardDuty findings generated in the current AWS Region. You can choose to view the Last 2 days (default), Last 7 days or Last 30 days. This dashboard provides 6 widgets, 3 of which include filter capabilities to customize your view.  Figure 10: GuardDuty summary page</p> <p>As the delegated administrator becomes more familiar with their organizations\u2019 GuardDuty findings, an advanced filtering technique may be employed using the Findings tab.  The Findings tab exposes 80+ finding attributes that can match the criteria you specify or filter out any unmatched findings. A common filter technique is one that focuses on threats that have a high indication that 1. A resource has been compromised, for example looking at high severity findings 2. The type of finding indicates potential risks for unwanted billing charges, for example CryptoMining.</p> <p>Surfacing Bitcoin mining is an example of one such finding filter that could be created. Bitcoin is a worldwide cryptocurrency and digital payment system that can be exchanged for other currencies, products, and services. Bitcoin is a reward for bitcoin-mining and is highly sought after by threat actors. To find out if any of your EC2 instances have been compromised for purposes of Bitcoin mining you can use this attribute and value pairing: Severity:High, Finding type:CryptoCurrency:EC2/BitcoinTool.B!DNS. Applying this filter will provide a view of any EC2 instances that are querying a domain name that is associated with Bitcoin or other cryptocurrency-related activity.  Figure 11: GuardDuty finding page</p> <p>Note: Frequently used filters can be saved to reduce future efforts. To save the specified attributes and their values (filter criteria) as a filter, select Save. Enter the filter name and description, and then choose Done.</p>"},{"location":"guides/guardduty/#reducing-potential-noise","title":"Reducing potential noise","text":"<p>As accounts and workloads grow within an AWS Organization, it is possible there will be an increase in GuardDuty findings because of certain configurations. Some of the findings may be deemed low value or threats not intended to be acted upon. To make it easier to recognize security threats that are most impactful to your environment, enabling quick remediation actions, it is recommended to deploy suppression rules. A suppression rule is a set of criteria, consisting of a filter attribute paired with a value, used to filter findings by automatically archiving new findings that match the specified criteria.</p> <p>After creating a suppression rule, new findings that match the criteria defined in the rule are automatically archived as long as the suppression rule is in place. Existing filters can be used to create suppression rules. Suppression rules can be configured to suppress entire finding types, or define more granular filter criteria to suppress only specific instances of a particular finding type. Suppression rules can be edited at any time. It is recommended to suppress with as much granularity as possible. This will help with reducing findings for only certain circumstances without losing total coverage of a finding type.</p> <p>Suppressed findings are not sent to AWS Security Hub, Amazon S3, Amazon Detective, or Amazon CloudWatch, reducing finding noise level if you consume GuardDuty findings via Security Hub, a third-party SIEM, or other alerting and ticketing applications.</p> <p>GuardDuty continues to generate findings even when they match suppression rules, however, those findings are automatically marked as archived. An archived finding is stored in GuardDuty for 90-days and can be viewed at any time during that period. You can view suppressed findings in the GuardDuty console by selecting Archived from the findings table, or through the GuardDuty API using the ListFindings API with a findingCriteria criterion of service.archived equal to true.</p> <p>A common use case for a suppression rule is filtering out a known resource that scans EC2 instances. This resource may result in a finding type Recon:EC2/Portscan which for certain resources in your environment may be intended. For this scenario it is recommended to suppress associated findings using a combination of resource tagging and Finding Type.</p>"},{"location":"guides/guardduty/#automated-notification-for-high-priority-findings","title":"Automated notification for high priority findings","text":"<p>After creating suppression rule filters to allow for surfacing only the most impactful findings for your environment, the next recommended action is automating the notification of high priority findings. Amazon EventBridge is a serverless event bus that makes it easier to build event-driven applications at scale using events generated from AWS services. GuardDuty creates an event for Amazon EventBridge for newly generated findings. All findings are dynamic, meaning that, if GuardDuty detects new activity related to the same security issue it will update the original finding with the new information as a newly aggregated finding. Thus, creating a new event for EventBridge to potentially process. Delegated administrators can optionally change the frequency of publishing new findings to EventBridge in GuardDuty Settings under Findings export options. By default, EventBridge is updated every 6 hours but can be changed to 1 hour or 15 minutes intervals.</p> <p>Every GuardDuty finding is assigned a finding ID. GuardDuty creates an EventBridge event for every finding with a unique finding ID.  By using Amazon EventBridge with GuardDuty, you can automate tasks to help you respond to security issues related to GuardDuty findings. One such task is sending notifications to a preferred notification channel. Commonly used notification channels are email and Enterprise messaging applications that support incoming messages via Webhook.</p> <p>Another common scenario is to send GuardDuty findings to a ticketing system or SIEM solution for tracking and event correlation. It is recommended to leverage Security Hub as a central aggregation point in AWS for security findings before sending these findings to a ticketing system or SIEM solution. You can also leverage EventBridge to send GuardDuty findings directly to a ticketing system or SIEM solution. Please refer to your solutions documentation for guidance on how to send information via EventBridge.</p> <p>When leveraging email as a preferred notification channel, GuardDuty is integrated with Amazon Simple Notification Services (Amazon SNS) via EventBridge. Amazon Simple Notification Service (Amazon SNS) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication. The notification workflow is as depicted in the image below.  Figure 12: GuardDuty notification workflow</p> <p>All GuardDuty findings not matching a suppression rule are automatically delivered to the account event bus. There are only two configurations required to deploy email notification via Amazon SNS for unsuppressed findings:</p> <ol> <li>Create and subscribe to a SNS topic</li> <li>Create an EventBridge rule</li> </ol> <p>SNS is the target service for EventBridge actions in this workflow. SNS only requires two steps: 1. Create a new Topic (choose type as Standard) and provide a topic Name, 2. Create a Subscription with Email as the protocol. Entering an email address under Endpoint and clicking Create subscription finalizes the steps required in the console. The last step is to click Confirm subscription within the email sent to the email address used for the endpoint.  Figure 13: GuardDuty SNS email</p> <p></p> <p>Figure 14: SNS subscription confirmation</p> <p>The basis of EventBridge is to create rules that route events to a target. In this example below, a rule with an event pattern is constructed. This results in the rule being ran when an event matches the defined pattern. This rule will look for EC2 instances that exhibit SSH brute force attack behavior with the added detail of the resource being leveraged as the threat actor.</p> <p>With both the SNS topic/subscription in place, deploying this EventBridge rule will automatically send an email notification to the registered email address. It is recommended to use an email alias associated with a team when sending notifications of high severity findings.  Figure 15: GuardDuty EventBridge rule</p> <p>Tip: For more detailed instructions on how to build an EventBridge rule for GuardDuty findings that target SNS for email notification with customization, refer to this AWS Knowledge Center video.</p>"},{"location":"guides/guardduty/#automated-remediation-for-common-finding-types","title":"Automated remediation for common finding types","text":"<p>Stealth:S3/ServerAccessLoggingDisabled A common use case for automation with GuardDuty finding types is to address S3 and EC2 related findings as a result of misconfiguration (intentional or unintentional). For example, finding type Stealth:S3/ServerAccessLoggingDisabled informs you that S3 server access logging is disabled for a bucket within your AWS environment. If disabled, no web request logs are created for any attempts to access the identified S3 bucket, however, S3 management API calls to the bucket, such as DeleteBucket, are still tracked. If S3 data event logging is enabled through CloudTrail for this bucket, web requests for objects within the bucket will still be tracked. This could be the result of a misconfiguration or part of an unauthorized user\u2019s technique to evade detection.</p> <p>Using the associated finding details, an administrator can learn about the AWS resources that were involved in the suspicious activity, when this activity took place, and additional information. After gathering the information, the administrator could pivot from the finding into the S3 bucket to re-enable Server Access Logging. Remediating this automatically would save the administrator time and allow for focused investigation into how a potential threat actor was able to perform the API call.</p> <p>A common remediation approach decouples the GuardDuty detection from the automation action using an AWS Config Managed Rule to trigger on s3-bucket-logging-enabled configuration changes. Once triggered, AWS Config will immediately evaluate the S3 bucket and display a status of NON_COMPLIANT if logging is disabled. You can associate remediation actions with AWS Config rules and choose to execute them automatically to address non-compliant resources without manual intervention. In this scenario, the non-compliant s3-bucket-logging-enabled status will trigger the Systems Manager Automation document named AWS-ConfigureS3BucketLogging. By selecting Automatic remediation and AWS-ConfigureS3BucketLogging as your Remediation action detail when configuring your AWS Config rule Action, S3 server access logging will be re-enabled anytime a S3 bucket is non-compliant to the Config rule.</p> <p></p> <p>Figure 16: GuardDuty finding automation</p> <p>Backdoor:EC2/C&amp;CActivity.B EC2 instances querying an IP address that is associated with a known command and control server is another common use case for automated remediation. This finding informs you that the listed instance within your AWS environment is querying an IP associated with a known command and control (C&amp;C) server. The listed instance might be compromised. This type of finding has multiple steps towards remediations, that may include actions like performing a snapshot of the volume(s) before terminating the instance. This snapshot can provide information to a forensics team. However, the first step in remediating this finding is to isolate the instance.</p> <p>The same event workflow described in the Automated Notifications section can be utilized to change the security group of the instance to stop inbound/outbound connections to the internet. An AWS Lambda function would be the target of an EventBridge rule set to match on Backdoor:EC2/C&amp;CActivity.B. (An EventBridge can support multiple targets; a best practice for this scenario would also include an SNS topic target) Here is an example Lambda function written Python to perform this task:</p> <pre><code>import boto3\n\nec2_client = boto3.client('ec2')\n\ndef lambda_handler(event, context):\n    try:\n        # Check if the event is triggered by GuardDuty finding\n        if 'detail' in event and 'type' in event['detail'] and event['detail']['type'] == 'Backdoor:EC2/C&amp;CActivity.B':\n            # Extract relevant information from the GuardDuty finding\n            instanceId = event['detail']['resource']['instanceDetails']['instanceId']\n            SecurityGroupId = \"sg-XXXXXXXXXXXX\"  # Replace with the desired Security Group ID\n\n            # Construct the modification command\n            command = {\n                'InstanceId': instanceId,\n                'Groups': [SecurityGroupId]\n            }\n\n            # Send the command to modify the EC2 instance's security group\n            ec2_client.modify_instance_attribute(**command)\n\n            return 'Updated security group for instance: {}'.format(instanceId)\n        else:\n            return 'No action taken. Event is not a GuardDuty finding of type Backdoor:EC2/C&amp;CActivity.B'\n    except Exception as e:\n        print('Error:', str(e))\n        raise e\n</code></pre> <p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</p> <p>=======</p>"},{"location":"guides/guardduty/#s3-malware-protection-automation-workflow","title":"S3 Malware Protection automation workflow","text":"<p>Giving GuardDuty the ability to add tags such as NO_THREATS_FOUND, THREATS_FOUND, UNSUPPORTED, ACCESS_DENIED, or FAILED based on the result of the malware scan gives you the ability to create automated workflows. For example, if you are allowing untrusted third parties to upload documents into S3 before you bring them into an application you can use GuardDuty S3 malware protection to scan these documents and then either move them into the application bucket or a quarantine bucket based on the tag given. As a primer we have included a picture below illustrating how GuardDuty S3 Malware Protection processes files and integrates with other AWS services.</p> <p> Figure 17: S3 Malware Protection automation workflow</p> <p>To achieve the example we described in the first paragraph we can use Amazon EventBridge and AWS Lambda to build out our automated solution based on the S3 object scan result getting published to your default event bus which is configured by default when S3 Malware Protection is configured in your AWS account. Then you can configure a Lambda function to process the file based on the object tag that was given by GuardDuty.</p> <p>You can see an example EventBridge rule in the GuardDuty documentation. From here you will need to configure a Lambda function to use that EventBridge rule as a trigger and then moves the file based on your needs. Example Python code to do this is shown below. Keep in mind this is example code and should be altered to fit your use case.</p> <pre><code>#Script that evaluates messages about the scan status of S3 malware scanning and then moves the files to\n#the appropriate bucket based on the status.\n\n\nimport boto3\nimport json\nimport os\nimport logging\nfrom botocore.exceptions import ClientError\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\ngdclient = boto3.client('guardduty')\ns3client = boto3.client('s3')\n\ninfected_bucket = os.environ['INFECTED_BUCKET']\nclean_bucket = os.environ['CLEAN_BUCKET']\n\ndef lambda_handler(event, context):\n    logger.info('Event Data')\n    logger.info(event)\n\n    scan_status = event[\"detail\"][\"scanStatus\"]\n    bucket_name = event[\"detail\"][\"s3ObjectDetails\"][\"bucketName\"]\n    object_key = event[\"detail\"][\"s3ObjectDetails\"][\"objectKey\"]\n    scan_result = event[\"detail\"][\"scanResultDetails\"][\"scanResultStatus\"]\n\n    if scan_status == 'COMPLETED':\n        logger.info(\"Scan status is COMPLETED\")\n\n        if scan_result == 'THREATS_FOUND':\n            logger.info(\"Threats found in the object\")\n            logger.info(\"Moving file to infected bucket: %s\",infected_bucket)\n\n            copy_source = {'Bucket': bucket_name, 'Key': object_key}\n\n            try:\n                response = s3client.copy_object(\n                    Bucket=infected_bucket,\n                    CopySource=copy_source,\n                    Key=object_key\n                )\n\n                print(response)\n\n                logger.info(\"File copy successful\")\n                logger.info(\"Deleting object from source bucket\")\n                response = s3client.delete_object(\n                    Bucket=bucket_name,\n                    Key=object_key)\n\n                print (response)\n\n            except ClientError:\n                raise\n\n        elif scan_result == 'NO_THREATS_FOUND':\n            logger.info(\"No threats found in the object\")\n            logger.info(\"Moving file to clean bucket: %s\",clean_bucket)\n\n            copy_source = {'Bucket': bucket_name, 'Key': object_key}\n\n            try:\n                response = s3client.copy_object(\n                    Bucket=clean_bucket,\n                    CopySource=copy_source,\n                    Key=object_key\n                )\n\n                print(response)\n\n                logger.info(\"File copy successful\")\n                logger.info(\"Deleting object from source bucket\")\n                response = s3client.delete_object(\n                    Bucket=bucket_name,\n                    Key=object_key)\n\n                print (response)\n\n            except ClientError:\n                raise\n\n\n        else:\n            logger.info(\"scan_result is: %s not moving\", scan_result)\n</code></pre> <p>421eb98926175b205b99d22ed374dd1ef9fc421b</p>"},{"location":"guides/guardduty/#cost-optimization","title":"Cost Optimization","text":"<p>When GuardDuty is enabled for the first time in an account, that account will be automatically provided with a 30-day free trial period for each region. Subsequently each feature also has a free trial too. For more information on pricing and free trials please refer to the GuardDuty pricing page. This is an important first step in understanding monthly GuardDuty costs in a given account. During the trial an administrator can view cost estimations based on Account ID, Data source, Feature and S3 buckets. If enabling GuardDuty in an AWS organization, the administrator can monitor cost metrics for all of the member accounts.</p> <p>AWS account administrators may be required to investigate charges related to GuardDuty if an increase in costs occurs. Some common reason for increased monthly GuardDuty costs includes:</p>"},{"location":"guides/guardduty/#cloudtrail-andor-s3-data-event-utilization-is-high","title":"CloudTrail and/or S3 data event utilization is high","text":"<p>An increase in CloudTrail charges could be attributed to multiple reasons depending on whether they originate from management event costs or data event costs. Although there are many ways to query AWS log data we will show examples using Athena to look at exact API calls and the services associated with them. To understand CloudTrail API volumes using Athena:</p> <ol> <li>If not already present, create a new CloudTrail trail [2]. Choose to account for both management and data events here. If the trail is not needed beyond the scope of this analysis, the trail may be deleted or disable logging data to it.</li> <li>Create an Athena table [3] that pulls from the S3 location where the CloudTrail logs are stored</li> <li>Once this table has been created, navigate to the Athena console to perform a query over a 24-hour period to understand the top event names and event sources that contributed to the spike</li> </ol> <pre><code>SELECT eventName,count(eventName) AS eventVolume,eventSource\nFROM your_athena_tablename\nWHERE eventtime between '2021-01-24' AND '2021-01-25'\nGROUP BY eventName, eventSource\nORDER BY eventVolume DESC limit 10;\n</code></pre> <ol> <li>Upon gaining an understanding of which events and event sources were responsible for a bulk of the activity, it may be suitable to alter the SQL query to view data by hour, user-agent, IP or principal. This will help to identify the exact source of the increased API calls</li> </ol> <p>[2] Creating a trail - https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-a-trail-using-the-console-first-time.html [3] Querying AWS CloudTrail Logs - Using the CloudTrail Console to Create an Athena Table for CloudTrail Logs  - https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html#create-cloudtrail-table-ct</p>"},{"location":"guides/guardduty/#vpc-flow-logs","title":"VPC Flow Logs","text":"<p>An increase in VPC Flow log related costs can be a result of GuardDuty analyzing flow logs to identify malicious, unauthorized, or unexpected behavior in AWS accounts and workloads. This data source is charged per Gigabyte (GB) per month and is offered with tiered volume discounts. To understand VPC Flow log volumes using Athena, perform the steps:</p> <ol> <li>Ensure that Flow logs are being published to S3 [4]</li> <li>Create an Athena table [5] that pulls from the S3 location where the flow logs are stored [5]</li> <li>Once this table has been created, navigate to the Athena console to perform a query over a 24 period to understand the top flows that contributed to the spike</li> </ol> <pre><code>SELECT COUNT(version) AS Flows,\n        SUM(numpackets) AS Packetcount,\n         sourceaddress\nFROM vpc_flow_logs\nWHERE date = DATE('2021-01-31')\nGROUP BY  sourceaddress\nORDER BY  Flows DESC LIMIT 10;\n</code></pre> <ol> <li>Alternatively, if VPC flow logs are setup with CloudWatch logs as their final destination, the customer may use CloudWatch insights to extract the top-talker information [6]. Here are some sample queries that can be called directly from the insights console [7]</li> </ol> <p>[4] Publishing flow logs to Amazon S3 - https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-s3.html [5] Querying Amazon VPC Flow Logs - https://docs.aws.amazon.com/athena/latest/ug/vpc-flow-logs.html [6] Analyzing Log Data with CloudWatch Logs Insights - https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html [7] Sample Queries - https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_QuerySyntax-examples.html</p> <p>Querying flow logs using Amazon Athena - https://docs.aws.amazon.com/vpc/latest/userguide/flow-logs-athena.html*</p>"},{"location":"guides/guardduty/#dns-query-logs","title":"DNS Query logs","text":"<p>An increase in DNS log related costs can be a result of GuardDuty analyzing DNS requests and responses to identify malicious, unauthorized, or unexpected behavior in AWS accounts and workloads. This data source is charged per Gigabyte (GB) per month and is offered with tiered volume discounts. Much like VPC Flow logs, an account may have their DNS flow logs exported to CloudWatch or S3 [8]. Top-talkers for DNS can be queried in CloudTrail Insights:</p> <ol> <li>Navigate to Route53 and verify that 'Query Logging' is enabled</li> <li>Check the logger configuration and confirm that a 'CloudWatch Logs log group' is the destination type</li> <li>Verify that one or more VPC\u2019s are present under 'VPC\u2019s that queries are logged for'</li> <li>Click the log group shown as a part of the 'Destination ARN'</li> <li>Click Insights on the left tab</li> <li>Select the log-group where the queries are being logged</li> <li>Select a time range and supply a query to understand which instance and hostnames have the most outbound queries</li> </ol> <pre><code>stats count(*) as numRequests by srcids.instance,query_name\n    | sort numRequests desc\n    | limit 10\n</code></pre> <p>Alternatively, if the DNS logs are being routed to a S3 bucket, Athena can be used to create a table that pulls this data from S3 [9]. Steps to perform:</p> <ol> <li>Navigate to Route53 and verify that 'Query Logging' is enabled</li> <li>Check the logger configuration and confirm that 'S3 bucket' is the destination type</li> <li>Verify that one or more VPC\u2019s are present under 'VPC\u2019s that queries are logged for'</li> <li>Navigate to Athena and create a table using the following command</li> </ol> <pre><code>CREATE EXTERNAL TABLE IF NOT EXISTS default.route53query (\n  `version` float,\n  `account_id` string,\n  `region` string,\n  `vpc_id` string,\n  `query_timestamp` string,\n  `query_name` string,\n  `query_type` string,\n  `query_class` string,\n  `rcode` string,\n  `answers` array&lt;string&gt;,\n  `srcaddr` string,\n  `srcport` int,\n  `transport` string,\n  `srcids` string,\n  `isMatchedDomain` string\n)\nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\nWITH SERDEPROPERTIES (\n  'serialization.format' = '1'\n) LOCATION 's3://&lt;bucket name&gt;/AWSLogs/&lt;account number&gt;/vpcdnsquerylogs/'\nTBLPROPERTIES ('has_encrypted_data'='false');\n</code></pre> <ol> <li>Query data using the following query</li> </ol> <pre><code>SELECT count(version) AS numRequests,\n         srcids,\n        query_name\nFROM route53query\nWHERE query_timestamp\n    BETWEEN '2021-01-14'\n        AND '2021-01-15'\nGROUP BY  srcids,query_name\nORDER BY  numRequests DESC limit 10;\n</code></pre> <p>[8] Log your VPC DNS queries with Route 53 Resolver Query Logs - https://aws.amazon.com/blogs/aws/log-your-vpc-dns-queries-with-route-53-resolver-query-logs/ [9] How to automatically parse Route 53 Resolver query logs - https://aws.amazon.com/blogs/networking-and-content-delivery/how-to-automatically-parse-route-53-resolver-query-logs/ --</p>"},{"location":"guides/guardduty/#other-considerations","title":"Other considerations","text":"<ul> <li>Sometimes an understanding of a surge in the costs occurring at the start of the month for CloudTrail S3 and VPC/DNS flow log analysis is required. This is most likely due to the pricing structure of these data sources. GuardDuty offers tiered discounts and becomes inexpensive to use as more events are analyzed through the course of the month. See pricing page (https://aws.amazon.com/guardduty/pricing/) for a better understanding.</li> <li>GuardDuty costs can increase due to configuration changes on an account including deployment of new applications or the use of a third-party security/scanning tool that makes repeated \u2018Describe\u2019 API calls. These might cause an increase in CloudTrail events, and/or traffic (VPC &amp; DNS logs).</li> <li>It is not recommended to rely on the CloudTrail event history's CSV download to compare event volumes because it may have event filters or incomplete data. This feature is more suited for smaller CSV downloads.</li> <li>CloudTrail insights can be useful to augment these investigations. CloudTrail Insights continuously monitors CloudTrail write management events, and uses mathematical models to determine the normal levels of API and service event activity for an account.</li> <li>If a new member account is added or if a GuardDuty service is resumed after being disabled for an extended time those costs may also be perceived as surges.</li> <li>S3 protection can be disabled but GuardDuty does not allow you to remove any additional data sources. However, if DNS is disabled at the VPC level, those logs are not processed by GuardDuty. Accounts will not be charged or have DNS-based results.</li> <li>GuardDuty is optimized for security value and will not charge customers for processing some low-risk events that would otherwise be delivered to a customer's CloudTrail. Therefore, the event counts may not exactly match.</li> <li>If Runtime Monitoring is enabled for your account, you will not be charged for analysis of VPC Flow Logs from instances where the GuardDuty agent is deployed and active.</li> </ul>"},{"location":"guides/guardduty/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/guardduty/#troubleshooting-aws-systems-manager-ssm-agent-issues","title":"Troubleshooting AWS Systems Manager (SSM) Agent Issues","text":"<p>There are several issues that might cause the SSM agent to work improperly. You can use the Systems Manager Automation runbook to automatically troubleshoot an EC2 instance that SSM is unable to manage</p> <ul> <li>Open this link to configure theSystems Manager Automation runbook.</li> <li>Navigate to the AWS Region for the EC2 instance you want to troubleshoot.</li> <li>Under the Input parameters, change the dropdown from \"Show managed instances only\" to \"Show all instances\".</li> <li>Select the EC2 instance you want to troubleshoot.</li> <li>Leave all the other settings, and at the bottom of the page, click Execute. This process normally takes up to 5 minutes to complete.</li> <li>Once the automation document has as an Overall status of Success, expand the Outputs section.</li> <li>Review the outputs to see the specific problem with your SSM configuration.</li> </ul>"},{"location":"guides/guardduty/#troubleshooting-shared-vpcs-issues","title":"Troubleshooting Shared VPCs Issues","text":"<p>EC2 instances in your AWS account might run in subnets from a shared VPC owned by another AWS account. In this scenario the GuardDuty console might show an error message \u201cVPC Endpoint Creation Failed\u201d for EC2 instances listed in the Runtime Monitoring section. * Enable GuardDuty EC2 Runtime Monitoring in the AWS account that owns the shared VPC. * Ensure there is at least one EC2 instance running in the shared VPC and in the AWS account that owns the VPC.</p>"},{"location":"guides/guardduty/#troubleshooting-custom-recursive-dns-architecture-issues","title":"Troubleshooting Custom Recursive DNS Architecture Issues","text":"<p>Another reason that you might see the error message \u201cVPC Endpoint Creation Failed\u201d can be caused by using a custom DNS server instead of the default Amazon Route 53 Resolver, often referred to as the \u201c.2 resolver\u201d. Since GuardDuty relies on the Route 53 resolver to know where how to find the GuardDuty VPC endpoint this can cause an issue with the VPC endpoint configuration. Although we recommend using the AWS managed DNS resolver for recursive DNS, some customers decide to use a custom DNS resolver. To fix this issue while still using your custom DNS server you have two options:</p> <p>Option 1: The best option is to leave the \u201cEnable DNS resolution\u201d box checked so that GuardDuty can use the Route 53 resolver to find the GuardDuty VPC endpoint. The DHCP option set that you have configured for your custom DNS server will have set the resolver.conf file for your hosts so that they use your custom DNS server. The GuardDuty agent will use the .2 resolver option. To enable the Route 53 DNS Resolver in your VPC follow the steps below.</p> <ul> <li>Visit the Amazon VPC Console. </li> <li>Select the VPC that you want to change the DNS settings.</li> <li>In the top right corner use the Actions drop down and click edit VPC settings</li> </ul> <p></p> <ul> <li>Once you\u2019re on the VPC settings page make sure the Enable DNS resolution box is checked. This will enable Route53 DNS Resolver for this VPC.</li> </ul> <p></p> <ul> <li>If you have configured a custom DNS server because you want to limit traffic that can use the Route 53 resolver then you can use Route 53 DNS firewall to allow resolution to the GuardDuty endpoint and nothing else. To do this you will need to make two rules in your Route 53 DNS firewall policy. The rule with the top priority will allow resolution to \u201cguardduty-data..amazonaws.com\u201d. The rule that will follow in rule evaluation priority should deny \u201c.\u201d. The combination of these rules will allow resolution to the GuardDuty endpoint through the Route 53 resolver, while denying any other resolution. (GuardDuty is also monitoring this Route53 resolver traffic for suspicious) activity.* <p>Option 2: If you\u2019re using a centralized endpoint VPC architecture you can create an entry in your custom DNS server to resolve the GuardDuty endpoint. To do this you will need to check the endpoint specific to your central VPC that the GuardDuty agent is trying to resolve and then add that as your DNS entry.</p>"},{"location":"guides/guardduty/#troubleshooting-ecs-task-execution-role-errors","title":"Troubleshooting ECS Task Execution Role Errors","text":"<p>If you see issue type Agent exited and additional information error message that says CannotPullContainerError this is likely caused by insufficient IAM permissions for the ECS Fargate task execution role. Fargate tasks must use a task execution role. This role grants the tasks permission to retrieve, update, and manage the GuardDuty security agent on your behalf. To fix this follow the steps below.</p> <ul> <li>Visit the AWS IAM console.</li> <li>In the left-hand navigation pane, choose Roles, and then choose Create role.</li> <li>For Trusted entity type, choose AWS service.</li> <li>For Service or use case, choose Elastic Container Service, and then choose the Elastic Container Service Task use case.</li> <li>Choose Next.</li> <li>In the Add permissions section, search for AmazonECSTaskExecutionRolePolicy, then select the policy.</li> <li>Choose Next.</li> <li>For Role name, enter ecsTaskExecutionRole.</li> <li>Review the role, and then choose Create role.</li> </ul> <p>Runtime Monitoring supports managing the security agent for your Amazon ECS clusters (AWS Fargate) only through GuardDuty. There is no support for managing the security agent manually on Amazon ECS clusters.</p>"},{"location":"guides/guardduty/#resources","title":"Resources","text":""},{"location":"guides/guardduty/#workshops","title":"Workshops","text":"<ul> <li>Activation Days</li> <li>Amazon GuardDuty workshop</li> <li>Amazon Detective workshop</li> <li>Container security workshop</li> </ul>"},{"location":"guides/guardduty/#videos","title":"Videos","text":"<ul> <li>Introducing GuardDuty ECS Runtime Monitoring</li> <li>Amazon GuardDuty EKS Protection Overview</li> <li>Amazon GuardDuty S3 Protection Overview</li> <li>Amazon GuardDuty findings summary view</li> <li>Enable GuardDuty Lambda Protection to monitor your Lambda execution environment</li> <li>GuardDuty EKS Runtime Monitoring</li> <li>Amazon GuardDuty RDS Protection &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</li> <li>Amazon Guardduty Extended Threat Detection</li> </ul> <p>======= * S3 Malware Protection overview</p> <p>421eb98926175b205b99d22ed374dd1ef9fc421b</p>"},{"location":"guides/guardduty/#blogs","title":"Blogs","text":"<ul> <li>Measure cluster performance impact of Amazon GuardDuty EKS Agent</li> <li>How AWS threat intelligence deters threat actors</li> <li>Improve your security investigations with Detective finding groups visualizations</li> <li>Three ways to accelerate incident response in the cloud: insights from re:Inforce 2023</li> <li>Detect threats to your data stored in RDS databases by using GuardDuty</li> <li>Reduce triage time for security investigations with Amazon Detective visualizations and export data</li> <li>How to use Amazon GuardDuty and AWS WAF v2 to automatically block suspicious hosts</li> <li>How to improve security incident investigations using Amazon Detective finding groups</li> <li>Automatically block suspicious DNS activity with Amazon GuardDuty and Route 53 Resolver DNS Firewall</li> <li>How to use new Amazon GuardDuty EKS Protection findings</li> </ul>"},{"location":"guides/inspector/","title":"Amazon Inspector","text":""},{"location":"guides/inspector/#introduction","title":"Introduction","text":"<p>Welcome to the Amazon Inspector Best Practices Guide. The purpose of this guide is to provide prescriptive guidance for leveraging Amazon Inspector for continuous monitoring of software vulnerabilities and unintended network exposure in AWS workloads such as Amazon EC2, AWS Lambda functions, and Amazon ECR. Publishing this guidance via GitHub will allow for quick iterations to enable timely recommendations that include service enhancements, as well as, the feedback of the user community. This guide is designed to provide value whether you are deploying Inspector for the first time in a single account, or looking for ways to optimize Inspector in an existing multi-account deployment.</p>"},{"location":"guides/inspector/#how-to-use-this-guide","title":"How to use this guide","text":"<p>This guide is geared towards security practitioners who are responsible for monitoring and remediation of security events, abnormal activity, and vulnerabilities within AWS accounts (and resources). The best practices are organized into different categories for easier consumption. Each category includes a set of corresponding best practices that begin with a brief overview, followed by detailed steps for implementing the guidance. The topics do not need to be read in a particular order:</p> <ul> <li>What is Amazon Inspector</li> <li>What are the benefits of enabling Amazon Inspector</li> <li>Getting started<ul> <li>AWS SSM agent</li> <li>Deployment considerations</li> <li>Region considerations</li> </ul> </li> <li>Implementation<ul> <li>Stand-alone account enablement</li> <li>Multi-Account organization enablement</li> </ul> </li> <li>Coverage<ul> <li>Amazon EC2 scanning</li> <li>Windows scanning</li> <li>ECR scanning</li> <li>Lambda scanning</li> <li>CI/CD</li> <li>CIS Scans</li> </ul> </li> <li>Operationalizing<ul> <li>Actioning Inspector findings</li> <li>Software bill of materials (SBOM) configuration</li> <li>Suppression rules</li> <li>Vulnerability database search</li> </ul> </li> <li>Cost considerations</li> <li>Resources</li> </ul>"},{"location":"guides/inspector/#what-is-amazon-inspector","title":"What is Amazon Inspector?","text":"<p>Amazon Inspector is a vulnerability management service that continuously monitors your AWS workloads for software vulnerabilities and unintended network exposure. Amazon Inspector automatically discovers and scans running Amazon EC2 instances, container images in Amazon Elastic Container Registry (Amazon ECR), and AWS Lambda functions.</p>"},{"location":"guides/inspector/#what-are-the-benefits-of-enabling-amazon-inspector","title":"What are the benefits of enabling Amazon Inspector?","text":"<p>Amazon Inspector continuously discovers resources across your AWS Organization. After you deploy Amazon Inspector it will identify and automatically assess your resources for vulnerabilities. Keeping a continuous state of your environment Amazon Inspector understands when resources no longer exist or more importantly when new resources are deployed so that it can begin assessing these resources for vulnerabilities without requiring any manual configurations.  </p> <p>Continuous assessment of your resources including Amazon EC2 instances, images in ECR, and Lambda functions for vulnerabilities so that you are always up to date on the current state of your resources, even as new vulnerabilities are discovered. Amazon Inspector continues to assess your environment throughout the lifecycle of your resources by automatically monitoring resources in response to changes that could introduce a new vulnerability, such as: installing a new package in an Amazon EC2 instance, installing a patch, and when a new common vulnerabilities and exposures (CVE) that impacts the resource is published. Unlike traditional security scanning software, Amazon Inspector has minimal impact on the performance of your fleet.</p> <p>Amazon Inspector creates a finding when it discovers a software vulnerability, code vulnerability, or network configuration issues. A finding describes the vulnerability, identifies the affected resource, rates the severity of the vulnerability, and provides remediation guidance. Amazon Inspector also provide an Amazon Inspector score that provides context into the severity of a vulnerability. Visit the Inspector documentation to learn more about the Amazon Inspector score. You can analyze findings using the Amazon Inspector console, or view and process your findings through other AWS services.</p> <p>Integration with Organizations enables you to quickly deploy and see vulnerability posture across all of their accounts from a single location and verify that Amazon inspector is automatically enabled and performing assessments as new accounts are added to the organization. This reduces the amount of effort needs to maintain a ready state of vulnerability management across your AWS environment.</p>"},{"location":"guides/inspector/#getting-started","title":"Getting started","text":"<p>In this section we will cover what you need to consider before activating Amazon Inspector in your AWS Organization. </p>"},{"location":"guides/inspector/#aws-ssm-agent","title":"AWS SSM Agent","text":"<p>Although not required, it is recommended when using Amazon Inspector to detect software vulnerabilities in Amazon EC2 instance, the instance is managed instance in AWS Systems Manager (SSM). An SSM managed instance has the AWS SSM Agent installed and running, and SSM has permission to manage the instance. AmazonSSMManagedInstanceCore is the recommended policy to use when you attach an instance profile. This policy has all the permissions needed for Amazon Inspector EC2 scanning. If you are already using SSM to manage your instances, no additional steps are needed for Amazon Inspector to begin scans.</p> <p>If you have instances that do not or can not have an SSM agent for example because of an unsupported operating system you will still be able to scan them for software vulnerabilities using Inspector agentless scanning also referred to as hybrid scanning mode. Scanning with the SSM agent uses an event driven pattern, for example when new packages are installed on an instance Inspector will initiate an evaluation of the package. With agentless scanning, Inspector scans instances once per 24 hours.</p> <p>The AWS SSM Agent is installed by default on Amazon EC2 instances created from some Amazon Machine Images (AMIs). For more information, see About SSM Agent in the AWS Systems Manager User Guide. However, even if it is installed, you may need to activate the AWS SSM Agent manually, and grant SSM permission to manage your instance.</p> <p>In addition to providing the visibility that is needed to look for vulnerabilities on Amazon EC2 instance the presence of the AWS SSM agent provides value for activities such as scheduling automated patching, creating maintenance windows, reporting on patching compliance, providing a mechanism to run automation across your fleet of instances, and provide secure access to instances without the need to expose port 22 or 3389 for remote access to SSH or RDP.</p> <p>If you're interested in using the AWS SSM agent, refer to the section below on enabling default host management in AWS Systems Manager.</p>"},{"location":"guides/inspector/#enabling-default-host-management-in-aws-systems-manager-ssm","title":"Enabling Default Host Management in AWS Systems Manager (SSM)","text":"<p>Hybrid scanning in Amazon Inspector includes agent-based scanning and agentless scanning. By default, Amazon Inspector uses these scan methods on all eligible Amazon EC2 instances. Agent-based scanning uses the SSM agent to collect software inventory. Agentless scanning uses Amazon EBS snapshots to collect software inventory. By default, the SSM agent is already installed in Amazon EC2 instances based on Amazon Machine Images. However, you might need to activate the SSM agent manually in some cases. For more information, see Working with the SSM agent in the AWS Systems Manager User Guide. To manage EC2 instances automatically with Systems Manager, use the Default Host Management Configuration setting.</p> <p>For assessing network reachability of Amazon EC2 instances, vulnerability scanning of container images, or vulnerability scanning of Lambda functions, no agents are necessary. </p> <ol> <li>Sign in to your organization management account for AWS Organizations.</li> <li>Navigate to Systems Manager and open the Quick Setup page.</li> <li>Click Create for Default Host Management Configuration.</li> <li>Make sure the box is selected for Enable automatic updates of the SSM Agent every two weeks.</li> <li>Click Create. You will see a banner stating, \"Your Default Host Management Configuration Quick Setup is being updated.\". It normally takes up to 30 minutes for the agent to connect with Systems Manager and the EC2 instance to appear in the Systems Manager Inventory as a managed instance (required for Inspector to perform agent-based scanning). </li> <li>You may continue to the next steps without waiting 30 minutes for the Host Management Configuration to complete. </li> </ol>"},{"location":"guides/inspector/#deployment-considerations","title":"Deployment Considerations","text":"<p>To deploy Inspector across your AWS Organization you need to enable it in the AWS management account and the security tooling account whether this is done in the Console, CLI, or API. If you are not familiar with the concept of the security tooling account it is recommended to familiarize yourself with the recommended account structure in the Security Reference Architecture. To summarize, this is a dedicated account in your AWS Organization that is used as the delegated administrator account for native AWS security services such as Amazon Inspector, Amazon GuardDuty, AWS Security Hub, and Amazon Detective.</p>"},{"location":"guides/inspector/#region-considerations","title":"Region Considerations","text":"<p>Amazon Inspector is a regional service. This means that to use Amazon Inspector you need to enable it in every region that you would like to have vulnerability monitoring capabilities in. You can enable Inspector across all accounts and regions using the AWS CLI script on GitHub or you can do this by toggling between regions in the console.</p> <p>A question that is often asked is \u201cShould I use a security service in a region that my company is not actively using?\u201d. Although there is not a straightforward answer to this question as there are many factors that might influence your answer such as risk appetite, budget, and compensating controls, among others. There are some things to keep in mind when making this decision.</p> <ol> <li>Inspector is a service that you only pay for what you use. So, if you have a region that has minimal use you can have Inspector on giving visibility into your vulnerability landscape without incurring disproportionate cost associated with the resources being used in a region.</li> <li>If you have a compensating control associated with none used regions such as a service control policy that blocks all use in this region. It is still recommended to have a detective control to validate detection of this compensating control being changed at any point in the future.</li> </ol> <p>This decision is ultimately one that your company needs to make based on your own circumstances, but a rule of thumb in security is that more visibility before an investigation is better as it often hard, if not impossible to obtain after the fact.</p>"},{"location":"guides/inspector/#implementation","title":"Implementation","text":"<p>In this section we will cover the minimum requirements for enabling Inspector in a stand-alone account and in a multi-account organization.</p>"},{"location":"guides/inspector/#stand-alone-account-enablement","title":"Stand-alone account enablement","text":"<p> Figure 1: Inspector getting started page</p> <p>The first step is to navigate to the Inspector console. Once you are in the Inspector console you should see a landing page with a getting started button. Click on \u201cGet started\u201d.</p> <p> Figure 2: Activate Inspector page</p> <p>After clicking on \u201cGet started\u201d you will be brought to the Inspector enablement page. Review the Service role permissions so that you have an understanding of the permissions Inspector needs in order to provide its features and functions then select the yellow box labeled \u201cActivate Inspector\u201d to enable Inspector.</p> <p>Once these two steps are done Inspector will be enabled in this account. It is important to refer to sections below to make sure you have enabled all relevant features of Inspector.</p>"},{"location":"guides/inspector/#multi-account-organization-enablement","title":"Multi-Account organization enablement","text":"<p>When you implement Inspector for the first time in your AWS Organization as stated above you will set the delegated administrator in your organization management account in each region that you want to use Inspector. Keep in mind you will need to use the same account in all regions.</p> <p> Figure 3: Inspector getting started page</p> <p>The first step is to navigate to the Inspector console in your organizations management account. Once you are on the Inspector console you will see a landing page with a getting started button. Click on \u201cGet started\u201d.</p> <p> Figure 4: Delegated admin assignment page</p> <p>After clicking on \u201cGet started\u201d you will be brought to the Inspector enablement page. Next you will need to enter the account ID for the account you want to designate as the Inspector delegated administrator account. Once you have entered the account ID select \u201cDelegate\u201d. At this point you will switch to the Delegated administrator to finish configuring Amazon Inspector across your AWS Organization.</p> <p> Figure 5: Inspector getting started page</p> <p>Once you are in the Inspector delegated administrator account you will again see a landing page with a getting started button. Click on \u201cGet started\u201d. Once this is complete Inspector will be enabled. There are a few more deployment considerations that we will cover.</p> <p> Figure 6: Inspector account management page</p> <p>Next you will need to make sure you activate scanning for all current accounts and all scanning types including Amazon EC2 scanning, Amazon ECR Scanning, AWS Lambda standard scanning, and AWS Lambda code scanning. It is also highly recommended to \u201cAutomatically activate Inspector for new member accounts\u201d. This helps you understand that you don\u2019t have a gap in vulnerability coverage when new accounts are created in your organization. This will also automate the process of onboarding new accounts and automatically aggregate all findings to your delegated administrator account reducing manual tasks and saving time.</p> <ul> <li>Sign in to the Delegated Admin account for Inspector.</li> <li>Open the Inspector console.</li> <li>From the Account management page, under Automatically activate Inspector for new member accounts toggle on the option to Automatically activate Inspector for new member accounts. </li> <li>Make sure that all of the options are selected and click Save. You should see a banner with the message \"You have successfully updated the auto-activate scan settings for your organization.\".</li> <li>Scroll down to where accounts are listed under Organization. Depending on how many accounts you have, you should click the gear icon on the right to view 50 accounts per page. </li> <li>Per page, select all accounts. Click the Activate dropdown menu, select all of the options, and click Submit. </li> <li>Within minutes, resources will start being scanned and you will see any generated findings on the Findings page.</li> </ul> <p>Alternatively, you may enable Inspector across all accounts and regions using CLI. Check out the enablement script on Github.</p>"},{"location":"guides/inspector/#coverage","title":"Coverage","text":"<p>In this section we will cover the enablement of different features to configure Inspector across all supported resources in your environment.</p>"},{"location":"guides/inspector/#amazon-ec2-scanning","title":"Amazon EC2 Scanning","text":"<p>After enabling Inspector in your Organization there are two more Amazon EC2 scanning related configurations that you need to address.</p> <p>Inspector Deep Inspection for Amazon EC2 gives Inspector the ability to detect package vulnerabilities for application programming language packages in your Linux-based Amazon EC2 Instances. New accounts enabling Inspector will have deep inspection enabled by default. If you have an account that was previously enabled and want to confirm that this is enabled, you can look at the account management page in the Inspector console and look for any accounts that have deep inspection scanning deactivated like the picture below. For full details on Inspector Deep inspection and instructions on how to configure deep inspection scans refer to the Inspector Deep inspection documentation.</p> <p> Figure 7: Inspector EC2 deep scan activated</p> <p> Figure 8: Inspector EC2 deep scan deactivated</p> <p>By default, Deep Inspection scanning looks at the default paths below.</p> <ul> <li>/usr/lib</li> <li>/usr/lib64</li> <li>/usr/local/lib</li> <li>/usr/local/lib64</li> </ul> <p>All individual accounts can add up to 5 custom paths. The delegated administrator can add an additional 5 paths that will apply across the organization. This amounts to a total of up to 10 custom paths scanned per account in the organization. An example of a custom path would be \u201c/home/usr1/project01\u201d. It is recommended to put in all custom paths applicable to your organization to provide full visibility for Inspector Deep inspection.</p> <p> Figure 9: Inspector EC2 scan settings</p>"},{"location":"guides/inspector/#windows-scanning","title":"Windows Scanning","text":"<p>Amazon Inspector automatically discovers all supported Windows instances and includes them in continuous scanning without any extra actions. Unlike scans for Linux based instances, Amazon Inspector runs Windows scans at regular intervals. Windows instances are initially scanned at discovery and then scanned every 6 hours. However, the default 6\u2010hour scan interval is adjustable. If 6 hours is your desired scan frequency than no action is needed. If you would like to adjust the scan frequency you can do this using the aws ssm update-association CLI command. For steps on how to adjust the scan frequency please refer to the Inspector Windows scanning documentation.</p>"},{"location":"guides/inspector/#ecr-scanning","title":"ECR Scanning","text":"<p>When you first activate ECR scanning, and your repository is configured for continuous scanning, Amazon Inspector detects all eligible images that you have pushed within 30 days, or pulled within the last 90 days. Amazon Inspector continues to monitor images as long as they were pushed or pulled within the last 90 days (by default), or within the ECR rescan duration you configure. You might choose to stop scanning instances after a defined period of time because they are no longer used for applications in your environment or based on other compensating controls you might have in place. You can configure Inspector to re-scan based on either image push date or image pull date. For example, if you select 60 days for push date, and 180 days for pull date configurations, Amazon Inspector will continue to monitor images if they were pushed in the last 60 days or if they have been pulled at least once in the last 180 days. We recommend understanding the application deployment patterns at your organization when deciding the re-scan duration settings. For example if you build images often you might want shorter scan durations. Also, if you utilize ECR lifecycle policies this might affect how long images exist in ECR. For detailed instructions on how to set this duration please refer to the ECR automated re-scan duration documentation.</p> <p> Figure 10: Inspector ECR scan settings</p>"},{"location":"guides/inspector/#lambda-scanning","title":"Lambda Scanning","text":"<p>Lambda scanning has two different functionalities. The first is the ability to scan Lambda functions for software vulnerabilities in programming languages and packages. The second is the ability to scan your custom privileged application code for code vulnerabilities. Under account management make sure that these are both activated to provide full Inspector visibility. If you see any accounts with Lambda scanning or code scanning deactivated refer to the Lambda scanning documentation for detailed steps on activation.</p> <p> Figure 11: Inspector Account management page</p>"},{"location":"guides/inspector/#cicd-scanning","title":"CI/CD Scanning","text":"<p>You can integrate Amazon Inspector directly in CI/CD tools such as Jenkins. For Jenkins and TeamCity tools Inspector has dedicated plugins that can be installed so you can add vulnerability scanning directly into these pipelines. These plugins can be used as a pass or fail mechanism based on finding severities.</p> <p>If Amazon Inspector does not provide plugins for your CI/CD solution, you can create your own custom CI/CD integration using a combination of the Amazon Inspector SBOM Generator and the Amazon Inspector Scan API.</p>"},{"location":"guides/inspector/#cis-scans","title":"CIS Scans","text":"<p>In addition to traditional vulnerability scans many customers also want to run scans to assess adherence to CIS Benchmarks. If you're not familiar, CIS Benchmarks from the Center for Internet Security (CIS) are a set of globally recognized and consensus-driven best practices to help security practitioners implement and manage their cybersecurity defenses. Developed with a global community of security experts, the guidelines help organizations proactively safeguard against emerging risks. Companies implement the CIS Benchmark guidelines to limit configuration-based security vulnerabilities in their digital assets. Just like you can check your AWS environment against CIS Benchmarks using AWS Security Hub, Inspector allows you to run CIS Benchmarks against your operating systems in AWS.</p> <p>Start by familiarizing yourself with the necessary requirements for running CIS scans such as the need for deep scanning to be enabled and an SSM agent to be present on an instance.</p> <p>Next, we recommend you think about your desired outcome for CIS scans before configuration. For example, you should answer questions before configuration such as what we have listed below, which is not an exhaustive list. This will help you get the most out of your CIS scans without creating extra findings that don't help with your overall security goals.</p> <ul> <li>How often do we build images? Do we need to run frequent scans or run one time scans based on frequent image build?</li> <li>What images / environments need to run level 1 vs level 2 scans?</li> <li>Will you push out CIS scans across your AWS Organization from the delegated administrator account or will individual account owners be responsible for creating them at the account level.</li> <li>Do you have a tag in place that you can use to target instances when configuring your scan configuration settings?</li> </ul>"},{"location":"guides/inspector/#operationalizing","title":"Operationalizing","text":""},{"location":"guides/inspector/#actioning-inspector-findings","title":"Actioning Inspector Findings","text":"<p>Many organizations have a well-established vulnerability management program, but if you don\u2019t or you are unsure of what you do now works with Amazon Inspector we want to highlight a few important themes that you should be thinking about.</p> <p> Figure 12: Inspector finding details</p> <p> Figure 13: Inspector score example</p> <p> Figure 14: Generative AI powered code remediation recommendation</p> <p>Inspector vulnerability findings give great detail into the resource affected, the CVE, and generative AI powered code remediation suggestions giving security and application teams the context needed to more quickly remediate vulnerabilities in your AWS environment. Inspector findings are also stateful. This means that if you update a package that contained a vulnerability Inspector will see the package change, initiate an assessment, and if the vulnerability was in fact remediated then close the finding. This understanding is important for understanding you will handle Inspector findings. Below will dive into some of the different themes we think you should be thinking through.</p> <ol> <li>A critical component of an effective vulnerability management program is the ability to assess and prioritize security findings. This is where pulling in context, organizational history, and tuning detection systems comes into place. Prioritization of security findings helps establish the appropriate speed for response level. We recommend prioritizing the investigation of all critical and high severity findings.</li> <li>Understand what you will do with findings when they are created by Inspector. For example, Amazon Inspector has 5 different finding severity levels described here. It is important to understand how quickly you will require teams in your organization to remediate findings. This is likely to depend on severity, as you will want to more quickly respond to critical findings vs informational findings.</li> <li>After understanding response times, it is important to focus on how you will alert on new vulnerabilities. For example, do you have a ticketing system that you would like to integrate with? If so, this is something that AWS Security Hub can assist with. You might want to send these alerts directly to applications teams for remediation, or maybe they need to be cleared through security first. With Inspectors integration with Amazon EventBridge and AWS Security Hub you have many different alerting and tracking options. It is important to understand what works best for your organization and to have this workflow established.</li> <li>Remediation of vulnerabilities should be pushed to application owners as they are the ones who understand the implications of updating packages used by their application code. It would be an anti-pattern for security teams to be responsible for software patching. A two-way communication channel between the security teams should be established for communicating risk, its acceptance and/or mitigation.</li> <li>Ideally at every organization we want to automate as much as possible to save time and potential errors associated with repetitive manual actions from humans. Unfortunately, this can\u2019t also be done 100% of the time, but we should always be working to get there. First work through what services you will use to automate vulnerability remediation such as AWS Systems Manager Patch Manager. Then understand what environments, resources, and CVEs you feel comfortable automatically remediating. The amount of automation that can be created will largely depend on resource timing and ability. There will be an intersection where risk is not great enough to constitute how much time you spent on furthering automation versus working on other high priority projects. Once you start to build this out there are a lot of different resources that can help with automating patch management in AWS. Since there are a number of different resources on this, we have created a dedicated section in the resources section below of valuable resources that you should look at.</li> <li>In addition to automating where possible many customers use Amazon Inspector to find vulnerabilities in resources early in the development lifecycle and remediate before being deployed in a production environment. Using Amazon Inspector in development and staging accounts during development and testing give you the visibility to understand what vulnerabilities exist in applications before deploying to production. There are also multiple blogs that cover how Inspector fits into a CI/CD pipeline in the resources section.</li> </ol> <p> Figure 15: Inspector finding flow</p>"},{"location":"guides/inspector/#software-bill-of-materials-sbom-configuration","title":"Software Bill of Materials (SBOM) Configuration","text":"<p>In Amazon Inspector you can export a Software Bill of Materials or SBOM for short. If you\u2019re not familiar an SBOM it is a nested inventory of all the open source and third-party software components of your codebase. This can help you gain visibility into information about your software supply, such as your commonly used packages, and associated vulnerabilities across your organization.</p> <p>To export SBOMs you need to use the Console or API set this up. Steps on how to configure this can be found in the Inspector SBOM documentation. It is important to keep in mind that this is a one-time export. If you need to do this on a regular basis it is recommended to set up a Lambda Function that uses the create sbom export API on a regular schedule to automatically create these SBOMs. This will help you if you need to look at an SBOM you have an update to date SBOM. This could also be event driven, for example running an SBOM export for an instance when it is created.</p> <p> Figure 16: Inspector SBOM settings</p>"},{"location":"guides/inspector/#suppression-rules","title":"Suppression Rules","text":"<p>You might have CVEs in your environment that are not applicable because of a compensating control or that you\u2019re unable to remediate and have categorized them as an accepted risk. For these circumstances you can create suppression rules in Inspector. You can use suppression rules to automatically exclude Amazon Inspector findings that match specified criteria. For example, you can create a rule to suppress all findings with a low vulnerability score. Suppression rules don't have any impact on the finding itself and don't prevent Amazon Inspector from generating a finding. Suppression rules are only used to filter your list of findings. If Amazon Inspector generates a new finding that matches a suppression rule, the service automatically sets the status of the finding to Suppressed. The findings that match suppression rule criteria won't appear in the console by default.</p> <p>Additionally, if you are using AWS Security Hub as your aggregation point for Inspector in other AWS security services you can use automation rules to address findings. For example, you can use automation rules to upgrade all findings for a particular production account to a severity that warrants an immediate action or downgrades the severity of findings for particular environments that have controls in place that don\u2019t allow public resources. To learn more about AWS Security Hub automation rules refer to the AWS Security Hub automation rules documentation.</p>"},{"location":"guides/inspector/#vulnerability-database-search","title":"Vulnerability Database Search","text":"<p>Amazon Inspector continuously updates its vulnerability database with the latest CVEs to confirm that our customers are able to assess their environments for up-to-date information. From time to time, you might ask \u201cIs Inspector looking for this CVE?\u201d. You can use the vulnerability database search capability in the AWS console to help answer this question, by simply providing a Common Vulnerability and Enumerations (CVE) ID, for example, \u201cCVE-2023-1264\u201c. This allows you to confirm the CVEs covered by Inspector scanning engine and do preliminary research on a CVE.</p> <p> Figure 17: Inspector vulnerability database page</p>"},{"location":"guides/inspector/#cost-considerations","title":"Cost considerations","text":"<p>Amazon Inspector pricing is thoroughly covered in the pricing page covering pricing components, and multiple pricing examples that go through examples of what pricing might look like in your environment. We won\u2019t cover that here and instead focus on two different questions that are frequently asked. 1. My organization is cost conscious or going through a cost optimization exercise, I want to confirm that I am using Amazon Inspector in a cost-effective way. 2. I am going to test Inspector or I want to enable Inspector in my environment but I want to estimate costs before getting started.</p> <p>Amazon Inspector is a cost-effective service that charges you based on usage in your environment. To verify that you are using Inspector cost effectively it is important to understand that since it is usage based you should make sure you do not have extra resources in your environment that are not being used. This will not only save Inspector cost but will potentially save you on Amazon EC2, ECR, and Lambda costs. Secondly each scanning function of Amazon Inspector is optional, so you can choose to use what you need. It is not recommended to disable any of the features as this could potentially cause a lack of visibility into vulnerabilities in your environment and introduce unnecessary challenges, but at every organization there is budgets limits that should be taken into consideration when using Amazon Inspector features. Another option that can be used but should be carefully evaluated is the tag that can be used to exlude scanning for EC2 instances. For more information on how to configure this tag please refer to the scanning Amazon EC2 Instances documentation.</p> <p>The best way to estimate costs with Amazon Inspector is to take advantage of the 15-day free trial. Inspector can be turned on and off across hundreds or thousands of accounts in an organization in a matter of minutes. Once you enable Inspector you will be able to see the costs associated with running Inspector in your organization past the 15-day free trial.</p>"},{"location":"guides/inspector/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/inspector/#troubleshooting-aws-systems-manager-ssm-agent-issues","title":"Troubleshooting AWS Systems Manager (SSM) Agent Issues","text":"<p>There are several issues that might cause the SSM agent to work improperly. You can use the Systems Manager Automation runbook to automatically troubleshoot an EC2 instance that SSM is unable to manage</p> <ul> <li>Open this link to configure the Systems Manager Automation runbook.</li> <li>Navigate to the AWS Region for the EC2 instance you want to troubleshoot.</li> <li>Under the Input parameters, change the dropdown from \"Show managed instances only\" to \"Show all instances\".</li> <li>Select the EC2 instance you want to troubleshoot.</li> <li>Leave all the other settings, and at the bottom of the page, click Execute. This process normally takes up to 5 minutes to complete.</li> <li>Once the automation document has as an Overall status of Success, expand the Outputs section.</li> <li>Review the outputs to see the specific problem with your SSM configuration.</li> </ul>"},{"location":"guides/inspector/#resources","title":"Resources","text":""},{"location":"guides/inspector/#workshops","title":"Workshops","text":"<ul> <li>Activation Days</li> <li>Amazon Inspector workshop</li> <li>Amazon Detective workshop</li> <li>EKS security workshop</li> </ul>"},{"location":"guides/inspector/#demo-videos","title":"Demo videos","text":"<ul> <li>Enhance workload security with agentless scanning and CI/CD integration</li> <li>Inspector overview demo</li> <li>Vulnerability intelligence database search</li> <li>Windows support for continual EC2 vulnerability scanning</li> <li>Software bill of materials export capability</li> <li>Inspector deep inspection of EC2 instances</li> <li>How to use Lambda code scanning</li> <li>AWS Lambda functions support</li> <li>Inspector for Lambda workloads</li> <li>Inspector suppression rules demo</li> </ul>"},{"location":"guides/inspector/#blogs","title":"Blogs","text":"<ul> <li>Use Amazon Inspector to manage your build and deploy pipelines for containerized applications</li> <li>How to scan EC2 AMIs using Amazon Inspector</li> <li>Automate vulnerability management and remediation in AWS using Amazon Inspector and AWS Systems manager part 1</li> <li>Automate vulnerability management and remediation in AWS using Amazon Inspector and AWS Systems manager part 2</li> </ul>"},{"location":"guides/inspector/#other-resources","title":"Other Resources","text":"<ul> <li>Building a scalable vulnerability management program on AWS - Guide</li> </ul>"},{"location":"guides/macie/","title":"Amazon Macie","text":""},{"location":"guides/macie/#introduction","title":"Introduction","text":"<p>Welcome to the Amazon Macie Best Practices Guide. The purpose of this guide is to provide prescriptive guidance for leveraging Amazon Macie for continuous monitoring of your data in your Amazon S3 estate. Publishing this guidance via GitHub will allow for quick iterations to enable timely recommendations that include service enhancements, as well as, the feedback of the user community. This guide is designed to provide value whether you are deploying Macie for the first time in a single account, or looking for ways to optimize Macie in an existing multi-account deployment.</p>"},{"location":"guides/macie/#how-to-use-this-guide","title":"How to use this guide","text":"<p>This guide is geared towards security practitioners who are responsible for monitoring and remediation of threats, malicious activity, and vulnerabilities within AWS accounts (and resources). The best practices are organized into three categories for easier consumption. Each category includes a set of corresponding best practices that begin with a brief overview, followed by detailed steps for implementing the guidance. The topics do not need to be read in a particular order:</p> <ul> <li>What is Amazon Macie</li> <li>What are the benefits of enabling Macie</li> <li>Getting started<ul> <li>Deployment considerations</li> <li>Region considerations</li> </ul> </li> <li>Implementation<ul> <li>Stand alone account enablement</li> <li>Multi-account organization enablement</li> <li>Account coverage</li> <li>Discovery results</li> <li>Publishing to Security Hub</li> <li>Automated sensitive data discovery vs sensitive data discovery jobs</li> <li>Resource coverage</li> </ul> </li> <li>Operationalizing Macie Findings<ul> <li>Action Macie findings</li> <li>Influencing sensitive data detections</li> <li>Suppression rules</li> </ul> </li> <li>Cost considerations</li> <li>Resources</li> </ul>"},{"location":"guides/macie/#what-is-amazon-macie","title":"What is Amazon Macie?","text":"<p>Amazon Macie is a data security service that discovers sensitive data by using machine learning and pattern matching, provides visibility into data security risks, and enables automated protection against those risks.</p> <p>To help you manage the security posture of your organization's Amazon Simple Storage Service (Amazon S3) data estate, Macie provides you with an inventory of your S3 buckets, and automatically evaluates and monitors the buckets for security and access control. If Macie detects a potential issue with the security or privacy of your data, such as a bucket that becomes publicly accessible, Macie generates a finding for you to review and remediate as necessary.</p> <p>Macie also automates discovery and reporting of sensitive data to provide you with a better understanding of the data that your organization stores in Amazon S3. To detect sensitive data, you can use built-in criteria and techniques that Macie provides, custom criteria that you define, or a combination of the two. If Macie detects sensitive data in an S3 object, Macie generates a finding to notify you of the sensitive data that Macie found.</p>"},{"location":"guides/macie/#what-are-the-benefits-of-enabling-macie","title":"What are the benefits of enabling Macie?","text":"<p>Macie is an AWS managed service that automatically provides statistics such as what buckets are publicly accessible, shared outside of your AWS Organization, or amount of buckets that contain sensitive data. These statistics offer insights into the security posture of your Amazon S3 data and where sensitive data might reside in your data estate.  The statistics and data can guide your decisions to perform deeper investigations of specific S3 buckets and objects. Since Macie is a managed service you can spend your time reviewing and analyzing findings, statistics, and other data by using the Amazon Macie console or the Amazon Macie API instead of setting up and managing the infrastructure needed to provide these insights. You can also leverage Macie integration with Amazon EventBridge and AWS Security Hub to monitor, process, and remediate findings by using other services, applications, and systems.</p> <p>Macie makes it easy to do this across your AWS Organization through automation and AWS Organizations integration Macie provides sensitive data discovery and bucket posture assessments across all of your S3 buckets and objects across all accounts within your organization. Giving you centralized visibility across your entire S3 data estate.</p> <p>Some of the most common use cases for Macie are listed below:</p> <ul> <li>Macie automated data discovery and native integration with AWS services such as AWS Security Hub and Amazon EventBridge make discovering and actioning sensitive data more cost effective and time efficient.</li> <li>Company A is being acquired by Company B. As part of the merger, Company A needs to understand the presence of Sensitive data in Company B\u2019s data to ensure they are adhering to compliance mandates.</li> <li>Company A is a data centric company and keeps tons of data for business needs. Company A also wants to ensure that the sensitive data is carefully protected by having an understanding of their sensitive data assets and the access patterns of this data.</li> <li>Your company has had a security issue and you need to understand the type of data that was accessed.</li> </ul>"},{"location":"guides/macie/#getting-started","title":"Getting started","text":"<p>Before getting started with Amazon Macie you need to ensure you have permissions to administer Macie and you should think through which account in your AWS Organization is best suited to be the Macie delegated administrator account. To get started with permissions make sure your role you are using to administer Macie has a minimum of the AWS managed policy name \u201cAmazonMacieFullAccess\u201d.</p>"},{"location":"guides/macie/#deployment-considerations","title":"Deployment Considerations","text":"<p>To deploy Macie across your AWS Organization you will need to enable it in the AWS management account and the security tooling account whether this is done in the Console, CLI, or API. If you are not familiar with the concept of the security tooling account it is recommended to familiarize yourself with the recommended account structure in the Security Reference Architecture. To summarize this is a dedicated account in your AWS Organization that is used as the delegated administrator account for native AWS security services such as Amazon Inspector, Amazon GuardDuty, AWS Security Hub, and Amazon Detective.</p>"},{"location":"guides/macie/#region-considerations","title":"Region Considerations","text":"<p>Amazon Macie is a regional service. This means that to use Macie you will need to enable it in every region that you would like to discover sensitive data. You can enable Macie across all accounts and regions using the AWS API or you can do this by toggling between regions in the console.</p>"},{"location":"guides/macie/#implementation","title":"Implementation","text":"<p>In this section we will cover the minimum requirements for enabling Macie in a stand alone account and in a multi-account organization.</p>"},{"location":"guides/macie/#stand-alone-account-enablement","title":"Stand alone account enablement","text":"<p> Figure 1: Macie Getting Started</p> <p>The first step is to navigate to the Macie console. Once you are on the Macie console you will see a landing page with a getting started button. Click on \u201cGet started\u201d.</p> <p> Figure 2: Macie enablement</p> <p>After clicking on \u201cGet started\u201d you will be brought to the Macie enablement page review the Service role permissions so that you have an understanding on the permissions Macie will need in order to provide its features and functions then select the yellow box labeled \u201cEnable Macie\u201d to enable Macie.</p> <p>Once these two steps are done Macie will be enabled in this account. It is important to refer to sections below to make sure you have enabled all relevant features of Macie.</p>"},{"location":"guides/macie/#multi-account-organization-enablement","title":"Multi-Account organization enablement","text":"<p>When you implement Macie for the first time in your AWS Organization as stated above you will set the delegated administrator in your organization management account in each region that you want to use Macie. Once that is complete there are other steps to completely configure Macie that we will cover in this section.</p> <p> Figure 3: Macie Getting Started</p> <p>The first step is to navigate to the Macie console in your Organizations management account. Once you are on the Macie console you will see a landing page with a getting started button. Click on \u201cGet started\u201d.</p> <p> Figure 4: Macie delegated admin enablement</p> <p>After clicking on \u201cGet started\u201d you will be brought to the Macie enablement page. Next you will need to enter the account ID for the account you want to designate the Macie delegated administrator account. Once you have entered the account ID select \u201cDelegate\u201d. At this point you will switch to the Delegated administrator to finish configuring Amazon Macie across your AWS Organization.</p>"},{"location":"guides/macie/#account-coverage","title":"Account Coverage","text":"<p>Once you set the delegated administrator in the organization management account Macie will be enabled in the delegated administrator account but it will be missing coverage across the accounts that already exist in your organization. So next you will need to go to the account settings or use the API to enable Macie across all member accounts. You will also want to check the \u201cAuto-enable\u201d toggle switch to on. This will ensure that any new accounts in your Organization automatically get Macie enabled, which will make sure you don\u2019t have a lack of visibility and save you manual effort of enabling Macie individually for accounts in your organization. Make sure the status of your accounts says enabled and the \u201cAuto-enabled\u201d toggle switch indicates on like what is pictured below.</p> <p> Figure 5: Macie Account page</p>"},{"location":"guides/macie/#discovery-results","title":"Discovery Results","text":"<p>It is important to set up an S3 bucket for your discovery results within the first 30 days of enabling Macie because Macie stores your sensitive data discovery results for only 90 days. To access the results and enable long-term storage and retention of them, you will need to configure Macie to store the results in an S3 bucket. This will enable you to keep a record of all discovery results going forward. For context, discovery results creates an entry for every file that Macie looked at, even if there was no sensitive data identified. This data can help you confirm certain files were scanned, identify where Macie could not scan a file, and files where sensitive data was found. To learn how to configure this repository, see Storing and retaining sensitive data discovery results.</p>"},{"location":"guides/macie/#publishing-to-security-hub","title":"Publishing to Security Hub","text":"<p>By default Macie automatically publishes policy and sensitive data findings to Amazon EventBridge as events and publishes policy findings to Security Hub. It is recommended to also publish sensitive data findings to Security Hub so that you can take advantage of any workflows or automation that you might have created for other security service findings such as sending findings to a ticketing system or using Lambda to remediate a resource based on a finding in Security Hub.</p> <p> Figure 6: Macie finding publish settings</p>"},{"location":"guides/macie/#automated-sensitive-data-discovery-vs-sensitive-data-discovery-jobs","title":"Automated Sensitive Data Discovery vs Sensitive Data Discovery Jobs","text":"<p>If you have large portions of S3 buckets which are used across your organization (customer service, backend software, 3rd party, host static web contents etc.) then its highly encouraged to enable ADD. ADD intelligently samples and crawl your S3 real estate. It samples very light levels of S3 buckets daily by looking for similar objects and probing a portion of those similar objects. This way even if you have large sets of data in TBs/PBs, you can get a high confidence sensitivity level of your data through ADD.</p> <p> Figure 7: Macie summary page</p> <p>If you need to do a deeper analysis of a particular bucket you also have the ability to run a sensitive data discovery job that allows you to be very specific in the scanning functionality you are looking for. For example you can look for certain managed data identifiers, custom data identifiers, only look for certain file types, or include/exclude data based on certain tags. The large majority of customers will not need to run individual jobs as they will get their results from automated discovery and then dive into certain data sets to learn more.</p> <p>A few use cases to run sensitive data discovery jobs that we hear often are customers who have a data set where they can\u2019t have any uncertainty, so they decide to scan the entirety of this data set giving them the ability to learn where all instances of sensitive data live in a given data set. Another use case would be to would be to run scans based on an ingest of data. Data discovery jobs would give you the ability to target where this data is being ingested and the frequency at which it is scanned. Finally you might want to use custom data identifiers on a smaller subset of data than across your entire S3 estate with automated discovery. Sensitive data discovery jobs would give you the ability to focus this custom data identifier on a smaller set of data.  </p> <p>For more information on how to configure a sensitive data discovery job please refer to the Amazon Macie documentation.</p>"},{"location":"guides/macie/#resource-coverage","title":"Resource Coverage","text":"<p>For Macie to scan data in S3 the data needs to be in a supported file format and be in a bucket that has permissions that allows Macie to scan the data. Most buckets have an explicit allow in the bucket policy but for buckets with explicit deny\u2019s in the bucket policy you will need to add the Macie service linked role to be allowed in the bucket policy. In the resource coverage page Macie lists bucket issues so you can more easily see and address any buckets that Macie is not able to scan.</p> <p> Figure 8: Macie S3 coverage insights</p>"},{"location":"guides/macie/#operationalizing-macie-findings","title":"Operationalizing Macie Findings","text":""},{"location":"guides/macie/#action-macie-findings","title":"Action Macie Findings","text":"<p>The first step we recommend for operationalizing findings from any security service is to understand what details a finding will give you and then understand how you will respond to a finding considering your organizations tooling capabilities and ownership model. For example, do you have a ticketing system you can integrate with to create tickets when a new sensitive data finding is created? Will the account owner automatically receive the ticket and be responsible for remediation? Will triage and alerting be the responsibility of a central security team? These are just a few of the questions that you will need to answer.</p> <p> Figure 9: Macie findings</p> <p>Macie findings give details such as the bucket and object location of the sensitive data finding, they also give information into what was the sensitive data and the location within the given finding making it easier for you to remediate. It is recommended to start out by understanding findings and making account owners and application teams responsible for addressing and cleaning up sensitive data discovered by Macie whether this alerting is done via automation through a ticketing system or email notification. Once you have established a familiarity with sensitive data findings and your environment we would recommend to automate wherever possible to save time and cut down on potential errors created from manual human actions. This could be something such as using Lambda to create a deny all S3 bucket policy for S3 buckets that have been found to have sensitive data. This type of solution will largely depend on the risk appetite of your organization and other factors such as tagging and compensating controls in an environment. We have linked to multiple blogs in the resource section that cover in detail how to create automation based on a Macie finding.</p>"},{"location":"guides/macie/#influencing-sensitive-data-detections","title":"Influencing Sensitive Data Detections","text":"<p>Customers often ask how can I tune what Amazon Macie is looking for. To more effectively tune Macie to look for data in your S3 data estate you have the option to create allow lists, custom data identifiers, and specify which managed data identifiers you would like to look for.</p> <p>Macie has managed data identifiers associated with sensitive data types such as credentials, financial information, personal health information, and personally identifiable information. Custom data identifiers give you the ability to create your own detection criteria that reflects your organization\u2019s particular scenarios, intellectual property, or proprietary data\u2014for example, employee IDs, customer account numbers, or internal data classifications. </p> <p>Allow lists define specific text and text patterns for Macie to ignore when it inspects S3 objects for sensitive data.</p> <p>As pictured below you can see that we can adjust each of these mechanisms in our automated discovery settings. Macie is not charged by the amount of data identifiers that are used so it is recommended to use all managed data identifiers and then adjust for false positives using allow lists. Then if you are needing to find data that is specific to your organization use custom data identifiers to extend the detection capabilities of Macie.</p> <p> Figure 10: Macie managed data identifiers</p>"},{"location":"guides/macie/#suppression-rules","title":"Suppression Rules","text":"<p>To streamline your analysis of findings, you can create and use suppression rules. A suppression rule is a set of attribute-based filter criteria that defines cases where you want Amazon Macie to archive findings automatically. Suppression rules are helpful in situations where you've reviewed a class of findings and don't want to be notified of them again. Refer to the documentation to learn more about how to configure suppression rules.</p>"},{"location":"guides/macie/#cost-considerations","title":"Cost considerations","text":"<p>For the most cost effective way to use Amazon Macie it is recommended to use the automated data discovery capabilities of Macie. This functionality uses sampling techniques to effectively identify and select representative S3 objects in your bucket making sensitive data discovery cost effective. For example Macie Automated Data Discovery can sample data in a 100TB data estate for only a few hunderd dollars per month.</p> <p>If you configure a sensitive data discovery job before creating the job Macie shows you an estimate of the potential cost of running the job based on the amount of data that will be scanned. This can help you understand your job costs and understand what your cost implications are. The usage page in the Macie console gives you the ability to understand your monthly Macie cost associated with your entire environment broken down by preventative control monitoring, sensitive data discovery jobs, and automated sensitive data discovery which is further broken down by object analysis and object monitoring.</p> <p>If you are just getting started with Macie or are enabling it on new accounts Amazon Macie has a 30 day free trialthat covers bucket inventory and up to 150GB per account of automated sensitive data discovery. Keep in mind sensitive data discovery jobs are not included in the 30 day free trial.</p> <p>More details and pricing examples can be found on the Macie pricing page.</p>"},{"location":"guides/macie/#resources","title":"Resources","text":""},{"location":"guides/macie/#workshops","title":"Workshops","text":"<ul> <li>Activation Days</li> <li>Amazon Macie workshop</li> <li>Amazon Detective workshop</li> </ul>"},{"location":"guides/macie/#videos","title":"Videos","text":"<ul> <li>Additional support for discovering more types of sensitive data</li> <li>Fine-tuning sensitive data findings with allow lists</li> <li>Automated Data Discovery Overview</li> <li>One-Click temporary retrieval</li> <li>How Amazon Macie uses keywords to discover sensitive data</li> </ul>"},{"location":"guides/macie/#blogs","title":"Blogs","text":"<ul> <li>How to use Amazon Macie to reduce cost of discovering sensitive data</li> <li>How to query and visualize Macie sensitive data discovery results with Athena and Quicksight</li> <li>How to use Amazon Macie to preview sensitive data in S3 buckets</li> <li>Correlate IAM Access Analyzer findings with Amazon Macie</li> <li>Creating a notification workflow from sensitive data discovery with Amazon Macie, Amazon EventBridge, and Slack</li> <li>Deploy an automated chatops solutions for remediating Amazon Macie findings</li> </ul>"},{"location":"guides/network-firewall/","title":"AWS Network Firewall","text":""},{"location":"guides/network-firewall/#introduction","title":"Introduction","text":"<p>Welcome to the AWS Network Firewall Best Practices Guide. The purpose of this guide is to provide prescriptive guidance for AWS Network Firewall for efficiently protecting your VPCs and their workloads. Publishing this guidance via GitHub will allow for quick iterations to enable timely recommendations that include service enhancements, as well as the feedback of the user community. This guide is designed to provide value whether you are deploying Network Firewall for the first time in a single account, or looking for ways to optimize Network Firewall in an existing multi-account and/or multi-VPC deployment.</p>"},{"location":"guides/network-firewall/#how-to-use-this-guide","title":"How to use this guide","text":"<p>This guide is geared towards security practitioners who are responsible for monitoring and remediation of security events, malicious activity and vulnerabilities within AWS accounts (and resources). The best practices are organized into different categories for easier consumption. Each category includes a set of corresponding best practices that begin with a brief overview, followed by detailed steps for implementing the guidance. The topics do not need to be read in a particular order.</p> <ul> <li>Getting Started</li> <li>Deployment Considerations</li> <li>Implementation</li> <li>Operationalizing</li> <li>Ensure Symmetric Routing</li> <li>Use Strict rule ordering and 'Application drop established' and 'Application alert established' default firewall policy actions</li> <li>Use Stateful rules over Stateless rules</li> <li>Use Custom Suricata rules instead of UI generated rules</li> <li>Use as few Custom Rule Groups as possible</li> <li>Ensure the $HOME_NET variable is set correctly</li> <li>Use Alert rule before Pass rule to log allowed traffic</li> <li>Use \u201cflow:to_server\u201d keyword in stateful rules</li> <li>How to make sure your new Stateful firewall rules apply to existing flows</li> <li>Set up logging and monitoring</li> <li>Options for Mitigating client side TLS SNI manipulation with AWS Network Firewall</li> <li>Cost Considerations</li> <li>Troubleshooting stateless rules for asymmetric forwarding</li> <li>Resources</li> </ul>"},{"location":"guides/network-firewall/#what-is-aws-network-firewall","title":"What is AWS Network Firewall?","text":"<p>AWS Network Firewall is a managed service that makes it easy to deploy essential L3-L7 deep packet inspection protections for all of your Amazon Virtual Private Clouds (VPCs). It can filter traffic at the subnet level of your VPC, including filtering traffic going to and coming from an internet gateway, NAT gateway, over VPN, or AWS Direct Connect.</p>"},{"location":"guides/network-firewall/#what-are-the-benefits-of-enabling-aws-network-firewall","title":"What are the benefits of enabling AWS Network Firewall?","text":"<p>AWS Network Firewall has a highly flexible rule engine so you can build custom firewall rules to protect your unique workloads. It supports thousands of rules, and the rules can be based on port, protocol, and FQDN/domain. AWS Network Firewall supports rules written in Suricata format, giving you the ability to create customized rules based on specific network traffic characteristics, such as packet size or byte match pattern. Network Firewall also offers AWS Managed domain lists and threat signatures so you don\u2019t have to worry about writing and maintaining your own Suricata IPS rules.</p>"},{"location":"guides/network-firewall/#getting-started","title":"Getting started","text":"<p>In this section we will cover what you need to consider before activating AWS Network Firewall in your AWS infrastructure.</p>"},{"location":"guides/network-firewall/#deployment-considerations","title":"Deployment Considerations","text":"<p>When customers first start deploying AWS Network Firewall, they might be tempted to start configuring it right away without looking at all its capabilities, for example deploying endpoints to each VPC, only using managed rules or not using Alert rules. We recommend looking into the Network Firewall documentation as this could be a significant time-saver later down the road.</p> <p>To get started you should understand the three main architecture patterns for Network Firewall deployments and what would be best suite your environment.</p> <ul> <li>Distributed deployment model \u2014 Network Firewall is deployed into each individual VPC.</li> <li>Centralized deployment model \u2014 Network Firewall is deployed into a centralized VPC attached to an instance of AWS Transit Gateway for East-West (VPC-to-VPC) or North-South (inbound and outbound from internet, on-premises) traffic. We refer to this VPC as the inspection VPC.</li> <li>Combined deployment model \u2014 Network Firewall is deployed into a centralized inspection VPC for East-West (VPC-to-VPC) and a subset of North-South (on-premises, egress) traffic. Internet ingress is distributed to VPCs that require dedicated inbound access from the internet, and Network Firewall is deployed accordingly.</li> </ul> <p>See the Deployment models for AWS Network Firewall blog post for further details about deployment models.</p>"},{"location":"guides/network-firewall/#implementation","title":"Implementation","text":"<p>In this section we will cover the minimum requirements for deploying AWS Network Firewall.</p> <p>To deploy Network Firewall, you just need one VPC, one subnet, but for resiliency we highly recommend a firewall endpoint/subnet be deployed for each AZ that you have workloads in.</p> <p></p> <p>Figure 1: Network Firewall VPC Configuration settings</p> <p>If you want to encrypt the Network Firewall configuration data at rest with your own key, you will need to specify a KMS key.</p> <p></p> <p>Figure 2: Network Firewall CMK Configuration</p> <p>For more information on deployment refer to the getting started with Network Firewall documentation</p> <p>If you're implementing AWS Network Firewall into a production environment where you want the least amount of traffic disruption, we recommend you set the Stream exception policy option to \"Continue\" or \"Reject\" and that you not have any default block actions. The Stream exception policy option of \"Drop\" can be more disruptive to production traffic since it silently blocks mid stream flows and does not send a TCP Reset.</p>"},{"location":"guides/network-firewall/#operationalizing","title":"Operationalizing","text":""},{"location":"guides/network-firewall/#ensure-symmetric-routing","title":"Ensure Symmetric Routing","text":"<p>Network Firewall does not support Asymmetric routing so you will need to ensure symmetric routing is configured in your VPC. When you deploy Network Firewall into a VPC, you need to modify the route tables to ensure traffic is sent through firewall endpoints so that it can be inspected. Network Firewall does not support asymmetric routing so the route tables have to account for network flows going to the firewall endpoint in both directions.</p> <p>When using AWS Transit Gateway (TGW) in a centralized deployment configuration and using Network Firewall to inspect East-West traffic between VPCs, the TGW\u2019s appliance mode option needs to be enabled for the attachments in the Inspection VPC. The appliance mode can be enabled in the AWS Console, as well as the API.</p> <p>If appliance mode is not enabled, the return path traffic could land on an endpoint in a different AZ, which will prevent the Network Firewall from correctly evaluating the traffic against the firewall policy.</p>"},{"location":"guides/network-firewall/#use-strict-rule-ordering-and-application-drop-established-and-application-alert-established-default-firewall-policy-actions","title":"Use Strict rule ordering and 'Application drop established' and 'Application alert established' default firewall policy actions","text":"<ul> <li>In Network Firewall there are two options for how the Suricata engine is going to process rules.</li> <li>The \u201cStrict\u201d option is recommended because it instructs Suricata to process the rules in the order you have defined.</li> <li>The \u201cAction Order\u201d option supports Suricata\u2019s default rule processing which is appropriate for IDS use cases but is not a good fit for typical firewall use cases.</li> <li>When selecting Strict rule-ordering you are also able to select a \u201cDefault\u201d action, or actions that are run at the end of your rules and will be applied to any traffic not matching earlier rules.</li> </ul> <p>Figure 3: Network Firewall Stateful Rule evaluation</p>"},{"location":"guides/network-firewall/#use-stateful-rules-over-stateless-rules","title":"Use Stateful rules over Stateless rules","text":"<ul> <li>Stateless rules should be used very sparingly because they can easily cause asymmetric flow forwarding issues (where only one side of the flow is seen by the stateful inspection engine of the firewall) and they tend to make the overall firewall ruleset more complex to understand and troubleshoot. For the large majority of use cases we recommend the stateless engine\u2019s default action be set to \u201cForward to stateful rule groups\u201d and we recommend not having any stateless rules configured since they take precedence over stateful rules.</li> <li>If you are going to use stateless rules, it\u2019s important to understand how to use the Network Firewall\u2019s Stateless Rule Group Analyzer to troubleshoot and resolve asymmetric flow issues. See the \u201cTroubleshooting stateless rules for asymmetric forwarding\u201d</li> <li>Customers should leverage Stateful rules if they want to get the deep packet inspection IPS capabilities of the Network Firewall. Some customers accidentally start with stateless rules only to find out later that they really needed to use stateful rules instead.</li> <li>Stateless rules could be used in the case where you don't want some traffic to be logged or alerted on and simply denied, but for the most part your rule groups should look like this (below) in the AWS Console:</li> </ul> <p>Figure 4: Network Firewall Stateless Rule Groups</p> <ul> <li>Pros of using Stateful rules</li> <li>Return traffic is automatically allowed so there is no need to define both ingress &amp; egress rules for the same flow of traffic</li> <li>Deep packet inspection is supported, which gives you a deeper visibility into layer 7 attributes of the traffic</li> <li>Supports logging so customers can review the full application level details of traffic, as well as the standard 5-tuple flow information</li> <li>These rules are easier to troubleshoot, and they are much more flexible and capable than the stateless rules<ul> <li>Customers can add a description to the rules, such as its creation date (with change request number), use case or other comments</li> </ul> </li> <li>The Reject action is supported</li> <li>The capacity calculation for these rules is easier to work with</li> </ul>"},{"location":"guides/network-firewall/#use-custom-suricata-rules-instead-of-ui-generated-rules","title":"Use Custom Suricata rules instead of UI generated rules","text":"<p>These are configurable under the Stateful rule group options and are a free-form text that you to have full control. They allow you to more easily leverage the full flexibility of Suricata. Here are example Suricata rules that customers have found helpful when getting started.</p> <p></p> <p>Figure 5: Network Firewall Stateful Rule Group</p> <p>We recommend you educate yourself and your team on using custom Suricata rules early in their adoption because often later they will need the power and flexibility of custom Suricata rules to support all their use cases.  </p> <p>The pros of using customer Suricata rules:</p> <ul> <li>Maximum flexibility</li> <li>Control over the alerting and how it shows up in the logs</li> <li>Custom rule signature ID can be used which helps troubleshooting and simplifying log analysis</li> <li>Free-form text rules are easier to copy, edit, share, and backup.</li> <li>Easy to switch rule(s) from one rule group to another (blue-green testing for example)</li> <li>Allow for adding the very important keyword: \u201cflow:to_server\u201d to rules easily</li> </ul> <p>Below we have also included a custom template for an egress security use case to show examples of custom suricata rules.</p> <pre><code># This is a \"Strict rule ordering\" egress security template meant only for the egress use case. These rules would need to be adjusted to accommodate any other use cases. Use this ruleset with \"Strict\" rule ordering firewall policy and no default block action, as this template includes custom default block rules at the end that block everything not explicently allowed.\n# This template will not work well with the \"Drop All\" or \"Drop Established\" default firewall policy actions.\n# Make sure the $HOME_NET variable is set correctly (do this at the firewall policy level so all Rule Groups inherit it)\n\n# Silently allow TCP 3-way handshake to be setup by $HOME_NET clients\n# Do not move this section, it's important that this be at the top of the entire firewall ruleset to reduce rule conflicts\npass tcp $HOME_NET any -&gt; any any (flow:not_established, to_server; sid:202501021;)\npass tcp any any -&gt; $HOME_NET any (flow:not_established, to_client; sid:202501022;)\n\n# Silently turn on JA3/S hash logging for all other tls alert rules (like sid:999991)\nalert tls $HOME_NET any -&gt; any any (ja3.hash; content:!\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"; noalert; flow:to_server; sid:202501024;)\nalert tls any any -&gt; $HOME_NET any (ja3s.hash; content:!\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"; noalert; flow:to_client; sid:202501025;)\n\n# Direct to IP connections\nreject http $HOME_NET any -&gt; any any (http.host; content:\".\"; pcre:\"/^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$/\"; msg:\"HTTP direct to IP via http host header (common malware download technique)\"; flow:to_server; sid:202501026;)\nreject tls $HOME_NET any -&gt; any any (tls.sni; content:\".\"; pcre:\"/^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$/\"; msg:\"TLS direct to IP via TLS SNI (common malware download technique)\"; flow:to_server; sid:202501027;)\n# JA4 No SNI Reject\nreject tls $HOME_NET any -&gt; any any (ja4.hash; content:\"_\"; startswith; content:!\"d\"; offset:3; depth:1; msg:\"JA4 No SNI Reject\"; sid:1297713;)\n\n# Block higher risk Geoip\ndrop ip $HOME_NET any -&gt; any any (msg:\"Egress traffic to RU IP\"; geoip:dst,RU; metadata:geo RU; flow:to_server; sid:202501028;)\ndrop ip $HOME_NET any -&gt; any any (msg:\"Egress traffic to CN IP\"; geoip:dst,CN; metadata:geo CN; flow:to_server; sid:202501029;)\n\n# Block higher risk ccTLDs\nreject tls $HOME_NET any -&gt; any any (tls.sni; content:\".ru\"; nocase; endswith; msg:\"Egress traffic to RU ccTLD\"; flow:to_server; sid:202501036;)\nreject http $HOME_NET any -&gt; any any (http.host; content:\".ru\"; endswith; msg:\"Egress traffic to RU ccTLD\"; flow:to_server; sid:202501037;)\nreject tls $HOME_NET any -&gt; any any (tls.sni; content:\".cn\"; nocase; endswith; msg:\"Egress traffic to CN ccTLD\"; flow:to_server; sid:202501038;)\nreject http $HOME_NET any -&gt; any any (http.host; content:\".cn\"; endswith; msg:\"Egress traffic to CN ccTLD\"; flow:to_server; sid:202501039;)\n\n# Block QUICK traffic\ndrop quic $HOME_NET any -&gt; any any (msg:\"QUIC traffic blocked\"; flow:to_server; sid:3898932;)\n\n# Log higher risk ports\nalert ip $HOME_NET any -&gt; any 53 (msg:\"Possible GuardDuty/DNS Firewall bypass!\"; flow:to_server; sid:202501055;)\nalert ip $HOME_NET any -&gt; any 1389 (msg:\"Possible Log4j callback!\"; flow:to_server; sid:202501059;)\nalert ip $HOME_NET any -&gt; any [4444,666,3389] (msg:\"Egress traffic to high risk port!\"; flow:to_server; sid:202501058;)\n\n# Port/protocol enforcement (TLS can only use TCP/443, TLS can't use anything other than TCP/443, etc.)\nreject tcp $HOME_NET any -&gt; any 443 (msg:\"Egress Port TCP/443 but not TLS\"; app-layer-protocol:!tls; flow:to_server; sid:202501030;)\nreject tls $HOME_NET any -&gt; any !443 (msg:\"Egress TLS but not port TCP/443\"; flow:to_server; sid:202501031;)\nreject tcp $HOME_NET any -&gt; any 80 (msg:\"Egress Port TCP/80 but not HTTP\"; app-layer-protocol:!http; flow:to_server; sid:202501032;)\nreject http $HOME_NET any -&gt; any !80 (msg:\"Egress HTTP but not port TCP/80\"; flow:to_server; sid:202501033;)\nreject tcp $HOME_NET any -&gt; any 22 (msg:\"Egress Port TCP/22 but not SSH\"; app-layer-protocol:!ssh; flow:to_server; sid:202501060;)\nreject ssh $HOME_NET any -&gt; any !22 (msg:\"Egress SSH but not port TCP/22\"; flow:to_server; sid:202501061;)\n\n# Silently (do not log) allow low risk protocols out to anywhere\npass ntp $HOME_NET any -&gt; any 123 (flow:to_server; sid:202501034;)\npass icmp $HOME_NET any -&gt; any any (flow:to_server; sid:202501035;)\n\n# Block high risk TLDs\nreject tls $HOME_NET any -&gt; any any (tls.sni; content:\".xyz\"; nocase; endswith; msg:\"High risk TLD .xyz blocked\"; flow:to_server; sid:202501040;)\nreject http $HOME_NET any -&gt; any any (http.host; content:\".xyz\"; endswith; msg:\"High risk TLD .xyz blocked\"; flow:to_server; sid:202501041;)\nreject tls $HOME_NET any -&gt; any any (tls.sni; content:\".info\"; nocase; endswith; msg:\"High risk TLD .info blocked\"; flow:to_server; sid:202501042;)\nreject http $HOME_NET any -&gt; any any (http.host; content:\".info\"; endswith; msg:\"High risk TLD .info blocked\"; flow:to_server; sid:202501043;)\nreject tls $HOME_NET any -&gt; any any (tls.sni; content:\".top\"; nocase; endswith; msg:\"High risk TLD .top blocked\"; flow:to_server; sid:202501044;)\nreject http $HOME_NET any -&gt; any any (http.host; content:\".top\"; endswith; msg:\"High risk TLD .top blocked\"; flow:to_server; sid:202501045;)\n\n# Alert on requests to possible suspicious TLDs\nalert tls $HOME_NET any -&gt; any any (tls.sni; pcre:\"/^(?!.*\\.(com|org|net|io|edu|aws)$).*/i\"; msg:\"Request to possible suspicious TLDs\"; flow:to_server; sid:202501065;)\nalert http $HOME_NET any -&gt; any any (http.host; pcre:\"/^(?!.*\\.(com|org|net|io|edu|aws)$).*/i\"; msg:\"Request to possible suspicious TLDs\"; flow:to_server; sid:202501066;)\n\n\n# Silently (do not log) allow AWS public service endpoints that we have not setup VPC endpoints for yet\n# VPC endpoints are highly encouraged. They reduce NFW data processing costs and allow for additional security features like VPC endpoint policies.\n\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ec2messages.\"; startswith; nocase; content:\".amazonaws.com\"; endswith; nocase; flow:to_server; sid:202501047;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ssm.\"; startswith; nocase; content:\".amazonaws.com\"; endswith; nocase; flow:to_server; sid:202501048;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ssmmessages.\"; startswith; nocase; content:\".amazonaws.com\"; endswith; nocase; flow:to_server; sid:202501049;)\n\n# Allow-list of strict FQDNs to silently allow\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"checkip.amazonaws.com\"; startswith; nocase; endswith; flow:to_server; sid:202501050;)\npass http $HOME_NET any -&gt; any any (http.host; content:\"checkip.amazonaws.com\"; startswith; endswith; flow:to_server; sid:202501051;)\n\n# Allow-List of strict FQDNs, but still alert on them\n# This method shows the verdict of \"pass\"\nalert tls $HOME_NET any -&gt; any any (tls.sni; content:\"www.example.com\"; startswith; nocase; endswith; msg:\"TLS SNI Allowed\"; flow:to_server; sid:202501052;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"www.example.com\"; startswith; nocase; endswith; flow:to_server; sid:202501053;)\n\n# Allow HTTPS domain and also log it\n# This method shows the verdict of \"alert\" instead of pass\npass tls $HOME_NET any -&gt; any any (alert; msg:\"www.example2.com allowed\"; tls.sni; content:\"www.example2.com\"; startswith; nocase; endswith; flow:to_server; sid:202506131;)\n\n\n# Allow-List of second level/registered domain and all of its subdomains\n# When using 'dotprefix': Always place it before 'content' and always include the leading dot in the domain name (.amazon.com in the following example)\npass tls $HOME_NET any -&gt; any any (tls.sni; dotprefix; content:\".amazon.com\"; nocase; endswith; flow:to_server; sid:202501078;)\n\n# Custom Block Rules\n# These replace \"Drop All\" or \"Drop Established\" default actions\n\n# Egress Default Block Rules\nreject tls $HOME_NET any -&gt; any any (msg:\"Default Egress HTTPS Reject\"; ssl_state:client_hello; ja4.hash; content:\"_\"; flowbits:set,blocked; flow:to_server; sid:999991;)\nalert tls $HOME_NET any -&gt; any any (msg:\"X25519Kyber768\"; flowbits:isnotset,blocked; flowbits:set,X25519Kyber768; noalert; flow:to_server; sid:999993;)\nreject http $HOME_NET any -&gt; any any (msg:\"Default Egress HTTP Reject\"; flowbits:set,blocked; flow:to_server; sid:999992;)\nreject tcp $HOME_NET any -&gt; any any (msg:\"Default Egress TCP Reject\"; flowbits:isnotset,blocked; flowbits:isnotset,X25519Kyber768; flow:to_server; sid:999994;)\ndrop udp $HOME_NET any -&gt; any any (msg:\"Default Egress UDP Drop\"; flow:to_server; sid:999995;)\ndrop icmp $HOME_NET any -&gt; any any (msg:\"Default Egress ICMP Drop\"; flow:to_server; sid:999996;)\ndrop ip $HOME_NET any -&gt; any any (msg:\"Default Egress All Other IP Drop\"; ip_proto:!TCP; ip_proto:!UDP; ip_proto:!ICMP; flow:to_server; sid:999997;)\n\n\n# Ingress Default Block Rules in case ingress traffic lands on this firewall\n# You may want to silence these rules by putting \"noalert\" on them to save on logging costs\ndrop tls any any -&gt; $HOME_NET any (msg:\"Default Ingress HTTPS Drop\"; ssl_state:client_hello; ja4.hash; content:\"_\"; flowbits:set,blocked; flow:to_server; sid:999999;)\nalert tls any any -&gt; $HOME_NET any (msg:\"X25519Kyber768\"; flowbits:isnotset,blocked; flowbits:set,X25519Kyber768; noalert; flow:to_server; sid:9999910;)\ndrop http any any -&gt; $HOME_NET any (msg:\"Default Ingress HTTP Drop\"; flowbits:set,blocked; flow:to_server; sid:9999911;)\ndrop tcp any any -&gt; $HOME_NET any (msg:\"Default Ingress TCP Drop\"; flowbits:isnotset,blocked; flowbits:isnotset,X25519Kyber768; flow:to_server; sid:9999912;)\ndrop udp any any -&gt; $HOME_NET any (msg:\"Default Ingress UDP Drop\"; flow:to_server; sid:9999913;)\ndrop icmp any any -&gt; $HOME_NET any (msg:\"Default Ingress ICMP Drop\"; flow:to_server; sid:9999914;)\ndrop ip any any -&gt; $HOME_NET any (msg:\"Default Ingress All Other IP Drop\"; ip_proto:!TCP; ip_proto:!UDP; ip_proto:!ICMP; flow:to_server; sid:9999915;)\n</code></pre>"},{"location":"guides/network-firewall/#use-as-few-custom-rule-groups-as-possible","title":"Use as few Custom Rule Groups as possible","text":"<p>The reasons for this we have listed below:</p> <ul> <li>When a custom rule group is created, its capacity needs to be defined and extra headroom needs to be taken into account because capacity cannot be modified after a rule group has been created. Having many rule groups creates additional headaches for managing rule capacity limits. For capacity it is recommended to set your custom rule group capcity to whatever leftover capacity you have after implementing your AWS managed rule groups.</li> <li>With several rule groups to manage, understanding how traffic is going to be handled becomes more complex since every rule group needs to be inspected to analyze how it impacts the traffic. Seeing your rules in one view makes it easier to identify if a rule conflicts or overshadows other rules instead of jumping between multiple rule groups to stitch together an understanding of how traffic will be evaluated by the policy.</li> <li>Network Firewall supports a maximum combined total of 20 rule groups (Managed and Custom).  If you create many custom rule groups you will limit how many AWS Managed Rule Groups can also be added.</li> <li>For troubleshooting purposes, you will want to make sure Signature IDs (SIDs) are unique across all rule groups.  Within a single rule group Network Firewall will enforce unique SIDs, but not across all rule groups. If you don\u2019t have unique SIDs across all rule groups then it can be more challenging to understand from the logs which rule actually handled the traffic.</li> </ul>"},{"location":"guides/network-firewall/#ensure-the-home_net-variable-is-set-correctly","title":"Ensure the $HOME_NET variable is set correctly","text":"<p>By default the $HOME_NET variable is set to the CIDR range of the VPC where Network Firewall is deployed.</p> <p></p> <p>Figure 6: Network Firewall HOME_NET Variable</p> <p>However this default behavior might not cover the CIDR ranges of the VPCs you want to protect, like Spoke VPC A and Spoke VPC B in the above example.</p> <p>You want to make sure that the $HOME_NET CIDR range lines up with all your VPCs that you intend to protect and match traffic against. Most customers benefit from setting $HOME_NET to all RFC 1918 IP address ranges (10.0.0.0/8, 172.16.0.0/12, and 192.168.0.0/16).</p> <p>This variable can be set at a global firewall policy level or in each rule group. If it\u2019s set at both levels, the rule group setting wins.</p> <p>The $HOME_NET variable and it\u2019s inverse ($EXTERNAL_NET) are used for matching traffic in AWS managed rules. $EXTERNAL_NET follows $HOME_NET and is always anything outside of $HOME_NET.</p> <p>When using the managed rules for an east/west use case you will want to decide which VPCs/CIDRs you want to protect and assign only those CIDRs to the $HOME_NET variable. If you assign all VPCs/CIDRs then none of those CIDR ranges will be matched by the $EXTERNAL_NET variable in the managed rules. You can also copy out the rules from the threat signatures and adjust the variables to your liking (even replacing the variables by \u201cany\u201c) if you want them to match any/all CIDRs. The downside of doing this is those rules will be static at that point in time and will not be automatically updated like the AWS managed rules.</p> <p>Here is an example custom Suricata rule that can help you identify if you have traffic going through the firewall that is not included in $HOME_NET and perhaps should be:</p> <p>alert tcp !$HOME_NET any -&gt; !$HOME_NET any (flow:to_server,established; msg:\"It looks like you might have $HOME_NET traffic that is not a part of the $HOME_NET variable. Please make sure your $HOME_NET variable is set correctly.\"; sid:39179777;)</p>"},{"location":"guides/network-firewall/#use-alert-rule-before-pass-rule-to-log-allowed-traffic","title":"Use Alert rule before Pass rule to log allowed traffic","text":"<p>If you have a mandate to log all traffic (denied or allowed), you need to add an alert rule for the same traffic as the pass rule before the pass rule itself in your rule group because Pass rules in Suricata simply allow the traffic and do not log it.</p> <pre><code>#Log allowed traffic to https://*.amazonaws.com\nalert tls $HOME_NET any -&gt; any any (tls.sni; content:\".amazonaws.com\"; nocase; endswith; msg:\"*.amazonaws.com allowed by sid:021420242\"; flow:to_server; sid:021420241;)\npass tls $HOME_NET any-&gt; any any (tls.sni; content:\".amazonaws.com\"; nocase; endswith; msg:\"Pass rules don't alert, alert is on sid:021420241\"; flow.to_server; sid:021420242;)\n</code></pre> <p>You need to use Strict Ordering and the Alert rule needs a higher priority than the Pass rule as demonstrated in the code sample above.</p> <p>The SID in the alert rule message can refer the SID of the pass rule and vice-versa. It can be helpful to use longer SIDs so that you can quickly search your logs for that SID without the query showing unrealted information that might also contain that identifier.</p> <p>Alternatively, you can add <code>alert;</code> keyword to pass rules, but they will produce a verdict in the alert log of alert instead of a verdict of pass. Example rule:</p> <pre><code># This method shows the verdict of \"alert\" instead of pass\npass tls $HOME_NET any -&gt; any any (alert; msg:\"www.example2.com allowed\"; tls.sni; content:\"www.example2.com\"; startswith; nocase; endswith; flow:to_server; sid:202506131;)\n</code></pre>"},{"location":"guides/network-firewall/#use-flowto_server-keyword-in-stateful-rules","title":"Use \u201cflow:to_server\u201d keyword in stateful rules","text":"<p>With Suricata, it\u2019s possible to configure conflicting rule sets. When traffic to a destination operates at different layers of the OSI model, traffic we want to allow that is operating at a higher level(for example TLS) might get blocked by a rule that is operating at a lower level. For example TCP:</p>"},{"location":"guides/network-firewall/#example-of-bad-ruleset-strict-rule-ordering-do-not-use","title":"Example of bad ruleset (Strict rule ordering) \u2013 DO NOT USE","text":"<pre><code># Rule 1 is intended to block http traffic to [baddomain.com](http://baddomain.com/)\nreject http $HOME_NET any \u2192 any 80 (http.host; content:\"baddomain.com\"; sid:1;)\n\n# Rule 2 allows the TCP port 80 traffic flow before application protocol inspection\npass tcp $HOME_NET any \u2192 any 80 (sid:2;)\n</code></pre> <p>Using \u201cflow:to_server\u201d in the rules will make them operate at the same level so the traffic can be evaluated at the same time, and the pass rule (sid:2) doesn\u2019t allow the traffic in a way that takes precedence over the reject rule (sid:1) </p>"},{"location":"guides/network-firewall/#example-of-good-ruleset-strict-rule-ordering-ok-to-use","title":"Example of good ruleset (Strict rule ordering) \u2013 Ok to use","text":"<pre><code># Rule 1 will block http traffic to [baddomain.com](http://baddomain.com/)\nreject http $HOME_NET any \u2192 any 80 (http.host; content:\"baddomain.com\"; sid:1;)\n\n# Rule 2 will NOT take precedence over rule 1\npass tcp $HOME_NET any \u2192 any 80 (flow:to_server; sid:2;)\n</code></pre> <p>See Troubleshooting rules in Network Firewall for more information on troubleshooting firewall rules</p>"},{"location":"guides/network-firewall/#how-to-make-sure-your-new-stateful-firewall-rules-apply-to-existing-flows","title":"How to make sure your new Stateful firewall rules apply to existing flows","text":"<p>Network Firewall leverages the Suricata deep packet inspection engine for all Stateful firewall rules. After a flow has been allowed by a Suricata rule, Suricata places that flow in the state table so that it knows it no longer needs to spend resources running deep packet inspection on that flow. For as long as that flow remains active, any new Stateful firewall rules will not apply to that traffic since a decision was already made on that flow. Sometimes you may want your newly added Stateful firewall rules to apply to all traffic, including already active traffic that has been previously allowed through the firewall. For example, perhaps you began setting up the network firewall and started with an, \"allow all traffic\" type of rule, but then as you get further along in the deployment  and testing of network firewall you may want to narrow down your ruleset, and ensure that even already allowed traffic must be processed by your new rules.</p> <p>How to clear the Network Firewall stateful rules state table</p> <ul> <li>Go into the \"Details\" page of your firewall policy</li> <li>Edit the \"Stream exception policy\" to something other than what it is currently set to, and click Save</li> <li>Then edit the \"Stream exception policy\" and set it back to what you had it set to before. In the majority of cases we recommend: \"Stream exception policy: Reject\"</li> </ul> <p>Now any and all traffic, even if it is traffic that was previously allowed, will be re-evaluated against the latest stateful firewall rules.</p>"},{"location":"guides/network-firewall/#set-up-logging-and-monitoring","title":"Set up logging and monitoring","text":"<p>Network Firewall supports two log types, Alert logs and Flow logs</p> <p>Alert logs   * Information from Suricata   * IPS engine   * Layer 7 attributes (like domains)   * Protocol detection</p> <p>Flow logs   * 5=tuple information that flows across the firewall   * Include the volume of traffic   * Helps identify the top producers and consumers of data</p> <p>The native firewall monitoring dashboard provides multiple options for viewing key metrics about your firewall. You can view all the metrics available as part of the dashboard here. </p>"},{"location":"guides/network-firewall/#cost-considerations","title":"Cost considerations","text":"<p>Because each Network Firewall endpoint has hourly charges even if it\u2019s not used, reduce the number of endpoints by leveraging a centralized inspection design and Transit Gateway (TGW).</p> <p>Do not send traffic to Network Firewall that does not need to be inspected. To avoid these unnecessary processing charges on Network Firewall, use TGW route tables to segment your network, for example keeping VPC Prod from talking to VPC Dev if these VPCs don\u2019t need to communicate.</p> <p>Use the traffic analysis report feature to see which domains are most likely driving up network processing charges.</p> <p>Use the free VPC endpoints for S3 and DynamoDB instead of sending that traffic through Network Firewall.</p> <p>Leverage PrivateLink endpoints provided by 3rd party services that do not need to be inspected by the firewall.</p> <p>Ensure route tables are sending traffic to the local Network Firewall endpoint and not to another AZ\u2019s endpoint. This design will avoid incurring cross-AZ data transfer charges.</p> <p>Use DNS Firewall to keep traffic off of Network Firewall. Basic blocks can be configured at the DNS layer for traffic that would otherwise reach Network Firewall, effectively blocking traffic \u201cclosest to the packet source\u201d.</p> <p>You can add <code>\"threshold: type limit, track by_both, seconds 600, count 1;\"</code> to Suricata rules if you want to suppress their logging output to reduce logging costs. For example, the below rule will only alert one time every ten minutes per source and destination IP pair that triggers the rule.</p> <p><code>alert ssh $HOME_NET any -&gt; any any (msg:\"Egress SSH - alert only once every ten minutes\"; threshold: type limit, track by_both, seconds 600, count 1; flow:to_server; sid:898233;)</code></p>"},{"location":"guides/network-firewall/#troubleshooting-stateless-rules-for-asymmetric-forwarding","title":"Troubleshooting stateless rules for asymmetric forwarding","text":"<p>Certain stateless rule configurations can cause traffic to be inspected by the stateful engine in one direction only, most commonly when a stateless \u201cPass\u201d or \u201cForward to stateful rules\u201d is used without a counterpart rule matching the return direction.</p> <p>To identify stateless rules causing this asymmetric forwarding, use the service\u2019s built-in rule analyzer, and then update your rule group to either remove the asymmetric rule or add a rule that matches the return traffic. You can use AWS Management Console to analyze your stateless rule group, or use the API or CLI by calling DescribeRuleGroup and setting the \u201cAnalyzeRuleGroup\u201d option.</p> <p>Here\u2019s an example of how you can analyze your rule group using AWS Management Console. Go to your stateless rule group and click \u201cAnalyze\u201d</p> <p></p> <p>The rule group analyzer identified that stateless rule with priority 2 will lead to asymmetric routing through Network Firewall.</p> <p></p> <p>To fix this issue you can click on \u201cEdit\u201d and add another rule to allow return traffic i.e. from 0.0.0.0/0 to 10.2.0.0/24.</p> <p></p> <p></p> <p>After updating the rules, run the analyzer again to confirm the issue has been resolved.</p> <p></p> <p>Please reach out to AWS Support team if you have any questions.</p>"},{"location":"guides/network-firewall/#options-for-mitigating-client-side-tls-sni-manipulation-with-aws-network-firewall","title":"Options for Mitigating client side TLS SNI manipulation with AWS Network Firewall","text":"<p>TLS SNI filtering is the industry standard mechanism for network appliances to perform domain filtering to control egress traffic. It\u2019s a straightforward and simple way to monitor and control TLS traffic without the need for resolving domains to IP addresses, which can be unreliable and also opens up a potential vulnerability since CDNs are so commonly used for website hosting, and allowing access to a CDN\u2019s IP allows access to all domains hosted on the CDN, if the client is able to craft forged requests. SNI filtering also has a similar limitation, in that if a client is able to craft forged requests, the request could claim to be going to a legitimate domain, but actually connect to an illegitimate IP address instead. In both of these client side SNI manipulation cases a prerequisite is that the system is already to some extent compromised.</p> <ul> <li>So how do we address this?</li> </ul> <p>First and foremost, we concentrate on reducing the likelihood that the workload could be compromised to the extent that SNI manipulation could occur. Leveraging AWS Network Firewall\u2019s Managed Rules, is a good place to start since they block well-known high risk threats. Many AWS customers start here, and then also move towards a least privilege security model where only a short list of legitimate domains is allowed, and all others are blocked by default. A domain allow-list reduces the risk surface substantially, again, further reducing the opportunity for a workload to be compromised and able to be used to send forged requests.</p> <ul> <li>But what if I want to directly block client side SNI manipulation with AWS Network Firewall?</li> </ul> <p>That can be accomplished by enabling AWS Network Firewall\u2019s TLS decryption feature. When TLS decrypt is enabled, client side SNI manipulation is blocked by default and this error message is displayed in the firewall\u2019s TLS log:</p> <pre><code>{\n    \"firewall_name\": \"NetworkFirewall\",\n    \"availability_zone\": \"us-east-1a\",\n    \"event_timestamp\": 1727451885,\n    \"event\": {\n        \"timestamp\": \"2024-09-27T15:44:45.321222Z\",\n        \"src_ip\": \"10.2.1.145\",\n        \"src_port\": \"39038\",\n        \"dest_ip\": \"44.193.128.70\",\n        \"dest_port\": \"443\",\n        \"sni\": \"spoofedsni.com\",\n        \"tls_error\": {\n            \"error_message\": \"SNI: spoofedsni.com Match Failed to server certificate names: checkip.us-east-1.prod.check-ip.aws.a2z.com/checkip.us-east-1.prod.check-ip.aws.a2z.com/checkip.amazonaws.com/checkip.check-ip.aws.a2z.com \"\n        }\n    }\n}\n</code></pre> <ul> <li>But what other options do I have to block client side SNI manipulation?</li> </ul> <p>It\u2019s possible to add both SNI Domain checks and DNS Domain-to-IP checks so that SNI requests are only allowed out to IP addresses that are associated with the domain in DNS. This is an example solution that accomplishes this with Network Firewall. Both of these solutions above have downsides in added cost and/or complexity, so we recommend customers first start by moving towards a Domain Allow-List, and then determine if the workload\u2019s security requirements dictate that more controls should be added.</p> <p>Another capability Network Firewall has in combating TLS SNI spoofing is JA3 filtering. You can think of JA3 as similar to an HTTP User-Agent, but for TLS, and not easily configurable. You can learn more about JA3 here. Let's look at a couple examples of how we can use tls.sni filtering AND ja3.hash filtering together.</p> <p>First let's assume my TLS domain allow-list looks like this:</p> <pre><code>pass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ssm.us-east-1.amazonaws.com\"; nocase; flow:to_server; sid:11111;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ssmmessages.us-east-1.amazonaws.com\"; nocase; flow:to_server; sid:22222;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ec2.us-east-1.amazonaws.com\"; nocase; flow:to_server; sid:33333;)\nreject tls $HOME_NET any -&gt; any any (msg:\"TLS not on domain allow-list blocked\"; flow:to_server; sid:44444;)\n</code></pre> <p>If I wanted to lock it down further so that these specific domains can only be accessed by the JA3 hash of their client agents, I could convert my domain allow-list to look like this:</p> <pre><code>pass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ssm.us-east-1.amazonaws.com\"; nocase; ja3.hash; content:\"7a15285d4efc355608b304698cd7f9ab\"; sid:11111;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ssmmessages.us-east-1.amazonaws.com\"; nocase; ja3.hash; content:\"1be8360b66649edee1de25f81d98ec27\"; sid:22222;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ec2.us-east-1.amazonaws.com\"; nocase; ja3.hash; content:\"fd75aaca18604d62f2bc8b02b345140f\"; sid:33333;)\nreject tls $HOME_NET any -&gt; any any (msg:\"TLS not on domain/JA3 allow-list blocked\"; flow:to_server; sid:44444;)\n</code></pre> <p>With the above ruleset in place, I can no longer curl to a domain on my domain allow-list, because each domain is locked down to only be accessible via the correct JA3. Now in order for SNI spoofing to be successful, not only would the client system need to be compromised to the extent that special commands could be crafted and launched from it, but now the attacker would have to have such deep control of the system that they could even manipulate the specific client agents/processes to make connections out to a domain on the allow-list.</p> <p>Another option is to not be so strict, but instead allow only the 3 JA3s to access any of the SNIs like this:</p> <pre><code>alert tls $HOME_NET any -&gt; any any (ja3.hash; content:\"7a15285d4efc355608b304698cd7f9ab\"; flowbits:set,ja3_allowed; noalert; sid:11111;)\nalert tls $HOME_NET any -&gt; any any (ja3.hash; content:\"1be8360b66649edee1de25f81d98ec27\"; flowbits:set,ja3_allowed; noalert; sid:22222;)\nalert tls $HOME_NET any -&gt; any any (ja3.hash; content:\"fd75aaca18604d62f2bc8b02b345140f\"; flowbits:set,ja3_allowed; noalert; sid:33333;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ssm.us-east-1.amazonaws.com\"; nocase; flowbits:isset,ja3_allowed; sid:44444;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ssmmessages.us-east-1.amazonaws.com\"; flowbits:isset,ja3_allowed; nocase; sid:55555;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ec2.us-east-1.amazonaws.com\"; nocase; flowbits:isset,ja3_allowed; sid:66666;)\nreject tls $HOME_NET any -&gt; any any (msg:\"TLS not on domain/JA3 allow-list blocked\"; flow:to_server; sid:77777;)\n</code></pre> <p>The above two options have drawbacks in that they require that any time there is a client agent upgrade the new JA3 hash be added to the allow-list before it'll work. This is added complexity for added security value.</p> <p>But how can we allow for a little bit of flexibility to ease the management of the allow-lists while still continuing to reduce the risk surface?</p> <p>Another option is to track which destination IPs have been contacted by approved JA3 hashes, remember them for a period of time, and then only let our TLS domain allow list work out to those higher trust destination IPs. Here is what that ruleset might look like:</p> <pre><code>alert tls $HOME_NET any -&gt; any any (ja3.hash; content:\"7a15285d4efc355608b304698cd7f9ab\"; xbits:set, allowed_ja3_destination_ips, track ip_dst, expire 21600; sid:11111;)\nalert tls $HOME_NET any -&gt; any any (ja3.hash; content:\"1be8360b66649edee1de25f81d98ec27\"; xbits:set, allowed_ja3_destination_ips, track ip_dst, expire 21600; sid:22222;)\nalert tls $HOME_NET any -&gt; any any (ja3.hash; content:\"fd75aaca18604d62f2bc8b02b345140f\"; xbits:set, allowed_ja3_destination_ips, track ip_dst, expire 21600; sid:33333;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ssm.us-east-1.amazonaws.com\"; nocase; xbits:isset, allowed_ja3_destination_ips, track ip_dst; sid:44444;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ssmmessages.us-east-1.amazonaws.com\"; xbits:isset, allowed_ja3_destination_ips, track ip_dst; nocase; sid:55555;)\npass tls $HOME_NET any -&gt; any any (tls.sni; content:\"ec2.us-east-1.amazonaws.com\"; nocase; xbits:isset, allowed_ja3_destination_ips, track ip_dst; sid:66666;)\nreject tls $HOME_NET any -&gt; any any (msg:\"TLS not on domain/A3 allow-list blocked\"; flow:to_server; sid:77777;)\n</code></pre> <p>The above option will let a new JA3 seen access the TLS domain allow list, but only if the request is going to an IP that an approved JA3 has already been talking to. This can provide some flexibility without providing too much flexibility. Each customer will have to determine if their specific application's threat model justifies the added overhead of maintaining a JA3 allow-list. Most AWS Network Firewall customers are satisfied with the significant risk reduction of a domain allow-list, and don't find it necessary to also lock down the domains to an allow-list of JA3 hashes too.</p>"},{"location":"guides/network-firewall/#resources","title":"Resources","text":""},{"location":"guides/network-firewall/#workshops","title":"Workshops","text":"<ul> <li>AWS Network Firewall Workshop</li> <li>Egress Controls Workshop</li> </ul>"},{"location":"guides/network-firewall/#videos","title":"Videos","text":"<ul> <li>Introduction, Best Practices and Custom Suricata Rules</li> <li>AWS Network Firewall console experience</li> <li>Decrypt, inspect, and re-encrypt TLS egress traffic at scale</li> <li>Decrypt, inspect, and re-encrypt TLS traffic at scale</li> <li>AWS Network Fireall Suricata HOME_NET variable override</li> <li>AWS Network Firewall support for reject action for TCP traffic</li> <li>AWS Network Firewall tag-based resource groups</li> <li>AWS re:Inforce 2023 - Firewalls, and where to put them (NIS306)</li> </ul>"},{"location":"guides/network-firewall/#blogs","title":"Blogs","text":"<ul> <li>Deployment models</li> <li>Cost considerations and common options for AWS Network Firewall log management</li> <li>TLS inspection configuration for encrypted traffic and AWS Network Firewall</li> <li>How to control non-HTTP and non-HTTPS traffic to a DNS domain with AWS Network Firewall and AWS Lambda</li> <li>Use AWS Network Firewall to filter outbound HTTPS traffic from applications hosted on Amazon EKS and collect hostnames provided by SNI</li> <li>How to deploy AWS Network Firewall by using AWS Firewall Manager</li> <li>Introducing Prefix Lists in AWS Network Firewall Stateful Rule Groups</li> <li>How to analyze AWS Network Firewall logs using Amazon OpenSearch Service \u2013 Part 1</li> <li>How to analyze AWS Network Firewall logs using Amazon OpenSearch Service \u2013 Part 2</li> </ul>"},{"location":"guides/network-firewall/#sample-code","title":"Sample Code","text":"<ul> <li>AWS Network Firewall CloudWatch Dashboard</li> <li>AWS Network Firewall Automation Examples</li> </ul>"},{"location":"guides/security-hub/","title":"Security Hub","text":""},{"location":"guides/security-hub/#introduction","title":"Introduction","text":"<p>Welcome to the AWS Security Hub Best Practices Guide. The purpose of this guide is to provide prescriptive guidance for leveraging AWS Security Hub for automated, continuous security best practice checks against your AWS resources. Publishing this guidance via GitHub will allow for quick iterations to enable timely recommendations that include service enhancements, as well as, the feedback of the user community. This guide is designed to provide value whether you are deploying Security Hub for the first time in a single account, or looking for ways to optimize Security Hub in an existing multi-account deployment.</p>"},{"location":"guides/security-hub/#how-to-use-this-guide","title":"How to use this guide","text":"<p>This guide is geared towards security practitioners who are responsible for monitoring and remediation of threats and malicious activity within AWS accounts (and resources). The best practices are organized into three categories for easier consumption. Each category includes a set of corresponding best practices that begin with a brief overview, followed by detailed steps for implementing the guidance. The topics do not need to be read in a particular order:</p> <ul> <li>What is Security Hub</li> <li>What are the benefits of enabling Security Hub</li> <li>Getting Started<ul> <li>Deployment considerations</li> <li>Region considerations</li> </ul> </li> <li>Implementations<ul> <li>Configuration</li> <li>Integrate your security tools</li> <li>Enable security standards</li> </ul> </li> <li>Operationalizing<ul> <li>Take action on critical and high findings</li> <li>Create customized insights</li> <li>Leverage available remediation instructions</li> <li>Fine tuning security standard controls</li> <li>Automation rules</li> <li>Automated response and remediation</li> <li>3rd party integrations</li> </ul> </li> <li>Cost considerations<ul> <li>AWS Config</li> </ul> </li> <li>Resources</li> </ul>"},{"location":"guides/security-hub/#what-is-security-hub","title":"What is Security Hub?","text":"<p>AWS Security Hub is a cloud security posture management (CSPM) service that performs automated, continuous security best practice checks against your AWS resources to help you identify misconfigurations, and aggregates your security alerts (i.e. findings) in a standardized format so that you can more easily enrich, investigate, and remediate them. It can be used by security teams, compliance teams, cloud architects, incident response teams, risk management teams, and MSSPs. Security Hub is currently used by customers of all sizes ranging from small startups to large enterprises.</p>"},{"location":"guides/security-hub/#what-are-the-benefits-of-enabling-security-hub","title":"What are the benefits of enabling Security Hub?","text":"<p>Security Hub reduces the complexity and effort of managing and improving the security of your AWS accounts, workloads, and resources. You can enable Security Hub within a particular Region in minutes, and the service helps you answer fundamental security questions you may have on a daily basis. Key benefits include:</p> <ul> <li>Detect deviations from security best practices with a single click. Security Hub runs continuous and automated account and resource-level configuration checks against the controls in the AWS Foundational Security Best Practices standard and other supported industry best practices and standards, including CIS AWS Foundations Benchmark, National Institute of Standards and Technology (NIST), AWS Resource Tagging Standard, and Payment Card Industry Data Security Standard (PCI DSS). Learn more about supported standards and controls available in Security Hub.</li> <li>Automatically aggregate security findings in a standardized data format from AWS and partner services. Security Hub collects findings from the security services enabled across your AWS accounts, such as threat detection findings from Amazon GuardDuty, vulnerability findings from Amazon Inspector, and sensitive data findings from Amazon Macie. Security Hub also collects findings from partner security products using a standardized AWS Security Finding Format, eliminating the need for time-consuming data parsing and normalization efforts. Customers can designate an administrator account that can access all findings across their accounts.</li> <li>Accelerate mean time to resolution with automated response and remediation actions. Create custom automated response, remediation, and enrichment workflows using the Security Hub integration with Amazon EventBridge, and other integrations to create Security Orchestration Automation and Response (SOAR) and Security Information and Event Management (SIEM) workflows. You can also use Security Hub Automation Rules to automatically update or suppress findings in near-real time.</li> </ul>"},{"location":"guides/security-hub/#getting-started","title":"Getting started","text":"<p>Some important considerations for AWS Security Hub is that you need to ensure you have the right permissions to administer Security Hub and you should think through which account in your AWS Organization is best suited to be the Security Hub delegated administrator account. To get started with permissions make sure the role you are using to administer Security Hub has a minimum of the AWS managed policy name \u201cAWSSecurityHubFullAccess\u201d. Another consideration is to make sure AWS Config is enabled on all accounts because AWS Security Hub uses service-linked AWS Config rules to perform most of its security checks for controls. Please refer to this document for more details.</p>"},{"location":"guides/security-hub/#deployment-considerations","title":"Deployment Considerations","text":"<p>To deploy Security Hub across your AWS Organization you will need to enable it in the AWS management and the security tooling account whether this is done in the Console, CLI, or API. If you are not familiar with the concept of the security tooling account it is recommended to familiarize yourself with the recommended account structure in the Security Reference Architecture. To summarize, this is a dedicated account in your AWS Organization that is used as the delegated administrator account for native AWS security services such as Amazon Inspector, Amazon GuardDuty, Amazon Macie, and Amazon Detective.</p>"},{"location":"guides/security-hub/#region-considerations","title":"Region Considerations","text":"<p>Amazon Security Hub is a regional service. This means that to use Security Hub you will need to enable it in every region that you would like to leverage Security Hub. You can enable Security Hub across all accounts and regions using the AWS API or you can do this by toggling between regions in the console. One other feature AWS Security Hub provides is called Cross-Region aggregation where you can aggregate findings, findings updates, insights, control compliance statuses, and security scores from multiple Regions to a single aggregation Region of your choice. You can then manage all of this data from the aggregation Region simplifying cross region deployments.</p>"},{"location":"guides/security-hub/#implementation","title":"Implementation","text":"<p>When you implement Security Hub for the first time in your AWS Organization, as stated above, you will set the delegated administrator in your organization management account in each region that you want to use Security Hub for more information on this process refer to the Security Hub documentation or follow the steps below. Once that is complete, there are other steps to completely configure Security Hub that we will cover in this guide.</p>"},{"location":"guides/security-hub/#configuration","title":"Configuration","text":"<p>Once you set the delegated administrator in the organization management account Security Hub will be enabled but it will be missing coverage across all of the existing accounts in your organization. So next you will need to go to the account configuration settings or use the API to enable Security Hub across all member accounts using a Security Hub central configuration policy. This configuration policy will enable you to specify what accounts, organizational units, or regions have Security Hub enabled and also which standards you want enabled. The default policy options will select all regions, all accounts, enable the AWS Foundational Security Best Practices standard, and enable this same configuration for any new accounts added in the AWS Organization. This will ensure you don\u2019t have a lack of visibility and save you manual effort of enabling Security Hub individually for accounts in your organization moving forward. Using central configuration will also set the finding aggregation to your region that was used when setting the central configuration.</p> <p> Figure 1: Security Hub accounts page</p> <p>You can choose to not use central configuration and instead use Local configuration to new organization accounts, but with this feature you must configure settings manually in each account and region. If you choose to use local configuration or have not started using central configuration you can still configure an aggregation region following the Security Hub documentation.</p>"},{"location":"guides/security-hub/#integrate-your-security-tools","title":"Integrate your security tools","text":"<p>You can integrate different security tools into Security Hub. This includes ingesting different findings into Security Hub from different data sources such as supported AWS services, 3rd party vendors, your AWS custom config rules, and even your own custom applications. The Security Hub Console integration page provides details on each integration and what you need to do to enable them. Not only that, you can forward these findings managed by Security Hub into other AWS services or integrated 3rd party tools such as supported ticketing systems, chat software, or a SIEM solution for alerting and management of findings.</p> <p> Figure 2: Security Hub Finding Flow</p>"},{"location":"guides/security-hub/#enable-security-standards","title":"Enable Security Standards","text":"<p>By default, when you enable Security Hub using the default policy, the AWS Foundational Best Practices is selected. We recommend that you start with the defaults and then enable any other security standards that you need to meet your business needs such as the CIS or NIST standards.</p> <p> Figure 3: Security Hub Standards</p>"},{"location":"guides/security-hub/#operationalizing","title":"Operationalizing","text":""},{"location":"guides/security-hub/#take-action-on-critical-and-high-findings","title":"Take action on CRITICAL and HIGH Findings","text":"<p>Most customers focus on the critical and high severity findings as a priority to respond to. We recommend you use the filter option as mentioned in these steps and pictured below. Once you understand what are your critical and high findings you will be able to understand the types of findings you will be responding too. You can then create the necessary runbooks and automation to complete this work.</p> <ul> <li>Filter Findings on Severity label and Status. Keep in mind filters are case sensitive.</li> <li>Review and Remediate.</li> </ul> <p> Figure 4: Security Hub finding</p>"},{"location":"guides/security-hub/#create-customized-insights","title":"Create Customized Insights","text":"<p>AWS Security Hub has Insights allow you to view your findings through different visualizaktions. Most customers use the default Insights and create custom insights to focus on finding trends that require attention. Below are some best practices of creating insights:</p> <ul> <li>Create insights with the context from your environment.</li> <li>Create insights by using the \u2018Group By\u2019 filter, for example use \u2018ResourceType\u2019 which groups findings by AWS resource or you can use AWS account ID which groups findings by AWS account in multi-account setup.</li> <li>Add filters before using \u2018Group By\u2019 to focus Insight. For example, Status EQUALS FAILED.</li> <li>Create insights that help you visualize and track progress of security programs your teams are working on. For example, reduction of critical vulnerabilities over time.</li> </ul>"},{"location":"guides/security-hub/#leverage-available-remediation-instructions","title":"Leverage available remediation instructions","text":"<p>Each Security Hub finding from a Security or Compliance Standard has an associated remediation instructions. This can provide valuable insights into how to respond to any given finding.</p> <p> Figure 5: Security Hub Finding Remediation guidance</p>"},{"location":"guides/security-hub/#fine-tuning-security-standard-controls","title":"Fine tuning Security Standard controls","text":"<p>Some customers when enabling a security standard might have one or more controls that are not applicable to their environment. For such findings you might want to disable them and add a note of why for historical reference. This can be done by selecting the control and clicking on the disable control button as shown below. This will stop generating additional findings for that control. As for the existing findings, they are archived automatically after 3-5 days.</p> <p> Figure 6: Security Hub control status</p> <p>Some Security Hub controls use parameters that affect how the control is evaluated. Typically, such controls are evaluated against the default parameter values that Security Hub defines. However, for a subset of these controls, you can customize the parameter values. When you customize a parameter value for a control, Security Hub starts evaluating the control against the value that you specify. This is a great feature to leverage if you need to update a control to specific information applicable to your environment. Custom control parameters can be configured at a single or multi-account level using configuration policies as shown below. Refer to the documentation for more information about configuring custom control parameters.</p> <p> Figure 7: Security Hub custom control parameter policy configuration</p>"},{"location":"guides/security-hub/#automation-rules","title":"Automation Rules","text":"<p>Automation Rules allows you to automatically update or suppress Security Hub findings, without any code. With this feature, rules are created by admininstrators to streamline cloud security posture management and act on findings in all accounts under the organization. This happens in near-real time as rules run at the time of finding ingestion. This can assist in removing repetitive tasks for security teams and reduce mean time to respond. For example, one of the most common customer uses cases is to elevate findings severity for top production accounts where you need to focus on, rather than the same findings identified on developers account. You can elevate the severity level of findings related to the production account IDs to a higher severity or you can lower the severity of the findings related to developers account IDs so your team can only focus on the important findings. Another use case is to use resource tags to further understand what resources are associated with a finding and what you want the automation rule to do with the finding.</p> <p> Figure 8: Security Hub Automation Rules page</p> <p>Here are some examples when it comes to automation rules:</p> <ul> <li>Change findings severity from HIGH to CRITICAL if the findings affect specific production accounts.</li> <li>Change the Security Hub findings with \u2018Informational\u2019 severity label to \u201cSuppressed\u201d workflow status.</li> <li>Set finding severity to CRITICAL if the finding\u2019s resource ID refers to a specific resource (for example, S3 buckets with PII).</li> </ul> <p>For example, let\u2019s say we created an automation rule to elevate severity of a finding for production environments from high to critical and then If a criteria is matched as part of this automated rule, an automated action will be taken as shown in the image below where the finding severity will be raised from high to critical.</p> <p> Figure 9: Security Hub Automation Rule</p> <p>Here are some considerations when it comes to automation rules:</p> <ul> <li>Including a rule description allows teams to provide context to responders and resource owners.</li> <li>Only the Security Hub admin account can create, delete, edit, and view automation rules.</li> <li>Automated remediation must be created in each region in Security Hub admin account.</li> <li>Define criteria and include member account ids.</li> <li>Security Hub updates control findings every 12-24 hours or when the associated resource changes state.</li> <li>Rule order matters - multiple rules may apply to same finding or finding field. Lowest numerical value first.</li> <li>If multiple findings have the same rule order, Security Hub applies a rule with an earlier value for the UpdatedAt field first (that is, the rule which was most recently edited is applied last).</li> <li>Security Hub currently supports a maximum of 100 automation rules for an administrator account.</li> </ul>"},{"location":"guides/security-hub/#automated-security-response","title":"Automated Security Response","text":"<p>This AWS Solution is an add-on that works with AWS Security Hub and provides predefined response and remediation actions based on industry compliance standards and best practices for security threats. It helps Security Hub customers to resolve common security findings and to improve their security posture in AWS. For more details about it, please refer to this document.</p> <p> Figure 10: Automated Security Response Diagram</p>"},{"location":"guides/security-hub/#3rd-party-integrations","title":"3rd party Integrations","text":"<p>Integration with 3rd party supported partners is available within Security Hub. One example of using integration to automate responses is forwarding findings to a ticketing system. for example, ServiceNow ITSM integration with Security Hub allows security findings from Security Hub to be viewed within ServiceNow ITSM. You can also configure ServiceNow to automatically create an incident or problem when it receives a finding from Security Hub. Any updates to these incidents and problems result in updates to the findings in Security Hub.</p> <p> Figure 11: Security Hub and ServiceNow integration diagram</p>"},{"location":"guides/security-hub/#cost-considerations","title":"Cost Considerations","text":"<p>Security Hub is priced along three dimensions: the quantity of security checks, the quantity of finding ingestion events, and the quantity of rule evaluations processed per month. With AWS Organizations support, Security Hub allows you to connect multiple AWS accounts and consolidate findings across those accounts to enjoy tiered pricing for your entire organization\u2019s security checks, finding ingestion events, and automation rule evaluations.</p> <p>If you are just getting started with Security Hub or are enabling it on new accounts. AWS Security Hub has a 30 day free trial. The trial includes the complete Security Hub feature set and security best practice checks. Every AWS account in each Region that is enabled with Security Hub receives a free trial. During the free trial, you will get an estimate of your monthly bill if you were to continue to use Security Hub across the same accounts and Regions.</p> <ul> <li>AWS Foundational Security Best Practices and CIS standards are on by default</li> <li>When you enable a standard, \u201call\u201d of the controls for that standard are enabled by default.   You can then disable and enable specific controls within an enabled standard, or disable the entire standard.  </li> <li>Note there is a tradeoff: lowering costs through disabling controls could lead to potential higher risks due to a lack of visibility.</li> <li>Turn off controls that deal with global resources in all regions except for the region that runs global recording. Examples:</li> <li>You can disable CIS 2.3 and 2.6 controls related to CloudTrail logging in all accounts and Regions except for the account and Region where a centralized S3 bucket is located.</li> <li>Disable CIS 1.2-1.14, 1.16, 1.22, and 2.5 controls that deal with global resources in all Regions except for the Region that runs global recording.</li> <li>Filter findings from integrations that are not useful and feeding into Security Hub. This will reduce the cost of finding ingestion events.</li> <li>Turn off security checks potentially not relevant to your environment. For example, if you do not plan to use Lambda inside of a VPC because you have compensating controls such as a strict CI/CD process and daily rollouts. But also take into consideration that in this example, you need to have a mechanism in place (Such as allow/deny list SCP) to control the use of services such as Lambda.</li> </ul>"},{"location":"guides/security-hub/#aws-config","title":"AWS Config","text":"<p>With AWS Config, you are charged based on the number of configuration items recorded, the number of active AWS Config rule evaluations, and the number of conformance pack evaluations in your account. A configuration item is a record of the configuration state of a resource in your AWS account. An AWS Config rule evaluation is a compliance state evaluation of a resource by an AWS Config rule in your AWS account. A conformance pack evaluation is the evaluation of a resource by an AWS Config rule within the conformance pack.</p> <ul> <li>Record global resources only in required region. Which will reduce the number of configuration items recorded</li> <li>Turn off compliance history timeline if you are not using Config outside of Security Hub \u2013 tracks the history of each individual resource compliance status. This is on by default if you have enabled recording all resource types in config</li> <li>Disable recording of resources not supported by Security Hub Controls \u2013 May introduce risk if using Config as CMDB. Also not recommended to disable recording of resources type you do use in your environment considering Security Hub control expansion</li> <li>There are other tips mentioned in this AWS Config cost optimization for Security Hub</li> </ul> <p>Important note to mention here is that AWS Config is NOT part of the 30 days trial version for Security Hub. So when you want to enable Security Standards on Security Hub, you will be charged for any AWS config rules used.</p>"},{"location":"guides/security-hub/#resources","title":"Resources","text":""},{"location":"guides/security-hub/#workshops","title":"Workshops","text":"<ul> <li>Activation Days</li> <li>Threat Detection and Response workshop</li> <li>Amazon Detective workshop</li> <li>EKS security workshop</li> <li>Amazon Macie workshop</li> </ul>"},{"location":"guides/security-hub/#videos","title":"Videos","text":"<ul> <li>Customize and contextualize security with AWS Security Hub</li> <li>AWS Security Hub - Bidirectional integration with ServiceNow ITSM</li> <li>Re:inforce Security Hub Automation Rules</li> <li>AWS Security Hub integration with AWS Control Tower</li> <li>AWS Security Hub automation rules</li> <li>Using Security Hub finding history feature</li> <li>Subscribing to Security Hub announcements</li> <li>Visualize Security Hub findings using Amazon Quicksight</li> <li>Cross-region finding aggregation</li> <li>Bidirectional integration with Atlassian Jira Service Management</li> </ul>"},{"location":"guides/security-hub/#blogs","title":"Blogs","text":"<ul> <li>Optimize AWS Config for AWS Security Hub to effectively manage your cloud security posture</li> <li>Consolidating controls in Security Hub: The new controls view and consolidated findings</li> <li>AWS Security Hub launches a new capability for automating actions to update findings</li> <li>Get details on security finding changes with the new Finding History feature in Security Hub</li> <li>Three recurring Security Hub usage patterns and how to deploy them</li> <li>How to subscribe to the new Security Hub Announcements topic for Amazon SNS</li> <li>How to export AWS Security Hub findings to CSV format</li> <li>Automatically block suspicious DNS activity with Amazon GuardDuty and Route 53 Resolver DNS Firewall</li> <li>How to build a multi-Region AWS Security Hub analytic pipeline and visualize Security Hub data</li> <li>How to enrich AWS Security Hub findings with account metadata</li> </ul>"},{"location":"guides/security-lake/","title":"Security Lake","text":""},{"location":"guides/security-lake/#introduction","title":"Introduction","text":"<p>Welcome to the Amazon Security Lake Best Practices Guide. The purpose of this guide is to provide prescriptive guidance for leveraging Amazon Security Lake to centralize your security data in a purpose-built data lake that's stored in your AWS account. Publishing this guidance via GitHub will allow for quick iterations to enable timely recommendations that include service enhancements and the feedback of the user community. This guide is designed to provide value whether you are deploying Security Lake for the first time, or looking for ways to optimize Security Lake in an existing multi-account deployment. </p>"},{"location":"guides/security-lake/#what-is-amazon-security-lake","title":"What is Amazon Security Lake?","text":"<p>Amazon Security Lake is a fully managed security data lake service. You can use Security Lake to automatically centralize security data from AWS environments, SaaS providers, on premises, cloud sources, and third-party sources into a purpose-built data lake that's stored in your AWS account. It is backed by Amazon Simple Storage Service (Amazon S3) buckets, meaning you retain ownership of the data you collect. Security Lake simplifies collecting security data and gathering insights from it. This supports a wide range of use cases with the ultimate goal of improving the protection of your workloads, applications, and data.</p>"},{"location":"guides/security-lake/#what-are-the-benefits-of-enabling-security-lake","title":"What are the benefits of enabling Security Lake?","text":"<p>Security Lake automates the collection of security-related log and event data from integrated AWS services and third-party services. It also helps you manage the lifecycle of data with customizable retention and replication settings. Security Lake converts ingested data into Apache Parquet format and a standard open-source schema called the Open Cybersecurity Schema Framework (OCSF). With OCSF support, Security Lake normalizes and combines security data from AWS and a broad range of enterprise security data sources.</p> <p>Other AWS services and third-party services can subscribe to the data that's stored in Security Lake for incident response and security data analytics.</p>"},{"location":"guides/security-lake/#how-to-use-this-guide","title":"How to use this guide","text":"<p>This guide is geared towards security practitioners who are responsible for collecting and normalizing security logs across AWS and hybrid environments. The best practices are presented in the order they should be considered for a Security Lake deployment for easier consumption. However, the topics can be consumed in any order necessary. </p> <ul> <li>Pre-requisites for deploying Security Lake<ul> <li>Permissions and roles</li> <li>Delegated administration</li> <li>CloudTrail management events</li> </ul> </li> <li>Plan your deployment<ul> <li>What regions to collect security data from</li> <li>Native data sources</li> <li>Security Lake partners</li> <li>Custom data sources</li> </ul> </li> <li>Implementing Security Lake</li> <li>Fine tuning your Security Lake implementation<ul> <li>Changes by log source</li> <li>Changes by region</li> <li>Changes by account and region</li> <li>Adding regions and accounts</li> <li>Changing S3 lifecycle directly</li> </ul> </li> <li>Configure collection of third-party data<ul> <li>Partner sources</li> <li>Using Security Hub to route findings to Security Lake</li> <li>Custom sources</li> </ul> </li> <li>Operationalizing Security Lake<ul> <li>AWS native analytics</li> <li>Subscriber partners</li> <li>OCSF schema updates</li> </ul> </li> <li>Cost considerations<ul> <li>Cost breakdown and visibility</li> <li>Built in cost savings</li> <li>Confirm you are not collecting duplicate native logs</li> <li>CloudTrail management event cost savings</li> <li>Rollup regions</li> <li>Moving logs to glacier</li> </ul> </li> <li>Additional resources</li> </ul>"},{"location":"guides/security-lake/#pre-requisites-for-deploying-security-lake","title":"Pre-requisites for deploying Security Lake","text":"<p>In order to deploy Security Lake, you will need to address the following pre-requisites ahead of time. </p>"},{"location":"guides/security-lake/#permissions-and-roles","title":"Permissions and roles","text":"<p>Before getting started with Amazon Security Lake you need to ensure you have permissions to administer the service. You can use the AWS Managed Policy called \u201cAmazonSecuriyLakeAdministrator\u201d, but always keep least privilege in mind. If you plan to enable or configure Security Lake programmatically, or plan to use the API or CLI to access it, you must create the IAM roles noted here . If you use the console to enable and configure Security Lake, these roles will be managed for you. While not necessary, we generally recommend customers deploy Security Lake using the console for this reason. </p>"},{"location":"guides/security-lake/#delegated-administration","title":"Delegated administration","text":"<p>You will need to use your AWS Organization Management Account to designate a Security Lake delegated administrator account. You should think through which account in your AWS Organization is best suited to use as the Security Lake delegated administrator. The AWS Security Reference Architecture recommends using the Log Archive account as the delegated administration for Security Lake. More details on recommended account structure for your organization can be found here. </p>"},{"location":"guides/security-lake/#cloudtrail-management-events","title":"CloudTrail management events","text":"<p>To collect CloudTrail Management events with Security Lake you will need to have at least one CloudTrail multi-region organization trail that collects read and write CloudTrail managements events enabled. If you\u2019re using Control Tower this is likely part of your Control Tower configuration. For information about creating and managing a trail through CloudTrail, see Creating a trail for an organization in the AWS CloudTrail User Guide. For information about creating and managing a trail through AWS Control Tower, see Logging AWS Control Tower actions with AWS CloudTrail in the AWS Control Tower User Guide.</p>"},{"location":"guides/security-lake/#plan-your-deployment","title":"Plan your deployment","text":"<p>Security Lake offers a 15-day free trial. We recommend making the most out of this free trial by making the following decisions ahead of enabling Security Lake. </p>"},{"location":"guides/security-lake/#what-regions-to-collect-security-data-from","title":"What regions to collect security data from","text":"<p>Security Lake is a regional service with global visibility. This gives you the ability to access the Security Lake console in one region and configure log collection across all of your desired regions. We recommend enabling Security Lake in all regions, even those you are not actively using. Security Lake is a pay as you go service. So, since it will collect next to no data in unused regions, it remains cost effective to enable Security Lake in all regions. Then, in the event activity occurs in an unexpected region, you will have captured the related logs and can use them for investigation. Alternatively, customers can choose to entirely block unused regions using an SCP such as this. </p>"},{"location":"guides/security-lake/#rollup-regions","title":"Rollup regions","text":"<p>You can optionally aggregate your data using \u201crollup regions\u201d. A rollup region is a region you choose to receive a copy of security data from \u201ccontributing regions\u201d that you specify. To elaborate on this point it is important to remember that although S3 bucket names are globally unique the data stored in S3 is specific to a region. When configuring Security Lake a separate bucket is created for each region you are collecting logs in. Using rollup regions, you can have contributing regions \u201crollup\u201d their data to your rollup region bucket allowing all of your logs to be stored in the region you specify. This can provide needed flexibility for data residency requirements. Note that data from the rollup region is replicated, not moved. If you\u2019d like to minimize duplicative data you can use a storage class transition for the contributing region to set an expiration period for the data. We recommend setting the expiration period to a comfortable length based on your own risk profile and control environment. For example, some customers find an expiration period of 3 days is the appropriate balance between cost and the risk of lost logs since it\u2019s long enough to cover an issue that occurs over the weekend. Other customers prefer an expiration period of 7 days to have a longer time to identify any issues.  </p>"},{"location":"guides/security-lake/#native-data-sources","title":"Native data sources","text":"<p>Before deploying Security Lake, consider what native data sources you would like to collect. Security Lake supports the managed collection of the following native AWS logs: </p> <ul> <li>Default logs<ul> <li>AWS CloudTrail management events</li> <li>Amazon Elastic Kubernetes Service (Amazon EKS) Audit Logs</li> <li>Amazon Route 53 resolver query logs</li> <li>AWS Security Hub findings (includes findings Security Hub consumes)</li> <li>Amazon Virtual Private Cloud (Amazon VPC) Flow Logs</li> </ul> </li> <li>Opt-in logs<ul> <li>AWS CloudTrail data events (S3, Lambda)</li> <li>Amazon Web Application Firewall logs</li> </ul> </li> </ul> <p>During deployment, you will be able to choose which logs to collect. Default logs will already be selected, but you can choose to not collect any of them. AWS CloudTrail data events and WAF logs aren\u2019t collected by default due to their high volume and associated cost in collecting them. If you would like to collect these logs, you will need to opt-in to collecting them during deployment or via a configuration change after deployment. </p> <p>We recommend turning on all of the default AWS log sources. Security Lake will collect these logs on your behalf without you needing to make any additional configuration at the service or resource level (except for CloudTrail management events as noted in the pre-requisites section above). Security Lake will then transform this data into Open Cybersecurity Schema Framework (OCSF) format, and put these logs in an S3 bucket in your environment in Apache Parquet files. Opt-in log sources are generally very high volume, resulting in higher costs to collect and store these logs. Many customers see value in collecting these logs but we recommend fully understanding the security benefit they provide versus the cost to collect them before deciding to collect these log sources.</p>"},{"location":"guides/security-lake/#security-lake-partners","title":"Security Lake partners","text":"<p>A major benefit of Security Lake is the pre-built partner integrations designed to send data to Security Lake or consume data from it. There are three types of Security Lake partners: source partners, subscriber partners, and service partners. Source partners are third-parties that can send security logs and findings to Security Lake in the OCSF format. Subscriber partners are third-parties that receive data from Security Lake. They are often tools that help with security use cases such as threat detection, investigation, and incident response. Service partners are third-parties that help you build or utilize Security Lake. </p> <p>To make the most of the Security Lake 15-day free trial, you can test integrations in a sandbox account to make sure you can quickly deploy partner integrations when testing Security Lake with production data. Each partner has specific integration details which can be found here. </p>"},{"location":"guides/security-lake/#custom-data-sources","title":"Custom data sources","text":"<p>Security Lake also supports aggregation of custom data sources. If you are using a tool that is currently not a Security Lake partner, creating a custom source will allow you to still send logs or findings to Security Lake. Some customers choose to deploy Security Lake in phases, starting with native AWS logs, then adding partner sources and subscribers, and then adding custom sources. Refer to the Security Lake documentation, or this blog, for details on how to set up custom sources.</p>"},{"location":"guides/security-lake/#implementing-security-lake","title":"Implementing Security Lake","text":"<p>You should now be ready to deploy Security Lake across your AWS Organization. During deployment you will select which accounts and regions where you would like to enable Security Lake, and specify the data sources you would like to collect. You can also optionally configure rollup regions and lifecycle policies at this time. We will walk through enabling Security Lake using the AWS console. For information on how to configure Security Lake using the CLI or API refer to the Security Lake documentation. </p> <ol> <li>From your Organization Management account, open the Security Lake console at: https://console.aws.amazon.com/securitylake </li> <li>Click \u201cGet started\u201d     </li> <li>Enter the 12 digit account ID of the account you\u2019d like to use as the delegated administrator for Security Lake, then click \u201cDelegate\u201d      </li> <li>Switch to the delegated admin account and open the Security Lake console at: https://console.aws.amazon.com/securitylake. You will see the same landing page you saw when in the Organization Management account. Click \u201cGet started\u201d again, which will now take you to the Security Lake enablement page.</li> <li>First, you will select what native log sources to ingest with Security Lake. You can either collect the default sources, or choose to specify what sources using the radio buttons at the top of this window. If you choose to specify log sources, you will then have to check which logs you\u2019d like to ingest.      </li> <li>Next, you will select what regions to ingest the previously selected log sources from. Again, you will have the option to collect from the default regions (all supported regions), or specify regions manually.      </li> <li>Next, you will select the accounts to ingest data from. You can select to collect from all accounts in your organization, specific accounts (listed in CSV format), or just the current account. The check box at the bottom of this window applies your Security Lake collection settings to any new accounts added to your organization. This allows for managed log collection even as your organization grows. We recommend leaving this feature enabled to reduce manual effort when new accounts are added to your organization. Uncheck this box if you do not wish to have his behavior. If you\u2019ve already deployed Security Lake, and would like to confirm this setting is enabled, you can use this API call to check, and this API call to enable the setting.      </li> <li>Please note that during initial setup, Security Lake can only be configured to collect all selected log sources from all selected accounts and regions. In other words, the logs you select will be applied to all accounts and regions where you are enabling Security Lake. If you have more specific requirements, like collecting different log sources in different regions or excluding a log source for a certain account you can configure this after initial deployment via the Security Lake console. </li> <li>Optionally configure service access and tags. For more details on service access, refer to the Security Lake documentation.     </li> <li>Click \u201cNext\u201d to be taken to the next page of options. Here, you can optionally configure rollup regions and storage class transitions.      </li> <li>If you choose to configure rollup regions, click \u201cAdd rollup Region\u201d and specify the rollup region and the contributing region. Repeat this for each region you\u2019d like to have contributing to the rollup region. Note that a region cannot be a contributing region and a rollup region. Also, more than one rollup region can be created like in the example below.      </li> <li>If you\u2019d like to add a storage transition that will apply across your entire Security Lake deployment, you can do so now by clicking \u201cAdd transition\u201d, specifying the desired storage class, and entering the number of days. You can create more granular storage transitions for each region after initial deployment via the Security Lake console.      </li> <li>If you configured a rollup region, you will also be prompted to create a new service roll for replication, or choose to use an existing one. For details on using an existing role, see the Security Lake documentation.      </li> <li>Click next to be taken to review your deployment of Security Lake. These settings can be changed later, but we recommend taking time to confirm your configuration options now. When you are ready, click \u201cCreate\u201d to deploy Security Lake. </li> <li>After a few minutes, you should see confirmation that security lake has been successfully deployed in all of the selected regions. Security Lake will immediately begin collecting data. </li> </ol>"},{"location":"guides/security-lake/#fine-tuning-your-security-lake-implementation","title":"Fine tuning your Security Lake implementation","text":"<p>After the initial deployment of Security Lake, you are able to further refine what logs you are collecting in which accounts and regions via the Security Lake console. You can also add coverage for new accounts and regions you excluded during initial set up. </p>"},{"location":"guides/security-lake/#changes-by-log-source","title":"Changes by log source","text":"<p>Changing your Security Lake configuration by log source allows you to pick a log source and specify exactly what regions it is collected in. This is the best way to change the collection of one log source for many regions. This is done via the Security Lake console, on the \u201cSources\u201d page. </p> <p>On this page, select a log source, and click \u201cConfigure\u201d in the top right. You will then be able to choose whether you\u2019d like to enable or disable that log source in the regions you specify. These changes will affect all accounts Security Lake is configured to collect logs from for the specified regions. </p> <p> </p>"},{"location":"guides/security-lake/#changes-by-region","title":"Changes by region","text":"<p>Changing your Security Lake configuration by region allows you to pick a region and specify what log sources you\u2019d like collected in that region. This is the best way to change a specific region\u2019s collection settings. You can also modify the storage transitions for the underlying S3 bucket using this method. This is done via the Security Lake console, on the \u201cRegions\u201d page. </p> <p>On this page, select a region, and click \u201cEdit\u201d in the upper right of the page. Next, click the check box next to the statement beginning with \u201cOverride sources for all accounts in...\u201d. Then, you can configure the collected log sources for all accounts in the selected region. You are also able to configure S3 storage transitions and add tags from this same page. </p> <p> </p>"},{"location":"guides/security-lake/#changes-by-account-and-region","title":"Changes by account and region","text":"<p>Changing your Security Lake configuration by account/region pairings allows for the most granular changes to what log sources are collected from where. This is done via the Security Lake console on the \u201cAccounts\u201d page. </p> <p>On this page, select an account and region pairing and click \u201cEdit\u201d in the upper right corner. You are then able to change log source collection for that specific account in that specific region. </p> <p> </p>"},{"location":"guides/security-lake/#adding-regions-and-accounts","title":"Adding regions and accounts","text":"<p>If you did not include all accounts and regions when initially setting up Security Lake, you are able to add them via the Security Lake console, on the \u201cRegions\u201d and \u201cAccounts\u201d pages. Each of these pages has an orange button in the top right corner to add new regions or accounts. You will then see a similar set up wizard to specify what log sources you\u2019d like for the entire region, or the new accounts that you specify (list by account ID separated by commas). </p>"},{"location":"guides/security-lake/#changing-s3-lifecycle-directly","title":"Changing S3 lifecycle directly","text":"<p>Since Security Lake\u2019s underlying storage architecture is S3, you can configure lifecycle management directly at the level of the buckets and prefixes. This allows for more granular customization to bucket lifecycles than the bulk changes supported in the ASL console. However, note that change made directly in S3 will not be reflected in the Security Lake console. Refer to the S3 documentation for more detail. </p>"},{"location":"guides/security-lake/#configure-collection-of-third-party-data","title":"Configure collection of third party data","text":"<p>Security Lake enables you to easily collect AWS log sources, but one of the main concepts of a data lake is to be able to collect many different data sources so that you can derive insights from this data.</p>"},{"location":"guides/security-lake/#partner-sources","title":"Partner sources","text":"<p>If you would like to collect a third party log source, begin by checking if it is a supported Security Lake source partner. Security Lake source partners have already built an integration that allows you to send logs or findings from their solution to Security Lake in OCSF format. The details of this integration vary by partner, but links to all partner integration documentation is can be found here. Follow the applicable guidance published by the partner.</p>"},{"location":"guides/security-lake/#using-security-hub-to-route-findings-to-security-lake","title":"Using Security Hub to route findings to Security Lake","text":"<p>Security Hub supports the collection of security findings from a wide range of third-party sources. These findings can then be sent to Security Lake. If you are looking to send security data from a third-party security source to Security Lake, and it isn\u2019t a source partner, we recommend checking if it has a Security Hub integration by reviewing this list.</p>"},{"location":"guides/security-lake/#custom-sources","title":"Custom sources","text":"<p>In Amazon Security Lake you can use Custom sources to collect logs or findings from any source. For example, you can collect logs from an on-premises DNS server, SaaS applications, custom applications, or from other cloud providers. You will be responsible for any necessary transformation to OCSF before bringing these types of log sources into Security Lake. AWS services like Glue, Data Pipeline, and Kinesis can help convert your custom log sources into OCSF. AWS AppFabric can also help by converting logs from supported SaaS applications into OCSF. For more details about AppFabric\u2019s integration with Security Lake, refer to the AppFabric documentation.</p> <p>To understand how to create a custom source in Security Lake refer to the Security Lake documentation. For a pattern on how to collect data from a custom source we recommend looking at this Security Lake blog post. </p>"},{"location":"guides/security-lake/#operationalizing-security-lake","title":"Operationalizing Security Lake","text":"<p>Once your security logs have been centrally collected and normalized to OCSF format, you will naturally want to derive insights and value from the data you are collecting. This can be done using AWS native analytics solutions or by using Security Lake subscriber partner solutions. </p>"},{"location":"guides/security-lake/#aws-native-analytics","title":"AWS native analytics","text":"<p>Your Security Lake data ultimately resides in S3, so there is a wide range of AWS native analytics solutions you can take advantage of. The native solutions we recommend considering for use with Security Lake are Athena, QuickSight, and OpenSearch.  </p> <p>Athena allows you to run SQL queries against Security Lake data. Once Security Lake is deployed, it is integrated with Athena by default. If you have never used Athena before, you will need to configure an S3 bucket for Athena to write query results to. See the Athena documentation for help doing this. You can find examples of queries you can run on your Security Lake data, along with additional guidance, here. Athena is the recommended analytics approach for security data you are not accessing frequently. It can also enable your security team to run ad-hoc investigations and reports. In some cases, Athena can enable customers to shorten retention windows in downstream, more costly, analytics solutions or stop sending rarely used logs all together. </p> <p>QuickSight allows you to create dashboards and reports based on Security Lake data and share them with your stakeholders. This the recommended approach for customers looking for cost effective reporting capabilities to use with Security Lake. QuickSight also offers generative AI features through Amazon Q in QuickSight, allowing users to interact with their security data using natural language. If you are interested in using QuickSight with Security Lake, we recommend evaluating the pre-built AWS Solution \u201cSecurity Insights on AWS\u201d. </p> <p>OpenSearch is an open-source search and analytics suite that can be deployed as a managed service in AWS. OpenSearch supports using an ingestion pipeline for Security Lake and a zero-ETL integration with Security Lake. The ingestion pipeline approach is recommended for Security Lake customers looking for near real-time search and analytics capabilities or that need alerting based on their Security Lake data. The zero-ETL approach is recommended for customers who would like to query Security Lake data without the extra step, and cost, of moving it to an OpenSearch cluster. The zero-ETL approach offers a balance between cost efficiency and performance, especially with OpenSearch features like indexed views and dashboards. Both deployment options are documented in the following blog posts: ingestion pipeline approach and zero-ETL approach.</p>"},{"location":"guides/security-lake/#subscriber-partners","title":"Subscriber partners","text":"<p>Subscriber partners offer additional ways to make use of your Security Lake data. Many popular SIEM, SOAR, XDR, and security analytics tools are Security Lake partners. This means they can ingest data from your deployment of Security Lake. These partners play a critical role in extending what you can do with your Security Lake data. Security Lake can also simplify the implementation and management of these security analytics solutions. Customers can use Security Lake as a managed solution to collect native AWS logs, aggregate them to one account and region, and then have one point of integration with the downstream analytics solution. </p> <p>Security Lake subscribers have two different access models. Data access, which allows logs to be sent to the partner solution, and query access, which only allows the partner solution to run queries against Security Lake data and return the results. When configuring a Security Lake subscriber of either type, you are able to select what log sources the subscriber has access to. For additional details on configuring subscribers and  managing their access, refer to the Security Lake documentation. </p> <p>Use cases for subscribers might be sending certain log data to a SIEM or SOAR solution for correlation or response and remediation. Another example would be to create a cross account subscriber that will allow a development team to access CloudTrail data to troubleshoot issues in their account without the need to create a separate individual account CloudTrail trail. There are also many other use cases such as using SageMaker to run machine learning analytics or integrating with a long list of third party integrationsthat can help you derive value from your security logs. Each subscriber partner will also offer guidance on how to properly set up the integration between their solution and Security Lake. Guidance on these integrations is offered here. However, all subscribers will require some configuration within the Security Lake console, on the subscriber page, like in the example below. </p> <p> </p>"},{"location":"guides/security-lake/#ocsf-schema-updates","title":"OCSF schema updates","text":"<p>Security Lake will periodically go through schema updates as newer versions of OCSF are released. When this occurs, you will have the option to choose when to update your deployment of Security Lake to the new schema version. However, partners may not support the new schema right as it is released. We recommend validating partner support of any new schema versions prior to updating.</p>"},{"location":"guides/security-lake/#cost-considerations","title":"Cost considerations","text":"<p>With Security Lake, you pay as you go with no upfront costs. Security Lake prices are based on two dimensions: data ingestion and data normalization. Monthly costs are determined by the volume of log and event data ingested from AWS services per gigabyte. There is no charge for bringing third-party or your own data.  For more information on pricing view the Security Lake pricing page.  In this section we want to highlight important cost considerations.</p>"},{"location":"guides/security-lake/#cost-breakdown-and-visibility","title":"Cost breakdown and visibility","text":"<p>Security Lake provisions other services on your behalf to successfully manage and orchestrate a data lake. This includes S3, Lake Formation, Glue, Lambda, EventBridge, Kinesis and SQS. Each of these services will generate cost independently following their respective pricing structure. The cost they generate will report to the individual services and is not included in the Security Lake pricing structure or reported back as charges tied to Security Lake in tools like Cost Explorer or the usage page of the Security Lake console. The majority of the orchestration costs related to Security Lake will be associated with the delegated admin account where you choose to run Security Lake. </p> <p>Security Lake\u2019s pricing structure includes charges for the ingestion and normalization of AWS log sources. These charges are associated with Security Lake in tools like Cost Explorer and are what is showed on the usage of page of the Security Lake console. These charges will be associated with the account that the logs originated from, meaning that in most cases, the majority of the charges directly associated to Security Lake will be tied to member accounts. </p>"},{"location":"guides/security-lake/#built-in-cost-savings","title":"Built in cost savings","text":"<p>By collecting all of your logs with Security Lake you are storing all of your logs in an S3 bucket in your environment. As a customer you get to choose what S3 storage tier you will use for these logs, everything from standard storage priced at \\$0.023/GB to deep archive priced at \\$0.00099/GB. In addition to this flexibility Security Lake also stores these logs as Apache Parquet files that allow 90%+ compression ratio of pre-text raw log volumes.</p>"},{"location":"guides/security-lake/#confirm-you-are-not-collecting-duplicate-native-logs","title":"Confirm you are not collecting duplicate native logs","text":"<p>Enabling Security Lake will not change any existing log collection settings you have. This means that if you enable Security Lake to collect a native log source you were already collecting, you will begin collecting more than one copy of the same data (although the format of the logs will differ). You can disable the previous log collection after enabling Security Lake. However, before doing so, we recommend identifying internal stakeholders or workloads using the previous logs and planning on how to switch them over to using Security Lake.</p>"},{"location":"guides/security-lake/#cloudtrail-management-event-cost-savings","title":"CloudTrail management event cost savings","text":"<p>To collect CloudTrail Management events you will need to create a CloudTrail organizational trail as mentioned above. This will result in CloudTrail sending logs to a bucket of your choice. The delivery of the first copy of these logs is free, however S3 storage cost is incurred. Security Lake will collect a copy of CloudTrail Management events and store these logs in the Security Lake S3 bucket. This will result in a duplication of CloudTrail logs. It is recommended to set an S3 lifecycle policy on the CloudTrail bucket to expire log files after a short period of time, largely dependent on what you decide at your organization. These log files will be delivered at the same time so expiration can be very short.</p>"},{"location":"guides/security-lake/#rollup-regions_1","title":"Rollup regions","text":"<p>When using a rollup region you are aggregating data from 1 or more regional buckets to a single region\u2019s bucket. This will result in a duplication of logs in each regional bucket as a copy now exists in your rollup region\u2019s bucket. Some customers prefer to keep multiple copies of their logs for resiliency. However, for cost optimization, it is recommended to handle this data duplication in the same manner as the duplication of CloudTrail Management events discussed in the previous section. Create an S3 lifecycle policy to expire these logs after a short period of time.</p>"},{"location":"guides/security-lake/#moving-logs-to-glacier","title":"Moving logs to glacier","text":"<p>Glacier storage classes can reduce S3 storage costs, but come with tradeoffs that should be carefully considered based on how you plan to interact with your security log data. Glacier storage classes are designed for long term storage of data that will not need to be accessed frequently. As a tradeoff for low storage cost, these classes have things like minimum retention periods, miniumum object sizes, added metadata for each object, slower retreival times, and direct retreival costs. For a full breakdown, refer to the S3 user guide. Be sure to fully understand the pricing structure of Glacier storage classes to make sure the storage savings outweigh the additional cost dimensions for your given use case. You should also consider how you will retrieve data when necessary, and make sure any applicable delays are acceptable for your organization. </p>"},{"location":"guides/security-lake/#additional-resources","title":"Additional resources","text":"<p>Documentation: Amazon Security Lake User Guide</p> <p>Documentation: Amazon Security Lake API Reference</p> <p>Video: AWS re:Inforce 2023 - Build your security data lake w/ Amazon Security Lake, featuring IPG</p> <p>Video: How to get started and manage Amazon Security Lake with AWS Organizations</p> <p>Video: Understanding Amazon Security Lake Cost</p> <p>Video: Amazon Security Lake with Amazon Athena and Amazon QuickSight</p> <p>Video: Amazon Security Lake Custom Source</p> <p>Video: Amazon Security Lake integration with Splunk</p> <p>Blog: Patterns for consuming custom log sources in Amazon Security Lake </p> <p>Blog: Get custom data into Amazon Security Lake through ingesting Azure activity logs</p> <p>Blog: Ingest transform and deliver events published by Amazon Security Lake to Amazon OpenSearch Service</p> <p>Blog: How Amazon Security Lake is helping customers simplify security data management for proactive threat analysis</p> <p>Blog: How to deploy an Amazon OpenSearch cluster to ingest logs from Amazon Security Lake</p> <p>Blog: Generate security insights from Amazon Security Lake data using Aamzon OpenSearch Ingestion</p> <p>Blog: Introducing Amazon OpenSearch Service and Amazon Security Lake integration to simplify security analytics.</p> <p>OCSF Github page</p>"},{"location":"guides/waf/","title":"Introduction","text":"<p>Welcome to the AWS WAF Best Practices Guide. The purpose of this project is to offer a set of operational best practices for AWS WAF.</p> <ul> <li>WAF Prerequisites</li> <li>Configuring WAF Rules</li> <li>Monitoring WAF Rules</li> <li>Using WAF with other AWS Services</li> </ul>"},{"location":"guides/waf/#related-guides","title":"Related guides","text":"<p>In addition to the AWS WAF User Guide, AWS has published several other guides that may help your implementation of AWS WAF.</p> <ul> <li>Migrating your AWS WAF Classic resources to AWS WAF</li> <li>Security Automations for AWS WAF</li> <li>Guidelines for Implementing AWS WAF</li> <li>Best practices for intelligent threat mitigation</li> <li>Best practices for using the CAPTCHA and Challenge actions</li> <li>Discover the benefits of AWS WAF advanced rate-based rules</li> <li>How to configure block duration for IP addresses rate limited by AWS WAF</li> </ul>"},{"location":"guides/waf/configuring-waf-rules/docs/","title":"Configuring WAF Rules","text":""},{"location":"guides/waf/configuring-waf-rules/docs/#understanding-terminating-actions","title":"Understanding terminating actions","text":"<p>One of the most important concepts to understand is that Allow and Block are terminating actions. When a rule matches and has one of these actions, no more rules are evaluated. Be cautious when using the Allow action, especially near the top of your web ACL. A rule with the Allow action will allow a request even if a subsequent rule would have blocked it (a false negative).</p>"},{"location":"guides/waf/configuring-waf-rules/docs/#general-approach-for-selecting-rules","title":"General approach for selecting rules","text":"<p>This section outlines a general thought process for selecting WAF rules. Assuming your web ACL's default action is Allow, it makes sense to block as many unwanted requests as possible near the top of the web ACL. Put rules toward the top that apply to the widest range of unwanted traffic. Put rules toward the end that have narrow criteria or have per-request charges. Rather than prescribing an exact ordering of rules, they are grouped into top, middle, and bottom categories.</p>"},{"location":"guides/waf/configuring-waf-rules/docs/#rules-toward-the-top","title":"Rules toward the top","text":"<ul> <li>Rate-based rules for blocking request floods</li> <li>Amazon IP reputation list managed rule group</li> <li>Anonymous IP list managed rule group</li> <li>Geographic-based rules for blocking or rate-limiting requests based on region of origin</li> </ul>"},{"location":"guides/waf/configuring-waf-rules/docs/#rules-toward-the-middle","title":"Rules toward the middle","text":"<ul> <li>Custom rules that validate expected HTTP request fields (user agents, headers)</li> <li>AWS Core rule set to block OWASP Top 10 threats</li> <li>SQL database managed rule group (only for applications that use SQL)</li> <li>Known bad inputs managed rule group (only for applications that use Java)</li> </ul>"},{"location":"guides/waf/configuring-waf-rules/docs/#rules-toward-the-bottom","title":"Rules toward the bottom","text":"<ul> <li>Bot Control rule group (with scope-down statement to limit applicability)</li> <li>Fraud Control account takeover prevention rule group (with scope-down statement to limit applicability)</li> </ul>"},{"location":"guides/waf/configuring-waf-rules/docs/#web-acl-capacity-units-wcu","title":"Web ACL capacity units (WCU)","text":"<p>Each web ACL has a capacity measured in web ACL capacity units (WCU). This capacity is used by rules and rule groups you add to the web ACL. By default the capacity is 1,500 WCU.</p> <p>To avoid unecessarily reaching this limit, make sure the web ACL only includes rules that are required by the protected applications. You can now use up to 5,000 WCU per web ACL without requesting a limit increase, but there are additional charges that come into play.</p>"},{"location":"guides/waf/configuring-waf-rules/docs/#selecting-the-default-action-for-your-web-acl","title":"Selecting the default action for your web ACL","text":"<p>If there are no matching rules in the web ACL that have a terminating action, the web ACL applies a default action. There are two possible default actions: Allow and Block. Most customers set the default action to Allow.</p> <p>If you want to allow most requests and only block attackers, use the Allow default action. For example, you might only want to block requests with malicious fields or that originate from an untrusted source IP or geography.</p> <p>If you want to allow specific requests and block everything else, use the Block default action. For example, you might only want to allow requests from a specific IP range or requests that contain specific values.</p>"},{"location":"guides/waf/configuring-waf-rules/docs/#rate-based-rules","title":"Rate-based rules","text":"<p>WAF rate-based rules count requests from up to 10,000 source IPs (per rate-based rule) and block requests when a client exceeds a threshold measured over a trailing 5-minute window. This is an often overlooked feature of AWS WAF, yet it's one of the most simple and valuable rules you can add to web ACLs. A majority of customers should have at least one rate-based rule in every web ACL.</p> <p>See the blog The three most important AWS WAF rate-based rules for a full explanation of how to leverage rate-based rules. In summary, the three most important rate-based rules are:</p> <ol> <li>A blanket rate-based rule that applies to all requests.</li> <li>URI-specific rate-based rules to protect specific parts of an application with more restrictive limits.</li> <li>Rate-based rules that limit the rate of requests from known malicious source IPs.</li> </ol> <p>AWS Shield Advanced provides DDoS cost protection to safeguard against scaling charges resulting from DDoS-related usage spikes on ALBs and CloudFront distributions protected by Shield Advanced. You can request a credit for charges through AWS Support. To be eligible to receive a credit for CloudFront and ALB protected resources, you must have associated an AWS WAF web ACL and implemented a rate-based rule in the web ACL.</p>"},{"location":"guides/waf/configuring-waf-rules/docs/#using-rule-labels","title":"Using rule labels","text":"<p>A label is metadata added to a web request by a matching rule. Use labels to make the results of one rule available to other rules. Labels can be inspected by other rules lower (not above) in the same web ACL by using the label match statement.</p> <p>Labels added by rules with terminating actions cannot be inspected by other rules. These labels are included in WAF log records and CloudWatch metric dimensions so you can analyze and visualize the behavior of terminating rules.</p> <p>Labels are commonly used to augment the behavior of a managed rule. The first step is to switch the managed rule's action from Block to Count. Then create another rule below that matches on managed rule's label along with other conditions that determine if the request should be blocked.</p> <p>See How to customize behavior of AWS Managed Rules for AWS WAF for more information on using labels.</p>"},{"location":"guides/waf/configuring-waf-rules/docs/#using-managed-rule-groups","title":"Using managed rule groups","text":""},{"location":"guides/waf/configuring-waf-rules/docs/#managed-rule-group-providers","title":"Managed rule group providers","text":"<p>AWS managed rule groups are maintained by Amazon's threat research team. These rules do not have a per-request fee, with the exception of Bot Control and Fraud Control rule groups. See above for a recommendation on which AWS managed rule groups are suitable for most customers.</p> <p>You can subscribe to managed rules provided by AWS partners using AWS Marketplace. These rule groups run in your WAF web ACL, but there are additional fees that come into play.</p> <p>To use an AWS Marketplace rule group in a Firewall Manager policy, each account in your organization must first subscribe to that rule group.</p>"},{"location":"guides/waf/configuring-waf-rules/docs/#versioning","title":"Versioning","text":"<p>A managed rule group provider might need to update their rules. For rule groups that support versioning, you can choose to let the provider manage which version you use (default) or you can manage the version setting yourself (static).</p> <p>When using the default version, subscribe to notifications of new versions. This informs you about upcoming changes to give you time to test them before they become the new default.</p> <p>When using a static version, it's also important to subscribe to notifications so you are aware of new static versions. Keep your version up to date to ensure your applications have the most current protections. Track version expiration to make sure you aren't forced to upgrade before you test.</p> <p>See How to customize behavior of AWS Managed Rules for AWS WAF for more information on version management.</p> <p>The AWS-managed IP reputation rule groups do not use versions. These rule groups are updated frequently based on the evolution of Amazon threat intelligence.</p>"},{"location":"guides/waf/configuring-waf-rules/docs/#scope-down-statements","title":"Scope-down statements","text":"<p>A scope-down statement is a statement that you add inside a managed rule group or a rate-based rule to narrow the set of requests that are evaluated.</p> <p>Use scope-down statements on advanced managed rule groups like Bot Control and Fraud Protection to specify which requests should be evaluated by the rule group. This is an effective way of optimizing your cost since you will not be charged for requests that are excluded by the scope-down statement.</p> <p>See How to customize behavior of AWS Managed Rules for AWS WAF for more information on scope-down statements.</p>"},{"location":"guides/waf/configuring-waf-rules/docs/#handling-false-positives","title":"Handling false positives","text":"<p>WAF rules are often looking for patterns that occur in HTTP request fields to determine if the request is malicious. Due to the infinite variety of data that might appear in HTTP requests, there are times when a WAF rule blocks a request that is not malicious. This is known as a false positive.</p> <p>To determine which rule is causing false positives, query your WAF logs to identify the label of the rule that blocked the request. Since this rule is incorrectly blocking requests, your instinct might be to write a rule that allows these requests. That turns out to be problematic because Allow is a terminating action, and your rule might allow requests that should have been blocked for other reasons.</p> <p>Follow these two steps to correct false positives caused by a managed rule. As an example, suppose the <code>Log4JRCE_BODY</code> rule inside the AWS managed rule group for Known bad inputs is causing false positives for the URI <code>/reports</code>.</p> <ol> <li>Switch the action for <code>Log4JRCE_BODY</code> from Block to Count. Requests matched by this rule will be labeled with <code>awswaf:managed:aws:known-bad-inputs:Log4JRCE_Body</code>.</li> <li>Write a rule below the managed rule group that has this behavior:</li> </ol> <pre><code>IF Label = awswaf:managed:aws:known-bad-inputs:Log4JRCE_Body AND\n   URI != \"/reports\"\nTHEN Block\n</code></pre> <p>Keep in mind that if you put a managed rule in Count mode then you must write a corresponding rule that blocks requests labeled by the managed rule. Otherwise, you will end up with false negatives.</p> <p>See How to customize behavior of AWS Managed Rules for AWS WAF for more information on handling false positives.</p>"},{"location":"guides/waf/configuring-waf-rules/docs/#handling-large-http-requests","title":"Handling large HTTP requests","text":"<p>AWS WAF has limits on the size and number of HTTP request components it can inspect. See Handling oversize web request components in AWS WAF for more details. Here is a summary of the size limits.</p> <ul> <li>AWS WAF can inspect request bodies up to 64 KB for CloudFront web ACLs. For regional web ACLs, AWS WAF can inspect bodies up to 8 KB.</li> <li>For all web ACLs, AWS WAF can inspect 8 KB of headers or cookies or the first 200 headers or cookies, whichever limit comes first.</li> </ul> <p>If you need to allow some oversized requests, add rules to explicitly Allow only those requests. Prioritize those rules so that they run before any other rules in the web ACL that inspect the same components. Here is an example that allows oversized requests only for a specific URI and HTTP method.</p> <pre><code>IF Body size &gt; 8,192 (with oversize handling set to Match) AND\n   URI path starts with \"/upload\" AND\n   HTTP method exactly matches \"POST\"\nTHEN Allow\n</code></pre> <p>For all other requests, add a rule to inspect components with size limits and Block requests that go over the limit. This prevents non-inspected request content from reaching your application.</p> <pre><code>IF Body size &gt; 8,192 (with oversize handling set to Match) OR\n   All headers size &gt; 8,192 (with oversize handling set to Match)\nTHEN Block\n</code></pre>"},{"location":"guides/waf/monitoring-waf-rules/docs/","title":"Monitoring WAF Rules","text":""},{"location":"guides/waf/monitoring-waf-rules/docs/#analyzing-aws-waf-metrics","title":"Analyzing AWS WAF metrics","text":"<p>Using CloudWatch metrics for AWS WAF you can create graphs on a dashboard that show over time how many requests were allowed, blocked, or counted by each rule in a web ACL. You can also see the same metrics by label.</p> <p>These metrics are useful in a few scenarios.</p> <ul> <li>Monitor the rate of requests matched by a new rule in Count mode before you decide to switch it to Block mode.</li> <li>Create an alarm that triggers when there is a spike of blocked or counted requests. This could indicate a threat that needs investigation.</li> <li>Determine which rule is contributing the most blocked requests over the past few days. This can help you isolate a rule that is not working as intended.</li> <li>Create an alarm that triggers when a rule in the Shield Advanced rule group starts blocking or counting a high number of requests.</li> </ul>"},{"location":"guides/waf/monitoring-waf-rules/docs/#analyzing-aws-waf-logs","title":"Analyzing AWS WAF logs","text":"<p>When storing AWS WAF logs in CloudWatch Logs, you can use Contributor Insights and Logs Insights to visualize logs with a CloudWatch dashboard. See the blog Visualize AWS WAF logs with an Amazon CloudWatch dashboard.</p> <p>You can analyze your AWS WAF logs in Amazon S3 using Amazon Athena. The documentation provides example queries as a starting point.</p> <p>You can leverage your Athena queries to create a dashboard in Amazon QuickSight.</p>"},{"location":"guides/waf/prerequisites/docs/","title":"WAF Prerequisites","text":"<p>Consider the following topics before creating your first AWS WAF web ACL.</p>"},{"location":"guides/waf/prerequisites/docs/#identifying-resources-to-protect-with-aws-waf","title":"Identifying resources to protect with AWS WAF","text":"<p>It's important to understand what type of resources you can protect natively with AWS WAF. If your use case is not supported directly, there may be architecture changes to consider. This section outlines the best practice for the most common use cases.</p>"},{"location":"guides/waf/prerequisites/docs/#aws-services-natively-supported-by-aws-waf","title":"AWS services natively supported by AWS WAF","text":"<p>AWS WAF is natively integrated with other AWS services. Simply associate the web ACL to the resource you want to protect.</p> <p> Figure 1: Global and regional AWS resources you can associate with AWS</p> <p>There are use cases where either the need for a WAF is unclear or the resource cannot be directly associated with a web ACL. The following sections offer guidance for these situations.</p>"},{"location":"guides/waf/prerequisites/docs/#private-application-load-balancers","title":"Private Application Load Balancers","text":"<p>A private Application Load Balancer (ALB) uses VPC subnets that do not have a route to an internet gateway. In other words, the ALB cannot be reached directly from the Internet.</p> <p> Figure 2: Protecting private Application Load Balancers</p> <p>It is uncommon to protect private ALBs with AWS WAF because the risk usually (but not always) does not justify the cost. Here are a few situations where you might protect private ALBs:</p> <ul> <li>Protect private ALBs with AWS WAF if they are indirectly handling unfiltered traffic from the Internet or any network you don't control. If the source IP address is not preserved, you may need to inspect an HTTP header that forwards the IP address. Typically this header is <code>X-Forwarded-For</code>.</li> <li>If your private ALB is subject to threats from your own network without other mitigating controls such as narrow VPC security group ingress rules then consider protecting it with AWS WAF.</li> </ul>"},{"location":"guides/waf/prerequisites/docs/#amazon-cloudfront-distributions-that-point-to-amazon-s3-buckets","title":"Amazon CloudFront distributions that point to Amazon S3 buckets","text":"<p>It is uncommon to protect CloudFront distributions that only have origins that point to Amazon S3 buckets. AWS is responsible for protecting Amazon S3 endpoints from malicious requests.</p> <p> Figure 3: Protecting CloudFront distributions with Amazon S3 bucket origins</p> <p>AWS WAF geographic match statements can be used to block requests based on region of origin. If you need to prevent users in specific geographic locations from accessing content distributed by CloudFront, you can use native geographic restrictions in CloudFront. If you use the CloudFront geo restriction feature, the feature doesn't forward blocked requests to AWS WAF. If you want to block requests based on geography and other AWS WAF criteria, use the AWS WAF geo match statement and do not use the CloudFront geo restriction feature.</p> <p>Use origin access control (OAC) to restrict access to your bucket so that Amazon S3 only allows requests from your CloudFront distribution.</p>"},{"location":"guides/waf/prerequisites/docs/#network-load-balancers","title":"Network Load Balancers","text":"<p>AWS WAF web ACLs cannot be associated with Network Load Balancers (NLB). If your NLB is handling HTTP traffic then consider using Application Load Balancer (ALB) instead. If you are using NLB to listen on static IP addresses, then you can create an ALB as the target of your NLB and protect the ALB with AWS WAF. By default, NLB preserves the client IP of traffic sent to ALB targets. This is important for IP-based AWS WAF rules to work properly.</p> <p> Figure 4: Using ALB with AWS WAF to protect NLB targets</p>"},{"location":"guides/waf/prerequisites/docs/#http-endpoints-hosted-outside-of-aws","title":"HTTP endpoints hosted outside of AWS","text":"<p>You can create an ALB with IP address targets. If your VPC has a route to your endpoint's network, you can specify any RFC 1918 address as a target. Contact AWS Support if you need to use IP targets outside of RFC 1918.</p> <p> Figure 5: Using AWS WAF to protect endpoints outside of AWS</p>"},{"location":"guides/waf/prerequisites/docs/#application-load-balancers-behind-a-cloudfront-distribution","title":"Application Load Balancers behind a CloudFront distribution","text":"<p>It is uncommon to use AWS WAF to protect an Application Load Balancer (ALB) that is the origin for a CloudFront distribution. Associate a web ACL with the CloudFront distribution to filter unwanted traffic at CloudFront edge locations. CloudFront forwards the client IP to the ALB in the <code>X-Forwarded-For</code> HTTP header, but there are some IP-based rules that will not work properly. For example, Amazon IP reputation list and Anonymous IP list currently do not support forwarded IPs.</p> <p> Figure 6: Using AWS WAF to protect ALBs behind CloudFront</p> <p>CloudFront requires ALB origins to be reachable by the Internet. Follow these steps make sure your ALB only accepts Internet traffic from your CloudFront distribution.</p> <ol> <li>Limit access to your origins using the AWS-managed prefix list for Amazon CloudFront</li> <li>Configure your ALB to respond to requests only when they include a customer header added by CloudFront. This ensures your origin only accepts requests from your CloudFront distribution.</li> </ol>"},{"location":"guides/waf/prerequisites/docs/#aws-resources-behind-a-non-cloudfront-content-delivery-network","title":"AWS resources behind a non-CloudFront content delivery network","text":"<p>If your application uses a CDN other than CloudFront, you can protect your origin with a regional AWS WAF web ACL. AWS WAF will not be able to see the client IP directly, so you must rely on the CDN forwarding client IPs in an HTTP header. Some AWS managed rule groups do not support forwarded IPs. See forwarded IP addresses for the rule statements that can use forwarded IPs.</p>"},{"location":"guides/waf/prerequisites/docs/#storing-aws-waf-logs","title":"Storing AWS WAF logs","text":"<p>AWS WAF logs can be stored in Amazon S3 or Amazon CloudWatch Logs. You can configure WAF to use Amazon Kinesis Data Firehose to deliver WAF logs to a variety of destinations, including Amazon S3.</p>"},{"location":"guides/waf/prerequisites/docs/#storing-aws-waf-logs-in-s3","title":"Storing AWS WAF logs in S3","text":"<p>Amazon S3 is the most cost-effective option for storing AWS WAF logs. There are two ways to configure AWS WAF to store logs in an S3 bucket.</p> <ol> <li>Using Kinesis Data Firehose.</li> <li>Directly by specifying the target bucket. This is simpler, but is more expensive than using KDF unless your log volume exceeds 80 billion events per month.</li> </ol> <p>When using Amazon S3 for AWS WAF logs, they are typically stored in a central bucket. In the web ACL you can specify the ARN of a bucket in any account or region. See the documentation for information about the bucket policy changes you need.</p> <p>See Monitoring WAF Rules for information on using logs Amazon S3 to monitor WAF behavior.</p>"},{"location":"guides/waf/prerequisites/docs/#storing-aws-waf-logs-in-cloudwatch-logs","title":"Storing AWS WAF logs in CloudWatch Logs","text":"<p>You can use subscription filters to have AWS WAF logs delivered to other services such as Amazon Kinesis or AWS Lambda for custom processing, analysis, or loading to other systems.</p> <p>You can use metric filters to parse log events and extract numerical values into CloudWatch metrics. These metrics can be used for configuring alarms or dashboards.</p> <p>You can centralize AWS WAF logs to a CloudWatch log group in a central account, but this is not supported natively and requires a non-trivial setup. First you set up a CloudWatch log group in the same region as the web ACL (<code>us-east-1</code> for global web ACLs). Then define a subscription filter with AWS Lambda. The Lambda function writes log events to a CloudWatch log group in your central account. For now, you can't use KDF to stream data across accounts.</p> <p>See Monitoring WAF Rules for information on using logs in CloudWatch Logs to monitor WAF behavior.</p>"},{"location":"guides/waf/prerequisites/docs/#filtering-waf-logs","title":"Filtering WAF logs","text":"<p>If you want to avoid logging fields that might contain sensitive data, you can omit fields from AWS WAF logs. Note that AWS WAF does not log the HTTP body.</p> <p>AWS WAF logs are detailed, as they can contain up to 100 rule labels. To optimize the cost of storing AWS WAF logs you can specify conditions that determine whether logs are kept or dropped (see log filtering). You can only filter based on rule labels and rule actions, not based on values in HTTP request fields.</p>"},{"location":"guides/waf/prerequisites/docs/#estimating-cost","title":"Estimating cost","text":"<p>There are a few topics to consider while reviwing AWS WAF pricing. </p> <p>The per-request pricing scales up based on two factors.</p> <ul> <li>Additional $0.20 per million requests for each 500 WCUs the Web ACL uses beyond the default allocation of 1,500</li> <li>Additional $0.30 per million requests for each additional 16KB analyzed beyond the default body inspection limit (16KB for CloudFront web ACLs, 8KB for regional web ACLs)</li> </ul> <p>The following AWS WAF charges are waived for resouces that are protected by AWS Shield Advanced.</p> <ul> <li>$0.60 per million requests analyzed by AWS WAF</li> <li>$5.00 per month per web ACL</li> </ul> <p>Shield Advanced does not cover these AWS WAF charges. This list may grow over time as more advanced capabilities are supported by AWS WAF.</p> <ul> <li>Additional charges when using more than 1,500 WCUs</li> <li>Additional charges when inspecting bodies larger than the default limit</li> <li>Charges for Fraud Control account creation fraud prevention</li> <li>Charges for Fraud Control account takeover prevention</li> <li>Charges for Bot Control</li> <li>Charges for CAPTCHA and Challenge actions</li> </ul> <p>CloudFront charges for requests even if they are blocked by AWS WAF. AWS Shield Advanced provides DDoS cost protection to safeguard against scaling charges resulting from DDoS-related usage spikes on CloudFront distributions protected by Shield Advanced. You can request a credit for charges through AWS Support. To be eligible to receive a credit for CloudFront and ALB protected resources, you must have associated an AWS WAF web ACL and implemented a rate-based rule in the web ACL.</p>"},{"location":"guides/waf/prerequisites/docs/#ownership-of-waf-rules","title":"Ownership of WAF rules","text":"<p>In a simple organization, application teams typically manage their own AWS WAF rules. They create web ACLs and own all the rules inside them. They also tend to keep AWS WAF logs in their own account.</p> <p>Larger organizations tend to have a company-wide security policy that requires all web applications to have a minimum baseline of AWS WAF rules. A security team needs to audit web applications for compliance reporting and automatic remediation. Customers use AWS Firewall Manager to define AWS WAF policies that enforce required rules. Application teams can customize FMS-managed web ACLs with application-specific rules. In this way, their web ACLs contain a mix of required and custom rules.</p> <p>Application teams should be aware of the responsibility model their organization uses. Security teams may wish to update the required rules, which requires coordinated testing. Also when using FMS, AWS WAF logs are typically stored in a central S3 bucket that application teams may need access to.</p>"},{"location":"guides/waf/prerequisites/docs/#how-many-web-acls-to-use","title":"How many web ACLs to use","text":"<p>AWS WAF web ACLs are associated with application resources, such as ALBs. Web ACLs can be associated with one or more resources in the same account. When should you share or dedicate web ACLs? The answer requires a balance between simplicity, isolation, and cost.</p> <p>To keep it simple, you can use one web ACL to protect multiple resources. Keep in mind this means multiple resources share the same AWS WAF rules. If one application needs a custom rule, it might impact requests for other applications sharing the same web ACL. If each application has unique rules, you might lean toward dedicated web ACLs instead of sharing.</p> <p>Cost is another factor that determines whether sharing web ACLs makes sense. AWS WAF charges are based on request volume and per-web ACL and per-rule charges. If the applications in your account handle over 10 million requests per month, request volume is the dominant pricing factor. In that case, using many web ACLs instead of one won't impact your costs very much. On the other hand, if you have many resources to protect and those resources handle a small number of requests per month, it might make sense to consolidate web ACLs to optimize cost.</p>"},{"location":"guides/waf/using-waf-with-other-services/docs/","title":"Using AWS WAF with other services","text":""},{"location":"guides/waf/using-waf-with-other-services/docs/#aws-firewall-manager-fms","title":"AWS Firewall Manager (FMS)","text":"<p>You can create a WAF policy in Firewall Manager. You specify which rule groups you want to have at the top and bottom of web ACLs. You define the scope of resources in your AWS Organization, and FMS creates web ACLs in each member account and associates them with resources in scope. FMS-managed web ACLs can be customized by member accounts, but the top and bottom rule groups cannot be modified.</p>"},{"location":"guides/waf/using-waf-with-other-services/docs/#deciding-what-rules-to-include-in-fms-security-policies-for-aws-waf","title":"Deciding what rules to include in FMS security policies for AWS WAF","text":"<p>FMS security policies for AWS WAF are typically managed by a central security team. This team is responsible for enforcing a baseline of rules across the organization. There might be a few parts of the organization that require a unique baseline and therefore a unique policy.</p> <p>The FMS-managed web ACL might need to be customized by a member account. A managed rule might be causing false positives that need to be addressed, or there might be an application-specific threat that is not mitigated by the baseline. It is recommended to update the web ACL rather than making application-specific changes to the FMS policy. Generally it is common to have a few FMS policies that apply to a wide range of resources rather than many FMS policies that each apply to a few resources.</p>"},{"location":"guides/waf/using-waf-with-other-services/docs/#using-cloudformation-to-update-fms-managed-web-acls","title":"Using CloudFormation to update FMS-managed web ACLs","text":"<p>AWS WAF V2 resources can be fully defined using CloudFormation templates. There are some considerations when using Firewall Manager to manage web ACLs.</p> <p>You can define Firewall Manager policies for AWS WAF using CloudFormation. The policy contains a definition of the AWS WAF rules you want at the top and bottom of the web ACLs created by FMS. This has a few implications when using CloudFormation (or any other infrastructure as code tool).</p> <p>The CloudFormation template format is not the same between AWS::WAFev2::WebACL and AWS::FMS::Policy. To use an existing AWS WAF web ACL as the basis for a Firewall Manager policy, you need to download the web ACL as JSON (in the AWS WAF console) or use the GetWebACL API. You use this JSON to construct the SecurityServicePolicyData element of the FMS policy resource. In simple cases, it might be easier to recreate the AWS WAF rules manually in the FMS console.</p> <p>Member accounts can customize web ACLs created by FMS using the AWS console or CLI or SDKs. However, you can't create a standard CloudFormation template that adds custom rules to the FMS-managed web ACL. AWS WAF rules are defined inside the AWS::WAFev2::WebACL resource. Since the web ACL already exists, you can't define it in your CloudFormation template. There are currently two options to work around this.</p> <ol> <li>Import the FMS-managed web ACL into your CloudFormation stack, then update the stack with custom rules. </li> <li>Define your custom rules in a AWS::WAFv2::RuleGroup, then use a Lambda-backed custom resource that has code to discover the FMS-managed web ACL and create a rule that references the rule group.</li> </ol>"},{"location":"guides/waf/using-waf-with-other-services/docs/#handling-false-positives-caused-by-fms-managed-rule-groups","title":"Handling false positives caused by FMS-managed rule groups","text":"<p>In large organizations it is not uncommon for an FMS-managed web ACL to include rules that cause false positives for one or more member accounts. You can use multiple FMS policies to handle these situations.</p> <ol> <li>An exception policy with the problematic rule in Count mode to allow handling of false positives. This policy's scope only includes resources that have a specific tag. Member accounts are responsible for adding the a rule that properly handles the false positive.</li> <li>A primary policy has all managed rules in Block mode. This policy is used to protect resources that are not concerned about false positives. This policy's scope excludes resources with the tag used by the exception policy.</li> </ol>"},{"location":"guides/waf/using-waf-with-other-services/docs/#amazon-guardduty","title":"Amazon GuardDuty","text":"<p>You can automate the creation of AWS WAF rules based on findings generated by Amazon GuardDuty. See the blog How to use Amazon GuardDuty and AWS WAF v2 to automatically block suspicious hosts.</p>"},{"location":"guides/waf/using-waf-with-other-services/docs/#aws-shield-advanced","title":"AWS Shield Advanced","text":"<p>Shield Advanced automatic application layer DDoS mitigation is a Shield Advanced feature that automatically detects application layer attacks and generates AWS WAF rules that isolate and block the anomalous traffic. When enabling this capability, a rule group managed by Shield Advanced is added to the bottom of the protected resource's web ACL.</p> <p>You can use AWS Shield policies in Firewall Manager to automatically configure this feature.</p>"},{"location":"guides/waf/using-waf-with-other-services/docs/#aws-transfer-family","title":"AWS Transfer Family","text":"<p>AWS Transfer Family is a service that manages file transfer protocols. See the blog Securing AWS Transfer Family with AWS Web Application Firewall and Amazon API Gateway.</p>"}]}